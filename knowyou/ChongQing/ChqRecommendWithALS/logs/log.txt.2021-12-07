2021-12-07 11:58:01,912 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.3.0
2021-12-07 11:58:02,307 [main] ERROR [org.apache.spark.SparkContext] - Error initializing SparkContext.
org.apache.spark.SparkException: A master URL must be set in your configuration
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:367)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2486)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
	at ValidationListSize$.main(ValidationListSize.scala:19)
	at ValidationListSize.main(ValidationListSize.scala)
2021-12-07 11:58:02,312 [main] ERROR [org.apache.spark.util.Utils] - Uncaught exception in thread main
java.lang.NullPointerException
	at org.apache.spark.SparkContext.org$apache$spark$SparkContext$$postApplicationEnd(SparkContext.scala:2382)
	at org.apache.spark.SparkContext$$anonfun$stop$1.apply$mcV$sp(SparkContext.scala:1897)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1896)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:578)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2486)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
	at ValidationListSize$.main(ValidationListSize.scala:19)
	at ValidationListSize.main(ValidationListSize.scala)
2021-12-07 11:58:02,315 [main] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2021-12-07 11:58:17,158 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.3.0
2021-12-07 11:58:17,415 [main] INFO [org.apache.spark.SparkContext] - Submitted application: ValidationListSize
2021-12-07 11:58:17,474 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: ACER,root
2021-12-07 11:58:17,474 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: ACER,root
2021-12-07 11:58:17,475 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2021-12-07 11:58:17,475 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2021-12-07 11:58:17,476 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ACER, root); groups with view permissions: Set(); users  with modify permissions: Set(ACER, root); groups with modify permissions: Set()
2021-12-07 11:58:18,068 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 50983.
2021-12-07 11:58:18,093 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2021-12-07 11:58:18,113 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2021-12-07 11:58:18,116 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2021-12-07 11:58:18,116 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2021-12-07 11:58:18,127 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\ACER\AppData\Local\Temp\blockmgr-c4e2c2c4-b637-47de-9aac-1dc9760f8d8f
2021-12-07 11:58:18,145 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1990.8 MB
2021-12-07 11:58:18,158 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2021-12-07 11:58:18,226 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @1869ms
2021-12-07 11:58:18,284 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2021-12-07 11:58:18,300 [main] INFO [org.spark_project.jetty.server.Server] - Started @1943ms
2021-12-07 11:58:18,330 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@4d63b624{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2021-12-07 11:58:18,330 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2021-12-07 11:58:18,352 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6b5966e1{/jobs,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,353 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@25bcd0c7{/jobs/json,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,353 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6c6357f9{/jobs/job,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,355 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@593e824f{/jobs/job/json,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,355 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2f7a7219{/stages,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,356 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@361c294e{/stages/json,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,357 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5118388b{/stages/stage,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,359 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5af28b27{/stages/stage/json,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,360 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@332a7fce{/stages/pool,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,361 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5217f3d0{/stages/pool/json,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,362 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@77307458{/storage,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,363 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@33617539{/storage/json,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,364 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f9b7332{/storage/rdd,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,365 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1bdf8190{/storage/rdd/json,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,366 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@ec0c838{/environment,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,367 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@21c64522{/environment/json,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,368 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@76a82f33{/executors,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,369 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@532a02d9{/executors/json,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,370 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@62923ee6{/executors/threadDump,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,371 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@b91d8c4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,379 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@a1217f9{/static,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,381 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1950e8a6{/,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,383 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@724f138e{/api,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,384 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@782a4fff{/jobs/job/kill,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,386 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6063d80a{/stages/stage/kill,null,AVAILABLE,@Spark}
2021-12-07 11:58:18,388 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://qb:4040
2021-12-07 11:58:18,477 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2021-12-07 11:58:18,524 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51027.
2021-12-07 11:58:18,525 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on qb:51027
2021-12-07 11:58:18,526 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2021-12-07 11:58:18,528 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, qb, 51027, None)
2021-12-07 11:58:18,531 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager qb:51027 with 1990.8 MB RAM, BlockManagerId(driver, qb, 51027, None)
2021-12-07 11:58:18,534 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, qb, 51027, None)
2021-12-07 11:58:18,534 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, qb, 51027, None)
2021-12-07 11:58:18,684 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@29a23c3d{/metrics/json,null,AVAILABLE,@Spark}
2021-12-07 11:58:19,210 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 312.7 KB, free 1990.5 MB)
2021-12-07 11:58:19,455 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KB, free 1990.5 MB)
2021-12-07 11:58:19,456 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on qb:51027 (size: 27.3 KB, free: 1990.8 MB)
2021-12-07 11:58:19,461 [main] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from textFile at ValidationListSize.scala:21
2021-12-07 11:58:19,868 [main] WARN [org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
2021-12-07 11:58:19,974 [main] INFO [org.apache.hadoop.mapred.FileInputFormat] - Total input paths to process : 1
2021-12-07 11:58:19,983 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at ValidationListSize.scala:25
2021-12-07 11:58:19,996 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (count at ValidationListSize.scala:25) with 2 output partitions
2021-12-07 11:58:19,997 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (count at ValidationListSize.scala:25)
2021-12-07 11:58:19,997 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 11:58:19,998 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 11:58:20,004 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at ValidationListSize.scala:23), which has no missing parents
2021-12-07 11:58:20,037 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 1990.5 MB)
2021-12-07 11:58:20,042 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1973.0 B, free 1990.5 MB)
2021-12-07 11:58:20,042 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on qb:51027 (size: 1973.0 B, free: 1990.8 MB)
2021-12-07 11:58:20,043 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2021-12-07 11:58:20,053 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at ValidationListSize.scala:23) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 11:58:20,054 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2021-12-07 11:58:20,090 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7914 bytes)
2021-12-07 11:58:20,090 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7914 bytes)
2021-12-07 11:58:20,096 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2021-12-07 11:58:20,096 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2021-12-07 11:58:20,133 [Executor task launch worker for task 1] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:106026+106027
2021-12-07 11:58:20,133 [Executor task launch worker for task 0] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:0+106026
2021-12-07 11:58:20,336 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 709 bytes result sent to driver
2021-12-07 11:58:20,336 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 709 bytes result sent to driver
2021-12-07 11:58:20,346 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 255 ms on localhost (executor driver) (1/2)
2021-12-07 11:58:20,347 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 268 ms on localhost (executor driver) (2/2)
2021-12-07 11:58:20,348 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2021-12-07 11:58:20,352 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (count at ValidationListSize.scala:25) finished in 0.332 s
2021-12-07 11:58:20,357 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: count at ValidationListSize.scala:25, took 0.372908 s
2021-12-07 11:58:20,387 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at ValidationListSize.scala:30
2021-12-07 11:58:20,396 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 3 (map at ValidationListSize.scala:27)
2021-12-07 11:58:20,396 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (take at ValidationListSize.scala:30) with 1 output partitions
2021-12-07 11:58:20,397 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (take at ValidationListSize.scala:30)
2021-12-07 11:58:20,397 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 1)
2021-12-07 11:58:20,397 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 1)
2021-12-07 11:58:20,398 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at map at ValidationListSize.scala:27), which has no missing parents
2021-12-07 11:58:20,403 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 4.7 KB, free 1990.5 MB)
2021-12-07 11:58:20,407 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 1990.5 MB)
2021-12-07 11:58:20,408 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on qb:51027 (size: 2.7 KB, free: 1990.8 MB)
2021-12-07 11:58:20,408 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2021-12-07 11:58:20,410 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at map at ValidationListSize.scala:27) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 11:58:20,410 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2021-12-07 11:58:20,411 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7903 bytes)
2021-12-07 11:58:20,412 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7903 bytes)
2021-12-07 11:58:20,412 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2021-12-07 11:58:20,412 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2021-12-07 11:58:20,416 [Executor task launch worker for task 2] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:0+106026
2021-12-07 11:58:20,416 [Executor task launch worker for task 3] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:106026+106027
2021-12-07 11:58:20,852 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 989 bytes result sent to driver
2021-12-07 11:58:20,868 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 457 ms on localhost (executor driver) (1/2)
2021-12-07 11:58:20,932 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 989 bytes result sent to driver
2021-12-07 11:58:20,934 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 523 ms on localhost (executor driver) (2/2)
2021-12-07 11:58:20,934 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2021-12-07 11:58:20,935 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 1 (map at ValidationListSize.scala:27) finished in 0.535 s
2021-12-07 11:58:20,935 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 11:58:20,935 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 11:58:20,936 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 2)
2021-12-07 11:58:20,936 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 11:58:20,939 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ValidationListSize.scala:29), which has no missing parents
2021-12-07 11:58:20,940 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 3.6 KB, free 1990.5 MB)
2021-12-07 11:58:20,945 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.5 MB)
2021-12-07 11:58:20,945 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on qb:51027 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 11:58:20,946 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2021-12-07 11:58:20,946 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ValidationListSize.scala:29) (first 15 tasks are for partitions Vector(0))
2021-12-07 11:58:20,946 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 1 tasks
2021-12-07 11:58:20,947 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7649 bytes)
2021-12-07 11:58:20,948 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2021-12-07 11:58:20,960 [Executor task launch worker for task 4] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 0 non-empty blocks out of 2 blocks
2021-12-07 11:58:20,962 [Executor task launch worker for task 4] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 6 ms
2021-12-07 11:58:20,979 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 1140 bytes result sent to driver
2021-12-07 11:58:20,980 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 32 ms on localhost (executor driver) (1/1)
2021-12-07 11:58:20,980 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2021-12-07 11:58:20,980 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (take at ValidationListSize.scala:30) finished in 0.040 s
2021-12-07 11:58:20,980 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: take at ValidationListSize.scala:30, took 0.593812 s
2021-12-07 11:58:20,987 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at ValidationListSize.scala:30
2021-12-07 11:58:20,988 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (take at ValidationListSize.scala:30) with 1 output partitions
2021-12-07 11:58:20,988 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (take at ValidationListSize.scala:30)
2021-12-07 11:58:20,988 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 3)
2021-12-07 11:58:20,988 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 11:58:20,988 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (MapPartitionsRDD[5] at map at ValidationListSize.scala:29), which has no missing parents
2021-12-07 11:58:20,991 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 3.6 KB, free 1990.4 MB)
2021-12-07 11:58:20,997 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.4 MB)
2021-12-07 11:58:20,998 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on qb:51027 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 11:58:20,998 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1039
2021-12-07 11:58:20,998 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[5] at map at ValidationListSize.scala:29) (first 15 tasks are for partitions Vector(1))
2021-12-07 11:58:20,999 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 1 tasks
2021-12-07 11:58:20,999 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 5, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 11:58:20,999 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 5)
2021-12-07 11:58:21,001 [Executor task launch worker for task 5] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 11:58:21,002 [Executor task launch worker for task 5] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 11:58:21,025 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 5). 1056 bytes result sent to driver
2021-12-07 11:58:21,025 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 5) in 26 ms on localhost (executor driver) (1/1)
2021-12-07 11:58:21,025 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2021-12-07 11:58:21,026 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (take at ValidationListSize.scala:30) finished in 0.035 s
2021-12-07 11:58:21,026 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: take at ValidationListSize.scala:30, took 0.038488 s
2021-12-07 11:58:21,028 [Thread-1] INFO [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2021-12-07 11:58:21,039 [Thread-1] INFO [org.spark_project.jetty.server.AbstractConnector] - Stopped Spark@4d63b624{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2021-12-07 11:58:21,040 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 72
2021-12-07 11:58:21,040 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 69
2021-12-07 11:58:21,040 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 46
2021-12-07 11:58:21,040 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 47
2021-12-07 11:58:21,040 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 54
2021-12-07 11:58:21,040 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 18
2021-12-07 11:58:21,040 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 26
2021-12-07 11:58:21,040 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 15
2021-12-07 11:58:21,041 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 44
2021-12-07 11:58:21,041 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 91
2021-12-07 11:58:21,041 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 59
2021-12-07 11:58:21,041 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 82
2021-12-07 11:58:21,041 [Thread-1] INFO [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://qb:4040
2021-12-07 11:58:21,048 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned shuffle 0
2021-12-07 11:58:21,055 [dispatcher-event-loop-2] INFO [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2021-12-07 11:58:21,068 [Thread-1] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2021-12-07 11:58:21,068 [Thread-1] INFO [org.apache.spark.storage.BlockManager] - BlockManager stopped
2021-12-07 11:58:21,069 [Thread-1] INFO [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2021-12-07 11:58:21,071 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2021-12-07 11:58:21,074 [Thread-1] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2021-12-07 11:58:21,074 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2021-12-07 11:58:21,075 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\ACER\AppData\Local\Temp\spark-3403ecd3-5f4b-48d8-b0b1-f76d0f1c7403
2021-12-07 13:32:00,915 [main] INFO [HelloTest$] - Hello
2021-12-07 13:32:00,916 [main] ERROR [HelloTest$] - BUG
2021-12-07 13:33:10,329 [main] INFO [HelloTest$] - Hello
2021-12-07 13:33:10,329 [main] ERROR [HelloTest$] - BUG
2021-12-07 13:33:31,832 [main] INFO [HelloTest$] - Hello
2021-12-07 13:33:31,832 [main] ERROR [HelloTest$] - BUG
2021-12-07 13:33:59,874 [main] INFO [HelloTest$] - Hello
2021-12-07 13:33:59,874 [main] ERROR [HelloTest$] - BUG
2021-12-07 13:35:02,635 [main] INFO [HelloTest$] - Hello
2021-12-07 13:35:02,635 [main] ERROR [HelloTest$] - BUG
2021-12-07 13:35:55,791 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.3.0
2021-12-07 13:35:56,122 [main] ERROR [org.apache.spark.SparkContext] - Error initializing SparkContext.
org.apache.spark.SparkException: A master URL must be set in your configuration
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:367)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2486)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
	at PaidPromotionAdjustParameter$.main(PaidPromotionAdjustParameter.scala:33)
	at PaidPromotionAdjustParameter.main(PaidPromotionAdjustParameter.scala)
2021-12-07 13:35:56,127 [main] ERROR [org.apache.spark.util.Utils] - Uncaught exception in thread main
java.lang.NullPointerException
	at org.apache.spark.SparkContext.org$apache$spark$SparkContext$$postApplicationEnd(SparkContext.scala:2382)
	at org.apache.spark.SparkContext$$anonfun$stop$1.apply$mcV$sp(SparkContext.scala:1897)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1896)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:578)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2486)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)
	at PaidPromotionAdjustParameter$.main(PaidPromotionAdjustParameter.scala:33)
	at PaidPromotionAdjustParameter.main(PaidPromotionAdjustParameter.scala)
2021-12-07 13:35:56,130 [main] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2021-12-07 13:36:25,427 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.3.0
2021-12-07 13:36:25,744 [main] INFO [org.apache.spark.SparkContext] - Submitted application: PaidPromotionAdjustParameter
2021-12-07 13:36:25,797 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: ACER,root
2021-12-07 13:36:25,797 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: ACER,root
2021-12-07 13:36:25,797 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2021-12-07 13:36:25,798 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2021-12-07 13:36:25,798 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ACER, root); groups with view permissions: Set(); users  with modify permissions: Set(ACER, root); groups with modify permissions: Set()
2021-12-07 13:36:26,394 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 50623.
2021-12-07 13:36:26,412 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2021-12-07 13:36:26,428 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2021-12-07 13:36:26,430 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2021-12-07 13:36:26,430 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2021-12-07 13:36:26,439 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\ACER\AppData\Local\Temp\blockmgr-6b1df013-ec25-4d9e-95cc-03165aa61401
2021-12-07 13:36:26,456 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1990.8 MB
2021-12-07 13:36:26,466 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2021-12-07 13:36:26,519 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @4009ms
2021-12-07 13:36:26,567 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2021-12-07 13:36:26,578 [main] INFO [org.spark_project.jetty.server.Server] - Started @4068ms
2021-12-07 13:36:26,601 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@3414a8c3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2021-12-07 13:36:26,601 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2021-12-07 13:36:26,621 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@27b45ea{/jobs,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,622 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4fc142ec{/jobs/json,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,623 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@702b06fb{/jobs/job,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,625 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2b22a1cc{/jobs/job/json,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,626 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7fd8c559{/stages,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,627 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@55a88417{/stages/json,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,628 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@19962194{/stages/stage,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,629 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3cbf1ba4{/stages/stage/json,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,630 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3dd818e8{/stages/pool,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,631 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@47b67fcb{/stages/pool/json,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,632 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@66420549{/storage,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,633 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@42b28ff1{/storage/json,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,634 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@706fe5c6{/storage/rdd,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,635 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6b3f6585{/storage/rdd/json,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,636 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@64469d8{/environment,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,637 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1cec219f{/environment/json,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,638 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3cb8c8ce{/executors,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,639 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@260a3a5e{/executors/json,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,640 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2e51d054{/executors/threadDump,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,641 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@608bc8f8{/executors/threadDump/json,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,647 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@381d7219{/static,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,648 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5f117b3d{/,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,650 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@11f406f8{/api,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,651 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@fb713e7{/jobs/job/kill,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,651 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@35f639fa{/stages/stage/kill,null,AVAILABLE,@Spark}
2021-12-07 13:36:26,653 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://qb:4040
2021-12-07 13:36:26,731 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2021-12-07 13:36:26,773 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50664.
2021-12-07 13:36:26,774 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on qb:50664
2021-12-07 13:36:26,775 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2021-12-07 13:36:26,776 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, qb, 50664, None)
2021-12-07 13:36:26,778 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager qb:50664 with 1990.8 MB RAM, BlockManagerId(driver, qb, 50664, None)
2021-12-07 13:36:26,780 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, qb, 50664, None)
2021-12-07 13:36:26,780 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, qb, 50664, None)
2021-12-07 13:36:26,904 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@28369db0{/metrics/json,null,AVAILABLE,@Spark}
2021-12-07 13:36:27,375 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 312.7 KB, free 1990.5 MB)
2021-12-07 13:36:27,578 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KB, free 1990.5 MB)
2021-12-07 13:36:27,579 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on qb:50664 (size: 27.3 KB, free: 1990.8 MB)
2021-12-07 13:36:27,583 [main] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from textFile at PaidPromotionAdjustParameter.scala:96
2021-12-07 13:36:27,929 [main] WARN [org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
2021-12-07 13:36:28,186 [main] INFO [org.apache.hadoop.mapred.FileInputFormat] - Total input paths to process : 1
2021-12-07 13:36:28,233 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:98
2021-12-07 13:36:28,243 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (count at PaidPromotionAdjustParameter.scala:98) with 2 output partitions
2021-12-07 13:36:28,243 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (count at PaidPromotionAdjustParameter.scala:98)
2021-12-07 13:36:28,244 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:36:28,245 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:36:28,251 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (/sk/chongqing/data/order_data.bcp MapPartitionsRDD[1] at textFile at PaidPromotionAdjustParameter.scala:96), which has no missing parents
2021-12-07 13:36:28,284 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 1990.5 MB)
2021-12-07 13:36:28,289 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1900.0 B, free 1990.5 MB)
2021-12-07 13:36:28,290 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on qb:50664 (size: 1900.0 B, free: 1990.8 MB)
2021-12-07 13:36:28,290 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:28,302 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (/sk/chongqing/data/order_data.bcp MapPartitionsRDD[1] at textFile at PaidPromotionAdjustParameter.scala:96) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:28,302 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2021-12-07 13:36:28,337 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7896 bytes)
2021-12-07 13:36:28,339 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7896 bytes)
2021-12-07 13:36:28,344 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2021-12-07 13:36:28,344 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2021-12-07 13:36:28,389 [Executor task launch worker for task 1] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:28,389 [Executor task launch worker for task 0] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:29,268 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 711 bytes result sent to driver
2021-12-07 13:36:29,286 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 947 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:29,346 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 754 bytes result sent to driver
2021-12-07 13:36:29,349 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 1021 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:29,350 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2021-12-07 13:36:29,350 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (count at PaidPromotionAdjustParameter.scala:98) finished in 1.086 s
2021-12-07 13:36:29,355 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: count at PaidPromotionAdjustParameter.scala:98, took 1.122211 s
2021-12-07 13:36:29,356 [main] INFO [PaidPromotion$] - 订购总数107294
2021-12-07 13:36:32,291 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:106
2021-12-07 13:36:32,291 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (count at PaidPromotionAdjustParameter.scala:106) with 2 output partitions
2021-12-07 13:36:32,291 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (count at PaidPromotionAdjustParameter.scala:106)
2021-12-07 13:36:32,291 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:36:32,292 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:36:32,292 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[2] at map at PaidPromotionAdjustParameter.scala:102), which has no missing parents
2021-12-07 13:36:32,293 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 1990.5 MB)
2021-12-07 13:36:32,297 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1985.0 B, free 1990.5 MB)
2021-12-07 13:36:32,298 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on qb:50664 (size: 1985.0 B, free: 1990.8 MB)
2021-12-07 13:36:32,298 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:32,299 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at map at PaidPromotionAdjustParameter.scala:102) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:32,299 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2021-12-07 13:36:32,300 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7896 bytes)
2021-12-07 13:36:32,300 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7896 bytes)
2021-12-07 13:36:32,300 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2021-12-07 13:36:32,300 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2021-12-07 13:36:32,303 [Executor task launch worker for task 2] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:32,303 [Executor task launch worker for task 3] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:32,647 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 711 bytes result sent to driver
2021-12-07 13:36:32,650 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 351 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:33,780 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 754 bytes result sent to driver
2021-12-07 13:36:33,785 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 9
2021-12-07 13:36:33,785 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 1485 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:33,785 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2021-12-07 13:36:33,785 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (count at PaidPromotionAdjustParameter.scala:106) finished in 1.493 s
2021-12-07 13:36:33,786 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: count at PaidPromotionAdjustParameter.scala:106, took 1.495101 s
2021-12-07 13:36:33,786 [main] INFO [PaidPromotion$] - 订购总数：107294
2021-12-07 13:36:33,800 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on qb:50664 in memory (size: 1900.0 B, free: 1990.8 MB)
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 10
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 15
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 18
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 14
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 8
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 20
2021-12-07 13:36:33,803 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:119
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 7
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 12
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 17
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 0
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 3
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 22
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 23
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 21
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 5
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 13
2021-12-07 13:36:33,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 6
2021-12-07 13:36:33,803 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (count at PaidPromotionAdjustParameter.scala:119) with 2 output partitions
2021-12-07 13:36:33,803 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (count at PaidPromotionAdjustParameter.scala:119)
2021-12-07 13:36:33,804 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:36:33,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 16
2021-12-07 13:36:33,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 11
2021-12-07 13:36:33,804 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:36:33,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 2
2021-12-07 13:36:33,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 19
2021-12-07 13:36:33,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 4
2021-12-07 13:36:33,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 24
2021-12-07 13:36:33,804 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[3] at randomSplit at PaidPromotionAdjustParameter.scala:112), which has no missing parents
2021-12-07 13:36:33,806 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 3.6 KB, free 1990.5 MB)
2021-12-07 13:36:33,809 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.5 MB)
2021-12-07 13:36:33,809 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on qb:50664 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:36:33,810 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:33,810 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[3] at randomSplit at PaidPromotionAdjustParameter.scala:112) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:33,810 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2021-12-07 13:36:33,811 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 7896 bytes)
2021-12-07 13:36:33,811 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 7896 bytes)
2021-12-07 13:36:33,812 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2021-12-07 13:36:33,812 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2021-12-07 13:36:33,814 [Executor task launch worker for task 4] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:33,815 [Executor task launch worker for task 5] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:34,221 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2.0 (TID 5). 711 bytes result sent to driver
2021-12-07 13:36:34,221 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2.0 (TID 5) in 410 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:34,600 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 711 bytes result sent to driver
2021-12-07 13:36:34,601 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 790 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:34,601 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2021-12-07 13:36:34,601 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (count at PaidPromotionAdjustParameter.scala:119) finished in 0.797 s
2021-12-07 13:36:34,601 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: count at PaidPromotionAdjustParameter.scala:119, took 0.798842 s
2021-12-07 13:36:34,602 [main] INFO [PaidPromotion$] - 初次切分训练集数量：47418
2021-12-07 13:36:34,607 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 30
2021-12-07 13:36:34,607 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 71
2021-12-07 13:36:34,607 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 73
2021-12-07 13:36:34,607 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 62
2021-12-07 13:36:34,607 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 49
2021-12-07 13:36:34,607 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 60
2021-12-07 13:36:34,607 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 37
2021-12-07 13:36:34,607 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 59
2021-12-07 13:36:34,607 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 48
2021-12-07 13:36:34,607 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 54
2021-12-07 13:36:34,607 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 67
2021-12-07 13:36:34,607 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 45
2021-12-07 13:36:34,608 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 46
2021-12-07 13:36:34,608 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 66
2021-12-07 13:36:34,608 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 58
2021-12-07 13:36:34,608 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 31
2021-12-07 13:36:34,608 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 32
2021-12-07 13:36:34,608 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 72
2021-12-07 13:36:34,608 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 39
2021-12-07 13:36:34,608 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 40
2021-12-07 13:36:34,608 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 53
2021-12-07 13:36:34,608 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 55
2021-12-07 13:36:34,608 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 68
2021-12-07 13:36:34,608 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:120
2021-12-07 13:36:34,609 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 52
2021-12-07 13:36:34,609 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 70
2021-12-07 13:36:34,609 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 63
2021-12-07 13:36:34,609 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (count at PaidPromotionAdjustParameter.scala:120) with 2 output partitions
2021-12-07 13:36:34,609 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 3 (count at PaidPromotionAdjustParameter.scala:120)
2021-12-07 13:36:34,609 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:36:34,609 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:36:34,610 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 3 (MapPartitionsRDD[4] at randomSplit at PaidPromotionAdjustParameter.scala:112), which has no missing parents
2021-12-07 13:36:34,610 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_3_piece0 on qb:50664 in memory (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:36:34,610 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 27
2021-12-07 13:36:34,610 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 42
2021-12-07 13:36:34,610 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 26
2021-12-07 13:36:34,610 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 38
2021-12-07 13:36:34,610 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 43
2021-12-07 13:36:34,610 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 28
2021-12-07 13:36:34,613 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 3.6 KB, free 1990.5 MB)
2021-12-07 13:36:34,613 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2_piece0 on qb:50664 in memory (size: 1985.0 B, free: 1990.8 MB)
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 64
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 35
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 25
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 51
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 36
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 50
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 29
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 34
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 69
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 41
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 57
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 56
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 44
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 74
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 61
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 47
2021-12-07 13:36:34,614 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 65
2021-12-07 13:36:34,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 33
2021-12-07 13:36:34,617 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.5 MB)
2021-12-07 13:36:34,618 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on qb:50664 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:36:34,618 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:34,619 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[4] at randomSplit at PaidPromotionAdjustParameter.scala:112) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:34,619 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 3.0 with 2 tasks
2021-12-07 13:36:34,619 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 7896 bytes)
2021-12-07 13:36:34,620 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 7896 bytes)
2021-12-07 13:36:34,620 [Executor task launch worker for task 6] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 3.0 (TID 6)
2021-12-07 13:36:34,620 [Executor task launch worker for task 7] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 3.0 (TID 7)
2021-12-07 13:36:34,622 [Executor task launch worker for task 6] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:34,622 [Executor task launch worker for task 7] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:34,993 [Executor task launch worker for task 6] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 3.0 (TID 6). 754 bytes result sent to driver
2021-12-07 13:36:34,993 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 3.0 (TID 6) in 374 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:35,587 [Executor task launch worker for task 7] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 3.0 (TID 7). 754 bytes result sent to driver
2021-12-07 13:36:35,587 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 3.0 (TID 7) in 968 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:35,587 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2021-12-07 13:36:35,588 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 3 (count at PaidPromotionAdjustParameter.scala:120) finished in 0.978 s
2021-12-07 13:36:35,588 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: count at PaidPromotionAdjustParameter.scala:120, took 0.979453 s
2021-12-07 13:36:35,588 [main] INFO [PaidPromotion$] - 初次切分验证集数量：59876
2021-12-07 13:36:35,998 [main] INFO [PaidPromotion$] - -----------------------------------
2021-12-07 13:36:36,000 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:136
2021-12-07 13:36:36,007 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 6 (distinct at PaidPromotionAdjustParameter.scala:126)
2021-12-07 13:36:36,008 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 4 (count at PaidPromotionAdjustParameter.scala:136) with 2 output partitions
2021-12-07 13:36:36,008 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 5 (count at PaidPromotionAdjustParameter.scala:136)
2021-12-07 13:36:36,008 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 4)
2021-12-07 13:36:36,008 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 4)
2021-12-07 13:36:36,009 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 4 (MapPartitionsRDD[6] at distinct at PaidPromotionAdjustParameter.scala:126), which has no missing parents
2021-12-07 13:36:36,020 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 5.2 KB, free 1990.5 MB)
2021-12-07 13:36:36,023 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1990.5 MB)
2021-12-07 13:36:36,024 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on qb:50664 (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:36:36,025 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:36,026 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[6] at distinct at PaidPromotionAdjustParameter.scala:126) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:36,026 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 2 tasks
2021-12-07 13:36:36,028 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 8, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:36:36,029 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 4.0 (TID 9, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:36:36,029 [Executor task launch worker for task 8] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 8)
2021-12-07 13:36:36,029 [Executor task launch worker for task 9] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 4.0 (TID 9)
2021-12-07 13:36:36,032 [Executor task launch worker for task 8] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:36,032 [Executor task launch worker for task 9] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:36,188 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 83
2021-12-07 13:36:36,188 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 84
2021-12-07 13:36:36,188 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 86
2021-12-07 13:36:36,188 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 80
2021-12-07 13:36:36,189 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 98
2021-12-07 13:36:36,190 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_4_piece0 on qb:50664 in memory (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:36:36,190 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 81
2021-12-07 13:36:36,190 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 93
2021-12-07 13:36:36,190 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 96
2021-12-07 13:36:36,190 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 92
2021-12-07 13:36:36,190 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 99
2021-12-07 13:36:36,190 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 77
2021-12-07 13:36:36,190 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 75
2021-12-07 13:36:36,190 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 79
2021-12-07 13:36:36,190 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 85
2021-12-07 13:36:36,191 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 91
2021-12-07 13:36:36,191 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 87
2021-12-07 13:36:36,191 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 89
2021-12-07 13:36:36,191 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 97
2021-12-07 13:36:36,191 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 88
2021-12-07 13:36:36,191 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 94
2021-12-07 13:36:36,191 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 78
2021-12-07 13:36:36,191 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 76
2021-12-07 13:36:36,191 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 95
2021-12-07 13:36:36,191 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 82
2021-12-07 13:36:36,191 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 90
2021-12-07 13:36:36,679 [Executor task launch worker for task 8] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 8). 1032 bytes result sent to driver
2021-12-07 13:36:36,693 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 8) in 666 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:37,011 [Executor task launch worker for task 9] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 4.0 (TID 9). 1032 bytes result sent to driver
2021-12-07 13:36:37,012 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 4.0 (TID 9) in 984 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:37,012 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2021-12-07 13:36:37,013 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 4 (distinct at PaidPromotionAdjustParameter.scala:126) finished in 1.002 s
2021-12-07 13:36:37,013 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:37,014 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:36:37,014 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 5)
2021-12-07 13:36:37,015 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:37,019 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 5 (MapPartitionsRDD[8] at distinct at PaidPromotionAdjustParameter.scala:126), which has no missing parents
2021-12-07 13:36:37,025 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 1990.5 MB)
2021-12-07 13:36:37,033 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.5 MB)
2021-12-07 13:36:37,034 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_6_piece0 in memory on qb:50664 (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:36:37,034 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 6 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:37,035 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at distinct at PaidPromotionAdjustParameter.scala:126) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:37,035 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 5.0 with 2 tasks
2021-12-07 13:36:37,039 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 5.0 (TID 10, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:36:37,040 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 5.0 (TID 11, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:36:37,040 [Executor task launch worker for task 11] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 5.0 (TID 11)
2021-12-07 13:36:37,040 [Executor task launch worker for task 10] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 5.0 (TID 10)
2021-12-07 13:36:37,051 [Executor task launch worker for task 11] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:37,051 [Executor task launch worker for task 10] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:37,052 [Executor task launch worker for task 11] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 4 ms
2021-12-07 13:36:37,052 [Executor task launch worker for task 10] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 4 ms
2021-12-07 13:36:37,157 [Executor task launch worker for task 11] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 5.0 (TID 11). 1055 bytes result sent to driver
2021-12-07 13:36:37,157 [Executor task launch worker for task 10] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 5.0 (TID 10). 1055 bytes result sent to driver
2021-12-07 13:36:37,158 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 5.0 (TID 11) in 118 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:37,158 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 5.0 (TID 10) in 123 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:37,158 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2021-12-07 13:36:37,159 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 5 (count at PaidPromotionAdjustParameter.scala:136) finished in 0.138 s
2021-12-07 13:36:37,159 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 4 finished: count at PaidPromotionAdjustParameter.scala:136, took 1.158749 s
2021-12-07 13:36:37,159 [main] INFO [PaidPromotion$] - 训练集用户数 = 44749
2021-12-07 13:36:37,162 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:137
2021-12-07 13:36:37,162 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 10 (distinct at PaidPromotionAdjustParameter.scala:127)
2021-12-07 13:36:37,162 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 5 (count at PaidPromotionAdjustParameter.scala:137) with 2 output partitions
2021-12-07 13:36:37,162 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 7 (count at PaidPromotionAdjustParameter.scala:137)
2021-12-07 13:36:37,163 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 6)
2021-12-07 13:36:37,163 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 6)
2021-12-07 13:36:37,163 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 6 (MapPartitionsRDD[10] at distinct at PaidPromotionAdjustParameter.scala:127), which has no missing parents
2021-12-07 13:36:37,164 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7 stored as values in memory (estimated size 5.2 KB, free 1990.4 MB)
2021-12-07 13:36:37,167 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1990.4 MB)
2021-12-07 13:36:37,168 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_7_piece0 in memory on qb:50664 (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:36:37,168 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 7 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:37,169 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[10] at distinct at PaidPromotionAdjustParameter.scala:127) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:37,169 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 6.0 with 2 tasks
2021-12-07 13:36:37,169 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 6.0 (TID 12, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:36:37,169 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 6.0 (TID 13, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:36:37,170 [Executor task launch worker for task 13] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 6.0 (TID 13)
2021-12-07 13:36:37,170 [Executor task launch worker for task 12] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 6.0 (TID 12)
2021-12-07 13:36:37,171 [Executor task launch worker for task 12] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:37,171 [Executor task launch worker for task 13] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:37,801 [Executor task launch worker for task 13] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 6.0 (TID 13). 989 bytes result sent to driver
2021-12-07 13:36:37,802 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 6.0 (TID 13) in 632 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:37,940 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 110
2021-12-07 13:36:37,940 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 136
2021-12-07 13:36:37,940 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 138
2021-12-07 13:36:37,940 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 107
2021-12-07 13:36:37,940 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 139
2021-12-07 13:36:37,940 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 119
2021-12-07 13:36:37,940 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 129
2021-12-07 13:36:37,940 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 133
2021-12-07 13:36:37,940 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 146
2021-12-07 13:36:37,940 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 122
2021-12-07 13:36:37,940 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 123
2021-12-07 13:36:37,941 [dispatcher-event-loop-8] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_5_piece0 on qb:50664 in memory (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 100
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 104
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 113
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 144
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 147
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 120
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 142
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 109
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 128
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 130
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 127
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 114
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 145
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 108
2021-12-07 13:36:37,941 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 149
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 131
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 132
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 106
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 112
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 117
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 126
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 124
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 116
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 103
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 118
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 102
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 143
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 141
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 121
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 125
2021-12-07 13:36:37,942 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 111
2021-12-07 13:36:37,942 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_6_piece0 on qb:50664 in memory (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:36:37,943 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 115
2021-12-07 13:36:37,943 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 148
2021-12-07 13:36:37,943 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 105
2021-12-07 13:36:37,943 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 140
2021-12-07 13:36:37,943 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 101
2021-12-07 13:36:37,943 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 135
2021-12-07 13:36:37,943 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 137
2021-12-07 13:36:37,943 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 134
2021-12-07 13:36:38,146 [Executor task launch worker for task 12] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 6.0 (TID 12). 1032 bytes result sent to driver
2021-12-07 13:36:38,147 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 6.0 (TID 12) in 978 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:38,147 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2021-12-07 13:36:38,147 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 6 (distinct at PaidPromotionAdjustParameter.scala:127) finished in 0.983 s
2021-12-07 13:36:38,147 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:38,147 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:36:38,147 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 7)
2021-12-07 13:36:38,147 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:38,147 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 7 (MapPartitionsRDD[12] at distinct at PaidPromotionAdjustParameter.scala:127), which has no missing parents
2021-12-07 13:36:38,149 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8 stored as values in memory (estimated size 3.7 KB, free 1990.5 MB)
2021-12-07 13:36:38,151 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.5 MB)
2021-12-07 13:36:38,152 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_8_piece0 in memory on qb:50664 (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:36:38,152 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 8 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:38,153 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[12] at distinct at PaidPromotionAdjustParameter.scala:127) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:38,153 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 7.0 with 2 tasks
2021-12-07 13:36:38,153 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 7.0 (TID 14, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:36:38,153 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 7.0 (TID 15, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:36:38,153 [Executor task launch worker for task 14] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 7.0 (TID 14)
2021-12-07 13:36:38,154 [Executor task launch worker for task 15] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 7.0 (TID 15)
2021-12-07 13:36:38,155 [Executor task launch worker for task 14] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:38,155 [Executor task launch worker for task 14] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:38,159 [Executor task launch worker for task 15] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:38,159 [Executor task launch worker for task 15] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:38,220 [Executor task launch worker for task 14] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 7.0 (TID 14). 1098 bytes result sent to driver
2021-12-07 13:36:38,220 [Executor task launch worker for task 15] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 7.0 (TID 15). 1098 bytes result sent to driver
2021-12-07 13:36:38,221 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 7.0 (TID 14) in 68 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:38,221 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 7.0 (TID 15) in 68 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:38,221 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2021-12-07 13:36:38,222 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 7 (count at PaidPromotionAdjustParameter.scala:137) finished in 0.074 s
2021-12-07 13:36:38,222 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 5 finished: count at PaidPromotionAdjustParameter.scala:137, took 1.059793 s
2021-12-07 13:36:38,222 [main] INFO [PaidPromotion$] - 验证集用户数 = 55770
2021-12-07 13:36:38,229 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:138
2021-12-07 13:36:38,230 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 14 (intersection at PaidPromotionAdjustParameter.scala:128)
2021-12-07 13:36:38,230 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 13 (intersection at PaidPromotionAdjustParameter.scala:128)
2021-12-07 13:36:38,230 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 6 (count at PaidPromotionAdjustParameter.scala:138) with 2 output partitions
2021-12-07 13:36:38,230 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 12 (count at PaidPromotionAdjustParameter.scala:138)
2021-12-07 13:36:38,230 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 9, ShuffleMapStage 11)
2021-12-07 13:36:38,231 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 9, ShuffleMapStage 11)
2021-12-07 13:36:38,231 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 9 (MapPartitionsRDD[14] at intersection at PaidPromotionAdjustParameter.scala:128), which has no missing parents
2021-12-07 13:36:38,233 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9 stored as values in memory (estimated size 4.0 KB, free 1990.5 MB)
2021-12-07 13:36:38,235 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.4 MB)
2021-12-07 13:36:38,236 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_9_piece0 in memory on qb:50664 (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:36:38,236 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 9 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:38,237 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[14] at intersection at PaidPromotionAdjustParameter.scala:128) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:38,237 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 9.0 with 2 tasks
2021-12-07 13:36:38,237 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 11 (MapPartitionsRDD[13] at intersection at PaidPromotionAdjustParameter.scala:128), which has no missing parents
2021-12-07 13:36:38,237 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 9.0 (TID 16, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:36:38,237 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 9.0 (TID 17, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:36:38,237 [Executor task launch worker for task 16] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 9.0 (TID 16)
2021-12-07 13:36:38,237 [Executor task launch worker for task 17] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 9.0 (TID 17)
2021-12-07 13:36:38,238 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10 stored as values in memory (estimated size 4.0 KB, free 1990.4 MB)
2021-12-07 13:36:38,240 [Executor task launch worker for task 16] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:38,240 [Executor task launch worker for task 16] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:38,240 [Executor task launch worker for task 17] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:38,240 [Executor task launch worker for task 17] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:38,246 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.4 MB)
2021-12-07 13:36:38,247 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_10_piece0 in memory on qb:50664 (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:36:38,247 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 10 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:38,247 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[13] at intersection at PaidPromotionAdjustParameter.scala:128) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:38,247 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 11.0 with 2 tasks
2021-12-07 13:36:38,248 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 11.0 (TID 18, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:36:38,248 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 11.0 (TID 19, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:36:38,249 [Executor task launch worker for task 18] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 11.0 (TID 18)
2021-12-07 13:36:38,249 [Executor task launch worker for task 19] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 11.0 (TID 19)
2021-12-07 13:36:38,252 [Executor task launch worker for task 19] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:38,252 [Executor task launch worker for task 19] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:36:38,252 [Executor task launch worker for task 18] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:38,252 [Executor task launch worker for task 18] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:38,317 [Executor task launch worker for task 19] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 11.0 (TID 19). 1204 bytes result sent to driver
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 157
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 159
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 198
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 168
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 197
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 187
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 152
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 181
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 153
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 195
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 176
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 169
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 196
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 151
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 193
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 177
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 191
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 174
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 182
2021-12-07 13:36:38,325 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 175
2021-12-07 13:36:38,326 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 161
2021-12-07 13:36:38,326 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 188
2021-12-07 13:36:38,326 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 11.0 (TID 19) in 77 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:38,326 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 173
2021-12-07 13:36:38,326 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 190
2021-12-07 13:36:38,326 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 166
2021-12-07 13:36:38,326 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 178
2021-12-07 13:36:38,326 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 150
2021-12-07 13:36:38,326 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 172
2021-12-07 13:36:38,327 [dispatcher-event-loop-5] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_7_piece0 on qb:50664 in memory (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:36:38,327 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 185
2021-12-07 13:36:38,327 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 199
2021-12-07 13:36:38,327 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 179
2021-12-07 13:36:38,327 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 165
2021-12-07 13:36:38,328 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 184
2021-12-07 13:36:38,328 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 164
2021-12-07 13:36:38,328 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 183
2021-12-07 13:36:38,328 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_8_piece0 on qb:50664 in memory (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 186
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 162
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 167
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 170
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 154
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 156
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 160
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 158
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 180
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 194
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 192
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 155
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 171
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 163
2021-12-07 13:36:38,329 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 189
2021-12-07 13:36:38,330 [Executor task launch worker for task 16] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 9.0 (TID 16). 1247 bytes result sent to driver
2021-12-07 13:36:38,331 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 9.0 (TID 16) in 94 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:38,332 [Executor task launch worker for task 18] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 11.0 (TID 18). 1247 bytes result sent to driver
2021-12-07 13:36:38,332 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 11.0 (TID 18) in 84 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:38,332 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2021-12-07 13:36:38,333 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 11 (intersection at PaidPromotionAdjustParameter.scala:128) finished in 0.095 s
2021-12-07 13:36:38,333 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:38,333 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set(ShuffleMapStage 9)
2021-12-07 13:36:38,333 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 12)
2021-12-07 13:36:38,333 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:38,338 [Executor task launch worker for task 17] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 9.0 (TID 17). 1247 bytes result sent to driver
2021-12-07 13:36:38,339 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 9.0 (TID 17) in 102 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:38,339 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2021-12-07 13:36:38,339 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 9 (intersection at PaidPromotionAdjustParameter.scala:128) finished in 0.108 s
2021-12-07 13:36:38,339 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:38,339 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:36:38,339 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 12)
2021-12-07 13:36:38,339 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:38,339 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 12 (MapPartitionsRDD[18] at intersection at PaidPromotionAdjustParameter.scala:128), which has no missing parents
2021-12-07 13:36:38,342 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11 stored as values in memory (estimated size 3.6 KB, free 1990.5 MB)
2021-12-07 13:36:38,344 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.5 MB)
2021-12-07 13:36:38,345 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_11_piece0 in memory on qb:50664 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:36:38,345 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 11 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:38,346 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[18] at intersection at PaidPromotionAdjustParameter.scala:128) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:38,346 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 12.0 with 2 tasks
2021-12-07 13:36:38,347 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 12.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:38,347 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 12.0 (TID 21, localhost, executor driver, partition 1, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:38,348 [Executor task launch worker for task 21] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 12.0 (TID 21)
2021-12-07 13:36:38,348 [Executor task launch worker for task 20] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 12.0 (TID 20)
2021-12-07 13:36:38,351 [Executor task launch worker for task 20] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:38,351 [Executor task launch worker for task 21] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:38,351 [Executor task launch worker for task 20] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:38,351 [Executor task launch worker for task 21] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:38,355 [Executor task launch worker for task 20] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:38,355 [Executor task launch worker for task 21] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:38,355 [Executor task launch worker for task 20] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:38,355 [Executor task launch worker for task 21] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:38,552 [Executor task launch worker for task 20] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 12.0 (TID 20). 1054 bytes result sent to driver
2021-12-07 13:36:38,552 [Executor task launch worker for task 21] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 12.0 (TID 21). 1054 bytes result sent to driver
2021-12-07 13:36:38,553 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 12.0 (TID 20) in 207 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:38,553 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 12.0 (TID 21) in 206 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:38,553 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2021-12-07 13:36:38,554 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 12 (count at PaidPromotionAdjustParameter.scala:138) finished in 0.214 s
2021-12-07 13:36:38,554 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 6 finished: count at PaidPromotionAdjustParameter.scala:138, took 0.324566 s
2021-12-07 13:36:38,555 [main] INFO [PaidPromotion$] - 共 同 用户数 = 5196
2021-12-07 13:36:38,558 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:139
2021-12-07 13:36:38,558 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 20 (distinct at PaidPromotionAdjustParameter.scala:131)
2021-12-07 13:36:38,558 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 7 (count at PaidPromotionAdjustParameter.scala:139) with 2 output partitions
2021-12-07 13:36:38,559 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 14 (count at PaidPromotionAdjustParameter.scala:139)
2021-12-07 13:36:38,559 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 13)
2021-12-07 13:36:38,559 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 13)
2021-12-07 13:36:38,559 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 13 (MapPartitionsRDD[20] at distinct at PaidPromotionAdjustParameter.scala:131), which has no missing parents
2021-12-07 13:36:38,560 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12 stored as values in memory (estimated size 5.2 KB, free 1990.4 MB)
2021-12-07 13:36:38,563 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1990.4 MB)
2021-12-07 13:36:38,563 [dispatcher-event-loop-8] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_12_piece0 in memory on qb:50664 (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:36:38,564 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 12 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:38,564 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[20] at distinct at PaidPromotionAdjustParameter.scala:131) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:38,564 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 13.0 with 2 tasks
2021-12-07 13:36:38,565 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 13.0 (TID 22, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:36:38,565 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 13.0 (TID 23, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:36:38,565 [Executor task launch worker for task 22] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 13.0 (TID 22)
2021-12-07 13:36:38,565 [Executor task launch worker for task 23] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 13.0 (TID 23)
2021-12-07 13:36:38,567 [Executor task launch worker for task 23] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:38,567 [Executor task launch worker for task 22] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 261
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 204
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 206
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 272
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 200
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 202
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 231
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 223
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 228
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 211
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 201
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 205
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 207
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 239
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 214
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 209
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 263
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 217
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 224
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 225
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 262
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 222
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 216
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 256
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 237
2021-12-07 13:36:38,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 221
2021-12-07 13:36:38,854 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_10_piece0 on qb:50664 in memory (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:36:38,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 251
2021-12-07 13:36:38,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 274
2021-12-07 13:36:38,855 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_9_piece0 on qb:50664 in memory (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:36:38,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 260
2021-12-07 13:36:38,856 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 247
2021-12-07 13:36:38,856 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 212
2021-12-07 13:36:38,856 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 235
2021-12-07 13:36:38,856 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 230
2021-12-07 13:36:38,856 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 236
2021-12-07 13:36:38,856 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 245
2021-12-07 13:36:38,856 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 242
2021-12-07 13:36:38,856 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 258
2021-12-07 13:36:38,856 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 273
2021-12-07 13:36:38,856 [dispatcher-event-loop-8] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_11_piece0 on qb:50664 in memory (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 241
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 269
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 253
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 213
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 249
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 271
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 265
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 220
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 254
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 270
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 264
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 268
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 229
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 208
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 234
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 250
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 252
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 248
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 218
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 210
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 233
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 266
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 232
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 227
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 259
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 255
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 243
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 219
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 244
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 238
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 203
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 257
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 246
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 226
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 240
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 215
2021-12-07 13:36:38,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 267
2021-12-07 13:36:38,972 [Executor task launch worker for task 22] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 13.0 (TID 22). 1032 bytes result sent to driver
2021-12-07 13:36:38,973 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 13.0 (TID 22) in 409 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:39,985 [Executor task launch worker for task 23] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 13.0 (TID 23). 1032 bytes result sent to driver
2021-12-07 13:36:39,985 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 13.0 (TID 23) in 1420 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:39,985 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2021-12-07 13:36:39,986 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 13 (distinct at PaidPromotionAdjustParameter.scala:131) finished in 1.426 s
2021-12-07 13:36:39,986 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:39,986 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:36:39,986 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 14)
2021-12-07 13:36:39,986 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:39,986 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 14 (MapPartitionsRDD[22] at distinct at PaidPromotionAdjustParameter.scala:131), which has no missing parents
2021-12-07 13:36:39,987 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13 stored as values in memory (estimated size 3.7 KB, free 1990.5 MB)
2021-12-07 13:36:39,989 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.5 MB)
2021-12-07 13:36:39,989 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_13_piece0 in memory on qb:50664 (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:36:39,990 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 13 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:39,990 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[22] at distinct at PaidPromotionAdjustParameter.scala:131) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:39,990 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 14.0 with 2 tasks
2021-12-07 13:36:39,991 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 14.0 (TID 24, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:36:39,991 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 14.0 (TID 25, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:36:39,991 [Executor task launch worker for task 25] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 14.0 (TID 25)
2021-12-07 13:36:39,991 [Executor task launch worker for task 24] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 14.0 (TID 24)
2021-12-07 13:36:39,992 [Executor task launch worker for task 25] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:39,992 [Executor task launch worker for task 24] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:39,992 [Executor task launch worker for task 25] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:39,992 [Executor task launch worker for task 24] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,007 [Executor task launch worker for task 24] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 14.0 (TID 24). 1010 bytes result sent to driver
2021-12-07 13:36:40,007 [Executor task launch worker for task 25] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 14.0 (TID 25). 1010 bytes result sent to driver
2021-12-07 13:36:40,008 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 14.0 (TID 24) in 18 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:40,008 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 14.0 (TID 25) in 17 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:40,008 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2021-12-07 13:36:40,008 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 14 (count at PaidPromotionAdjustParameter.scala:139) finished in 0.022 s
2021-12-07 13:36:40,009 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 7 finished: count at PaidPromotionAdjustParameter.scala:139, took 1.450622 s
2021-12-07 13:36:40,009 [main] INFO [PaidPromotion$] - 训练集节目数 = 122
2021-12-07 13:36:40,011 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:140
2021-12-07 13:36:40,012 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 24 (distinct at PaidPromotionAdjustParameter.scala:132)
2021-12-07 13:36:40,012 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 8 (count at PaidPromotionAdjustParameter.scala:140) with 2 output partitions
2021-12-07 13:36:40,012 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 16 (count at PaidPromotionAdjustParameter.scala:140)
2021-12-07 13:36:40,012 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 15)
2021-12-07 13:36:40,012 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 15)
2021-12-07 13:36:40,012 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 15 (MapPartitionsRDD[24] at distinct at PaidPromotionAdjustParameter.scala:132), which has no missing parents
2021-12-07 13:36:40,013 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14 stored as values in memory (estimated size 5.2 KB, free 1990.4 MB)
2021-12-07 13:36:40,016 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1990.4 MB)
2021-12-07 13:36:40,016 [dispatcher-event-loop-5] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_14_piece0 in memory on qb:50664 (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:36:40,017 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 14 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:40,017 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[24] at distinct at PaidPromotionAdjustParameter.scala:132) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:40,017 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 15.0 with 2 tasks
2021-12-07 13:36:40,018 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 15.0 (TID 26, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:36:40,018 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 15.0 (TID 27, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:36:40,018 [Executor task launch worker for task 26] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 15.0 (TID 26)
2021-12-07 13:36:40,018 [Executor task launch worker for task 27] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 15.0 (TID 27)
2021-12-07 13:36:40,020 [Executor task launch worker for task 27] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:40,020 [Executor task launch worker for task 26] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:40,415 [Executor task launch worker for task 26] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 15.0 (TID 26). 989 bytes result sent to driver
2021-12-07 13:36:40,416 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 15.0 (TID 26) in 398 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:40,594 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 285
2021-12-07 13:36:40,594 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 304
2021-12-07 13:36:40,594 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 276
2021-12-07 13:36:40,594 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 281
2021-12-07 13:36:40,594 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 303
2021-12-07 13:36:40,594 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 319
2021-12-07 13:36:40,594 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 322
2021-12-07 13:36:40,594 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 324
2021-12-07 13:36:40,595 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_12_piece0 on qb:50664 in memory (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 279
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 299
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 312
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 314
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 296
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 316
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 284
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 313
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 307
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 320
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 311
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 309
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 298
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 301
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 308
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 317
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 288
2021-12-07 13:36:40,596 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 297
2021-12-07 13:36:40,596 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_13_piece0 on qb:50664 in memory (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 305
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 283
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 294
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 310
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 291
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 275
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 282
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 292
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 306
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 323
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 277
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 300
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 287
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 278
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 293
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 286
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 318
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 295
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 302
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 315
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 321
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 289
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 280
2021-12-07 13:36:40,597 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 290
2021-12-07 13:36:40,802 [Executor task launch worker for task 27] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 15.0 (TID 27). 989 bytes result sent to driver
2021-12-07 13:36:40,803 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 15.0 (TID 27) in 785 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:40,803 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2021-12-07 13:36:40,803 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 15 (distinct at PaidPromotionAdjustParameter.scala:132) finished in 0.790 s
2021-12-07 13:36:40,803 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:40,803 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:36:40,803 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 16)
2021-12-07 13:36:40,803 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:40,804 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 16 (MapPartitionsRDD[26] at distinct at PaidPromotionAdjustParameter.scala:132), which has no missing parents
2021-12-07 13:36:40,804 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15 stored as values in memory (estimated size 3.7 KB, free 1990.5 MB)
2021-12-07 13:36:40,807 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.5 MB)
2021-12-07 13:36:40,807 [dispatcher-event-loop-5] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_15_piece0 in memory on qb:50664 (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:36:40,807 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 15 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:40,808 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 16 (MapPartitionsRDD[26] at distinct at PaidPromotionAdjustParameter.scala:132) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:40,808 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 16.0 with 2 tasks
2021-12-07 13:36:40,808 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 16.0 (TID 28, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:36:40,808 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 16.0 (TID 29, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:36:40,808 [Executor task launch worker for task 28] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 16.0 (TID 28)
2021-12-07 13:36:40,808 [Executor task launch worker for task 29] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 16.0 (TID 29)
2021-12-07 13:36:40,810 [Executor task launch worker for task 28] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,810 [Executor task launch worker for task 29] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,810 [Executor task launch worker for task 28] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,810 [Executor task launch worker for task 29] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,823 [Executor task launch worker for task 28] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 16.0 (TID 28). 1053 bytes result sent to driver
2021-12-07 13:36:40,823 [Executor task launch worker for task 29] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 16.0 (TID 29). 1053 bytes result sent to driver
2021-12-07 13:36:40,823 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 16.0 (TID 28) in 15 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:40,823 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 16.0 (TID 29) in 15 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:40,823 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2021-12-07 13:36:40,824 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 16 (count at PaidPromotionAdjustParameter.scala:140) finished in 0.020 s
2021-12-07 13:36:40,824 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 8 finished: count at PaidPromotionAdjustParameter.scala:140, took 0.812498 s
2021-12-07 13:36:40,824 [main] INFO [PaidPromotion$] - 验证集节目数 = 125
2021-12-07 13:36:40,827 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:141
2021-12-07 13:36:40,827 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 28 (intersection at PaidPromotionAdjustParameter.scala:133)
2021-12-07 13:36:40,827 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 27 (intersection at PaidPromotionAdjustParameter.scala:133)
2021-12-07 13:36:40,827 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 9 (count at PaidPromotionAdjustParameter.scala:141) with 2 output partitions
2021-12-07 13:36:40,827 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 21 (count at PaidPromotionAdjustParameter.scala:141)
2021-12-07 13:36:40,827 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 20, ShuffleMapStage 18)
2021-12-07 13:36:40,827 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 20, ShuffleMapStage 18)
2021-12-07 13:36:40,828 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 18 (MapPartitionsRDD[28] at intersection at PaidPromotionAdjustParameter.scala:133), which has no missing parents
2021-12-07 13:36:40,828 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16 stored as values in memory (estimated size 4.0 KB, free 1990.5 MB)
2021-12-07 13:36:40,830 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.4 MB)
2021-12-07 13:36:40,831 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_16_piece0 in memory on qb:50664 (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:36:40,831 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 16 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:40,835 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[28] at intersection at PaidPromotionAdjustParameter.scala:133) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:40,835 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 18.0 with 2 tasks
2021-12-07 13:36:40,836 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 20 (MapPartitionsRDD[27] at intersection at PaidPromotionAdjustParameter.scala:133), which has no missing parents
2021-12-07 13:36:40,836 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 18.0 (TID 30, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:36:40,836 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 18.0 (TID 31, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:36:40,836 [Executor task launch worker for task 30] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 18.0 (TID 30)
2021-12-07 13:36:40,836 [Executor task launch worker for task 31] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 18.0 (TID 31)
2021-12-07 13:36:40,837 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17 stored as values in memory (estimated size 4.0 KB, free 1990.4 MB)
2021-12-07 13:36:40,837 [Executor task launch worker for task 31] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,837 [Executor task launch worker for task 30] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,837 [Executor task launch worker for task 31] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,837 [Executor task launch worker for task 30] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,838 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.4 MB)
2021-12-07 13:36:40,839 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_17_piece0 in memory on qb:50664 (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:36:40,839 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 17 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:40,839 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[27] at intersection at PaidPromotionAdjustParameter.scala:133) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:40,839 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 20.0 with 2 tasks
2021-12-07 13:36:40,840 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 20.0 (TID 32, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:36:40,840 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 20.0 (TID 33, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:36:40,840 [Executor task launch worker for task 32] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 20.0 (TID 32)
2021-12-07 13:36:40,840 [Executor task launch worker for task 33] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 20.0 (TID 33)
2021-12-07 13:36:40,841 [Executor task launch worker for task 32] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,841 [Executor task launch worker for task 33] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,841 [Executor task launch worker for task 32] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,841 [Executor task launch worker for task 33] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,855 [Executor task launch worker for task 31] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 18.0 (TID 31). 1161 bytes result sent to driver
2021-12-07 13:36:40,855 [Executor task launch worker for task 30] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 18.0 (TID 30). 1161 bytes result sent to driver
2021-12-07 13:36:40,856 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 18.0 (TID 31) in 20 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:40,856 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 18.0 (TID 30) in 20 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:40,856 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2021-12-07 13:36:40,856 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 18 (intersection at PaidPromotionAdjustParameter.scala:133) finished in 0.028 s
2021-12-07 13:36:40,856 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:40,856 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set(ShuffleMapStage 20)
2021-12-07 13:36:40,856 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 21)
2021-12-07 13:36:40,856 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:40,858 [Executor task launch worker for task 33] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 20.0 (TID 33). 1161 bytes result sent to driver
2021-12-07 13:36:40,859 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 20.0 (TID 33) in 19 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:40,859 [Executor task launch worker for task 32] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 20.0 (TID 32). 1161 bytes result sent to driver
2021-12-07 13:36:40,859 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 20.0 (TID 32) in 19 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:40,859 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 20.0, whose tasks have all completed, from pool 
2021-12-07 13:36:40,859 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 20 (intersection at PaidPromotionAdjustParameter.scala:133) finished in 0.023 s
2021-12-07 13:36:40,859 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:40,859 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:36:40,859 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 21)
2021-12-07 13:36:40,859 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:40,860 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 21 (MapPartitionsRDD[32] at intersection at PaidPromotionAdjustParameter.scala:133), which has no missing parents
2021-12-07 13:36:40,860 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18 stored as values in memory (estimated size 3.6 KB, free 1990.4 MB)
2021-12-07 13:36:40,862 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.4 MB)
2021-12-07 13:36:40,863 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_18_piece0 in memory on qb:50664 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:36:40,863 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 18 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:40,863 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 21 (MapPartitionsRDD[32] at intersection at PaidPromotionAdjustParameter.scala:133) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:40,863 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 21.0 with 2 tasks
2021-12-07 13:36:40,863 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 21.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:40,864 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 21.0 (TID 35, localhost, executor driver, partition 1, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:40,864 [Executor task launch worker for task 35] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 21.0 (TID 35)
2021-12-07 13:36:40,864 [Executor task launch worker for task 34] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 21.0 (TID 34)
2021-12-07 13:36:40,865 [Executor task launch worker for task 34] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,865 [Executor task launch worker for task 35] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,865 [Executor task launch worker for task 34] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,865 [Executor task launch worker for task 35] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,867 [Executor task launch worker for task 34] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,868 [Executor task launch worker for task 35] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,868 [Executor task launch worker for task 34] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:36:40,868 [Executor task launch worker for task 35] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:36:40,875 [Executor task launch worker for task 35] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 21.0 (TID 35). 1010 bytes result sent to driver
2021-12-07 13:36:40,875 [Executor task launch worker for task 34] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 21.0 (TID 34). 1053 bytes result sent to driver
2021-12-07 13:36:40,875 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 21.0 (TID 35) in 12 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:40,875 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 21.0 (TID 34) in 12 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:40,875 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 21.0, whose tasks have all completed, from pool 
2021-12-07 13:36:40,875 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 21 (count at PaidPromotionAdjustParameter.scala:141) finished in 0.015 s
2021-12-07 13:36:40,876 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 9 finished: count at PaidPromotionAdjustParameter.scala:141, took 0.048948 s
2021-12-07 13:36:40,876 [main] INFO [PaidPromotion$] - 共 同 节目数 = 115
2021-12-07 13:36:40,884 [main] INFO [org.apache.spark.SparkContext] - Starting job: collect at PaidPromotionAdjustParameter.scala:144
2021-12-07 13:36:40,885 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 10 (collect at PaidPromotionAdjustParameter.scala:144) with 2 output partitions
2021-12-07 13:36:40,885 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 26 (collect at PaidPromotionAdjustParameter.scala:144)
2021-12-07 13:36:40,885 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 25, ShuffleMapStage 23)
2021-12-07 13:36:40,885 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:36:40,885 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 26 (MapPartitionsRDD[18] at intersection at PaidPromotionAdjustParameter.scala:128), which has no missing parents
2021-12-07 13:36:40,886 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19 stored as values in memory (estimated size 3.8 KB, free 1990.4 MB)
2021-12-07 13:36:40,888 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.4 MB)
2021-12-07 13:36:40,889 [dispatcher-event-loop-8] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_19_piece0 in memory on qb:50664 (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:36:40,889 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 19 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:40,889 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 26 (MapPartitionsRDD[18] at intersection at PaidPromotionAdjustParameter.scala:128) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:40,889 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 26.0 with 2 tasks
2021-12-07 13:36:40,890 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 26.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:40,890 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 26.0 (TID 37, localhost, executor driver, partition 1, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:40,890 [Executor task launch worker for task 36] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 26.0 (TID 36)
2021-12-07 13:36:40,890 [Executor task launch worker for task 37] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 26.0 (TID 37)
2021-12-07 13:36:40,891 [Executor task launch worker for task 37] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,891 [Executor task launch worker for task 36] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,891 [Executor task launch worker for task 37] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,891 [Executor task launch worker for task 36] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,893 [Executor task launch worker for task 36] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,893 [Executor task launch worker for task 36] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,893 [Executor task launch worker for task 37] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,893 [Executor task launch worker for task 37] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,974 [Executor task launch worker for task 37] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 26.0 (TID 37). 88113 bytes result sent to driver
2021-12-07 13:36:40,976 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 26.0 (TID 37) in 86 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:40,977 [Executor task launch worker for task 36] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 26.0 (TID 36). 86387 bytes result sent to driver
2021-12-07 13:36:40,978 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 26.0 (TID 36) in 88 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:40,978 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 26.0, whose tasks have all completed, from pool 
2021-12-07 13:36:40,979 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 26 (collect at PaidPromotionAdjustParameter.scala:144) finished in 0.094 s
2021-12-07 13:36:40,979 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 10 finished: collect at PaidPromotionAdjustParameter.scala:144, took 0.094526 s
2021-12-07 13:36:40,985 [main] INFO [org.apache.spark.SparkContext] - Starting job: collect at PaidPromotionAdjustParameter.scala:145
2021-12-07 13:36:40,986 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 11 (collect at PaidPromotionAdjustParameter.scala:145) with 2 output partitions
2021-12-07 13:36:40,986 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 31 (collect at PaidPromotionAdjustParameter.scala:145)
2021-12-07 13:36:40,986 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 30, ShuffleMapStage 28)
2021-12-07 13:36:40,986 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:36:40,986 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 31 (MapPartitionsRDD[32] at intersection at PaidPromotionAdjustParameter.scala:133), which has no missing parents
2021-12-07 13:36:40,987 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20 stored as values in memory (estimated size 3.8 KB, free 1990.4 MB)
2021-12-07 13:36:40,993 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 336
2021-12-07 13:36:40,993 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 430
2021-12-07 13:36:40,993 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 464
2021-12-07 13:36:40,993 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 407
2021-12-07 13:36:40,993 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 414
2021-12-07 13:36:40,993 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 473
2021-12-07 13:36:40,993 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 384
2021-12-07 13:36:40,993 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 455
2021-12-07 13:36:40,993 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 428
2021-12-07 13:36:40,993 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 468
2021-12-07 13:36:40,994 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 397
2021-12-07 13:36:40,994 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 332
2021-12-07 13:36:40,994 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 389
2021-12-07 13:36:40,994 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 423
2021-12-07 13:36:40,994 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.4 MB)
2021-12-07 13:36:40,994 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_20_piece0 in memory on qb:50664 (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:36:40,994 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_19_piece0 on qb:50664 in memory (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:36:40,994 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 20 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:40,995 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 436
2021-12-07 13:36:40,995 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 31 (MapPartitionsRDD[32] at intersection at PaidPromotionAdjustParameter.scala:133) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:40,995 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 31.0 with 2 tasks
2021-12-07 13:36:40,995 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 460
2021-12-07 13:36:40,995 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 415
2021-12-07 13:36:40,995 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 376
2021-12-07 13:36:40,995 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 472
2021-12-07 13:36:40,995 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 443
2021-12-07 13:36:40,995 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 344
2021-12-07 13:36:40,995 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 419
2021-12-07 13:36:40,995 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 369
2021-12-07 13:36:40,995 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 400
2021-12-07 13:36:40,995 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 367
2021-12-07 13:36:40,995 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 349
2021-12-07 13:36:40,996 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 31.0 (TID 38, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:40,996 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 31.0 (TID 39, localhost, executor driver, partition 1, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:40,996 [Executor task launch worker for task 38] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 31.0 (TID 38)
2021-12-07 13:36:40,996 [Executor task launch worker for task 39] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 31.0 (TID 39)
2021-12-07 13:36:40,996 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_15_piece0 on qb:50664 in memory (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:36:40,996 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 377
2021-12-07 13:36:40,996 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 380
2021-12-07 13:36:40,996 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 398
2021-12-07 13:36:40,997 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 448
2021-12-07 13:36:40,997 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 446
2021-12-07 13:36:40,997 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 337
2021-12-07 13:36:40,997 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 351
2021-12-07 13:36:40,997 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 386
2021-12-07 13:36:40,997 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 405
2021-12-07 13:36:40,997 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 387
2021-12-07 13:36:40,997 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 404
2021-12-07 13:36:40,997 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 327
2021-12-07 13:36:40,997 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 470
2021-12-07 13:36:40,997 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 361
2021-12-07 13:36:40,997 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 463
2021-12-07 13:36:40,997 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 467
2021-12-07 13:36:40,997 [Executor task launch worker for task 38] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,997 [Executor task launch worker for task 39] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,997 [Executor task launch worker for task 38] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,997 [Executor task launch worker for task 39] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,998 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_14_piece0 on qb:50664 in memory (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:36:40,998 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 433
2021-12-07 13:36:40,998 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 371
2021-12-07 13:36:40,998 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 364
2021-12-07 13:36:40,998 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 375
2021-12-07 13:36:40,998 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 379
2021-12-07 13:36:40,998 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 434
2021-12-07 13:36:40,998 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 341
2021-12-07 13:36:40,998 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 431
2021-12-07 13:36:40,998 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 365
2021-12-07 13:36:40,998 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 333
2021-12-07 13:36:40,999 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_16_piece0 on qb:50664 in memory (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:36:40,999 [Executor task launch worker for task 38] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,999 [Executor task launch worker for task 39] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:40,999 [Executor task launch worker for task 38] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,999 [Executor task launch worker for task 39] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:40,999 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 418
2021-12-07 13:36:41,000 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 370
2021-12-07 13:36:41,000 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 427
2021-12-07 13:36:41,000 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 438
2021-12-07 13:36:41,000 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 402
2021-12-07 13:36:41,000 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 338
2021-12-07 13:36:41,000 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 343
2021-12-07 13:36:41,000 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 441
2021-12-07 13:36:41,000 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 340
2021-12-07 13:36:41,000 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 350
2021-12-07 13:36:41,000 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 359
2021-12-07 13:36:41,000 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 362
2021-12-07 13:36:41,000 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 373
2021-12-07 13:36:41,001 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_17_piece0 on qb:50664 in memory (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 406
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 417
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 444
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 462
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 449
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 459
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 420
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 345
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 457
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 451
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 432
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 391
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 412
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 381
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 357
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 339
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 403
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 409
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 411
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 334
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 393
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 331
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 335
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 447
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 346
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 366
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 374
2021-12-07 13:36:41,001 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 456
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 347
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 353
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 388
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 325
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 469
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 382
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 392
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 429
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 426
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 358
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 410
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 326
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 458
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 466
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 435
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 408
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 450
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 445
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 416
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 348
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 471
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 328
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 461
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 401
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 465
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 439
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 354
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 372
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 442
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 329
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 452
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 394
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 425
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 352
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 342
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 363
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 413
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 383
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 356
2021-12-07 13:36:41,002 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 453
2021-12-07 13:36:41,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 368
2021-12-07 13:36:41,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 330
2021-12-07 13:36:41,003 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_18_piece0 on qb:50664 in memory (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:36:41,004 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 390
2021-12-07 13:36:41,004 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 474
2021-12-07 13:36:41,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 422
2021-12-07 13:36:41,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 437
2021-12-07 13:36:41,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 399
2021-12-07 13:36:41,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 440
2021-12-07 13:36:41,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 454
2021-12-07 13:36:41,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 424
2021-12-07 13:36:41,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 395
2021-12-07 13:36:41,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 360
2021-12-07 13:36:41,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 355
2021-12-07 13:36:41,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 385
2021-12-07 13:36:41,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 421
2021-12-07 13:36:41,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 378
2021-12-07 13:36:41,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 396
2021-12-07 13:36:41,008 [Executor task launch worker for task 38] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 31.0 (TID 38). 2872 bytes result sent to driver
2021-12-07 13:36:41,008 [Executor task launch worker for task 39] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 31.0 (TID 39). 2668 bytes result sent to driver
2021-12-07 13:36:41,009 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 31.0 (TID 38) in 14 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:41,009 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 31.0 (TID 39) in 13 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:41,009 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 31.0, whose tasks have all completed, from pool 
2021-12-07 13:36:41,009 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 31 (collect at PaidPromotionAdjustParameter.scala:145) finished in 0.023 s
2021-12-07 13:36:41,010 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 11 finished: collect at PaidPromotionAdjustParameter.scala:145, took 0.024622 s
2021-12-07 13:36:41,533 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:154
2021-12-07 13:36:41,535 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 12 (count at PaidPromotionAdjustParameter.scala:154) with 4 output partitions
2021-12-07 13:36:41,535 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 32 (count at PaidPromotionAdjustParameter.scala:154)
2021-12-07 13:36:41,535 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:36:41,535 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:36:41,535 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 32 (UnionRDD[35] at union at PaidPromotionAdjustParameter.scala:152), which has no missing parents
2021-12-07 13:36:41,545 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21 stored as values in memory (estimated size 185.5 KB, free 1990.3 MB)
2021-12-07 13:36:41,548 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21_piece0 stored as bytes in memory (estimated size 65.8 KB, free 1990.2 MB)
2021-12-07 13:36:41,548 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_21_piece0 in memory on qb:50664 (size: 65.8 KB, free: 1990.7 MB)
2021-12-07 13:36:41,549 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 21 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:41,549 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ResultStage 32 (UnionRDD[35] at union at PaidPromotionAdjustParameter.scala:152) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:36:41,549 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 32.0 with 4 tasks
2021-12-07 13:36:41,551 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 32.0 (TID 40, localhost, executor driver, partition 0, ANY, 8005 bytes)
2021-12-07 13:36:41,551 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 32.0 (TID 41, localhost, executor driver, partition 1, ANY, 8005 bytes)
2021-12-07 13:36:41,551 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 32.0 (TID 42, localhost, executor driver, partition 2, ANY, 8005 bytes)
2021-12-07 13:36:41,552 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 32.0 (TID 43, localhost, executor driver, partition 3, ANY, 8005 bytes)
2021-12-07 13:36:41,552 [Executor task launch worker for task 40] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 32.0 (TID 40)
2021-12-07 13:36:41,552 [Executor task launch worker for task 41] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 32.0 (TID 41)
2021-12-07 13:36:41,552 [Executor task launch worker for task 43] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 32.0 (TID 43)
2021-12-07 13:36:41,552 [Executor task launch worker for task 42] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 32.0 (TID 42)
2021-12-07 13:36:41,556 [Executor task launch worker for task 43] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:41,556 [Executor task launch worker for task 41] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:41,557 [Executor task launch worker for task 42] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:41,557 [Executor task launch worker for task 40] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:42,995 [Executor task launch worker for task 41] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 32.0 (TID 41). 711 bytes result sent to driver
2021-12-07 13:36:42,996 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 32.0 (TID 41) in 1445 ms on localhost (executor driver) (1/4)
2021-12-07 13:36:44,317 [Executor task launch worker for task 42] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 32.0 (TID 42). 711 bytes result sent to driver
2021-12-07 13:36:44,318 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 32.0 (TID 42) in 2767 ms on localhost (executor driver) (2/4)
2021-12-07 13:36:45,564 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 481
2021-12-07 13:36:45,564 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 475
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 489
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 493
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 492
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 480
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 496
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 497
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 476
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 484
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 495
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 486
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 485
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 477
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 499
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 498
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 491
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 487
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 488
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 494
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 479
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 482
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 478
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 483
2021-12-07 13:36:45,565 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 490
2021-12-07 13:36:45,566 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_20_piece0 on qb:50664 in memory (size: 2.2 KB, free: 1990.7 MB)
2021-12-07 13:36:46,160 [Executor task launch worker for task 43] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 32.0 (TID 43). 754 bytes result sent to driver
2021-12-07 13:36:46,160 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 32.0 (TID 43) in 4608 ms on localhost (executor driver) (3/4)
2021-12-07 13:36:46,276 [Executor task launch worker for task 40] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 32.0 (TID 40). 754 bytes result sent to driver
2021-12-07 13:36:46,277 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 32.0 (TID 40) in 4727 ms on localhost (executor driver) (4/4)
2021-12-07 13:36:46,277 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 32.0, whose tasks have all completed, from pool 
2021-12-07 13:36:46,277 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 32 (count at PaidPromotionAdjustParameter.scala:154) finished in 4.741 s
2021-12-07 13:36:46,277 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 12 finished: count at PaidPromotionAdjustParameter.scala:154, took 4.744365 s
2021-12-07 13:36:46,277 [main] INFO [PaidPromotion$] - 最终训练集数量：100905
2021-12-07 13:36:46,279 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:155
2021-12-07 13:36:46,280 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 13 (count at PaidPromotionAdjustParameter.scala:155) with 2 output partitions
2021-12-07 13:36:46,280 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 33 (count at PaidPromotionAdjustParameter.scala:155)
2021-12-07 13:36:46,280 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:36:46,280 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:36:46,280 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 33 (MapPartitionsRDD[33] at filter at PaidPromotionAdjustParameter.scala:148), which has no missing parents
2021-12-07 13:36:46,282 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22 stored as values in memory (estimated size 185.0 KB, free 1990.0 MB)
2021-12-07 13:36:46,285 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22_piece0 stored as bytes in memory (estimated size 65.5 KB, free 1990.0 MB)
2021-12-07 13:36:46,285 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_22_piece0 in memory on qb:50664 (size: 65.5 KB, free: 1990.6 MB)
2021-12-07 13:36:46,285 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 22 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:46,286 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 33 (MapPartitionsRDD[33] at filter at PaidPromotionAdjustParameter.scala:148) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:46,286 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 33.0 with 2 tasks
2021-12-07 13:36:46,286 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 33.0 (TID 44, localhost, executor driver, partition 0, ANY, 7896 bytes)
2021-12-07 13:36:46,286 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 33.0 (TID 45, localhost, executor driver, partition 1, ANY, 7896 bytes)
2021-12-07 13:36:46,286 [Executor task launch worker for task 44] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 33.0 (TID 44)
2021-12-07 13:36:46,286 [Executor task launch worker for task 45] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 33.0 (TID 45)
2021-12-07 13:36:46,289 [Executor task launch worker for task 44] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:46,289 [Executor task launch worker for task 45] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:47,656 [Executor task launch worker for task 44] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 33.0 (TID 44). 710 bytes result sent to driver
2021-12-07 13:36:47,657 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 33.0 (TID 44) in 1371 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:47,664 [Executor task launch worker for task 45] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 33.0 (TID 45). 710 bytes result sent to driver
2021-12-07 13:36:47,664 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 33.0 (TID 45) in 1378 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:47,664 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 33.0, whose tasks have all completed, from pool 
2021-12-07 13:36:47,664 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 33 (count at PaidPromotionAdjustParameter.scala:155) finished in 1.383 s
2021-12-07 13:36:47,664 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 13 finished: count at PaidPromotionAdjustParameter.scala:155, took 1.383894 s
2021-12-07 13:36:47,664 [main] INFO [PaidPromotion$] - 最终验证集数量：6389
2021-12-07 13:36:47,862 [main] INFO [PaidPromotion$] - -----------------------------------
2021-12-07 13:36:47,863 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:167
2021-12-07 13:36:47,863 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 37 (distinct at PaidPromotionAdjustParameter.scala:158)
2021-12-07 13:36:47,863 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 14 (count at PaidPromotionAdjustParameter.scala:167) with 4 output partitions
2021-12-07 13:36:47,863 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 35 (count at PaidPromotionAdjustParameter.scala:167)
2021-12-07 13:36:47,863 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 34)
2021-12-07 13:36:47,863 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 34)
2021-12-07 13:36:47,864 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 34 (MapPartitionsRDD[37] at distinct at PaidPromotionAdjustParameter.scala:158), which has no missing parents
2021-12-07 13:36:47,866 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23 stored as values in memory (estimated size 187.1 KB, free 1989.8 MB)
2021-12-07 13:36:47,868 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23_piece0 stored as bytes in memory (estimated size 66.8 KB, free 1989.7 MB)
2021-12-07 13:36:47,868 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_23_piece0 in memory on qb:50664 (size: 66.8 KB, free: 1990.6 MB)
2021-12-07 13:36:47,869 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 23 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:47,869 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[37] at distinct at PaidPromotionAdjustParameter.scala:158) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:36:47,869 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 34.0 with 4 tasks
2021-12-07 13:36:47,869 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 34.0 (TID 46, localhost, executor driver, partition 0, ANY, 7994 bytes)
2021-12-07 13:36:47,869 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 34.0 (TID 47, localhost, executor driver, partition 1, ANY, 7994 bytes)
2021-12-07 13:36:47,869 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 34.0 (TID 48, localhost, executor driver, partition 2, ANY, 7994 bytes)
2021-12-07 13:36:47,870 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 34.0 (TID 49, localhost, executor driver, partition 3, ANY, 7994 bytes)
2021-12-07 13:36:47,870 [Executor task launch worker for task 48] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 34.0 (TID 48)
2021-12-07 13:36:47,870 [Executor task launch worker for task 47] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 34.0 (TID 47)
2021-12-07 13:36:47,870 [Executor task launch worker for task 46] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 34.0 (TID 46)
2021-12-07 13:36:47,870 [Executor task launch worker for task 49] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 34.0 (TID 49)
2021-12-07 13:36:47,873 [Executor task launch worker for task 46] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:47,873 [Executor task launch worker for task 47] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:47,873 [Executor task launch worker for task 49] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:47,873 [Executor task launch worker for task 48] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 537
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 540
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 530
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 520
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 501
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 536
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 504
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 532
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 529
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 503
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 535
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 522
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 508
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 538
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 510
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 548
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 511
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 506
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 525
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 534
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 527
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 539
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 543
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 517
2021-12-07 13:36:48,117 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 524
2021-12-07 13:36:48,118 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 549
2021-12-07 13:36:48,118 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 545
2021-12-07 13:36:48,118 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 516
2021-12-07 13:36:48,118 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 526
2021-12-07 13:36:48,118 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 523
2021-12-07 13:36:48,118 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 521
2021-12-07 13:36:48,118 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 505
2021-12-07 13:36:48,118 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 515
2021-12-07 13:36:48,118 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 547
2021-12-07 13:36:48,118 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 500
2021-12-07 13:36:48,118 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 507
2021-12-07 13:36:48,118 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 528
2021-12-07 13:36:48,119 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_21_piece0 on qb:50664 in memory (size: 65.8 KB, free: 1990.6 MB)
2021-12-07 13:36:48,120 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 531
2021-12-07 13:36:48,120 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 541
2021-12-07 13:36:48,120 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 513
2021-12-07 13:36:48,120 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 514
2021-12-07 13:36:48,121 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_22_piece0 on qb:50664 in memory (size: 65.5 KB, free: 1990.7 MB)
2021-12-07 13:36:48,122 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 533
2021-12-07 13:36:48,122 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 518
2021-12-07 13:36:48,122 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 512
2021-12-07 13:36:48,122 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 542
2021-12-07 13:36:48,122 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 519
2021-12-07 13:36:48,122 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 509
2021-12-07 13:36:48,122 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 544
2021-12-07 13:36:48,122 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 546
2021-12-07 13:36:48,122 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 502
2021-12-07 13:36:49,176 [Executor task launch worker for task 46] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 34.0 (TID 46). 1120 bytes result sent to driver
2021-12-07 13:36:49,176 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 34.0 (TID 46) in 1307 ms on localhost (executor driver) (1/4)
2021-12-07 13:36:49,237 [Executor task launch worker for task 47] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 34.0 (TID 47). 1077 bytes result sent to driver
2021-12-07 13:36:49,237 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 34.0 (TID 47) in 1368 ms on localhost (executor driver) (2/4)
2021-12-07 13:36:49,257 [Executor task launch worker for task 48] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 34.0 (TID 48). 1077 bytes result sent to driver
2021-12-07 13:36:49,257 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 34.0 (TID 48) in 1388 ms on localhost (executor driver) (3/4)
2021-12-07 13:36:49,771 [Executor task launch worker for task 49] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 34.0 (TID 49). 1077 bytes result sent to driver
2021-12-07 13:36:49,771 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 34.0 (TID 49) in 1902 ms on localhost (executor driver) (4/4)
2021-12-07 13:36:49,771 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 34.0, whose tasks have all completed, from pool 
2021-12-07 13:36:49,771 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 34 (distinct at PaidPromotionAdjustParameter.scala:158) finished in 1.907 s
2021-12-07 13:36:49,771 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:49,771 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:36:49,771 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 35)
2021-12-07 13:36:49,771 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:49,772 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 35 (MapPartitionsRDD[39] at distinct at PaidPromotionAdjustParameter.scala:158), which has no missing parents
2021-12-07 13:36:49,772 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24 stored as values in memory (estimated size 3.7 KB, free 1990.2 MB)
2021-12-07 13:36:49,774 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.2 MB)
2021-12-07 13:36:49,775 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_24_piece0 in memory on qb:50664 (size: 2.2 KB, free: 1990.7 MB)
2021-12-07 13:36:49,775 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 24 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:49,775 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ResultStage 35 (MapPartitionsRDD[39] at distinct at PaidPromotionAdjustParameter.scala:158) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:36:49,775 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 35.0 with 4 tasks
2021-12-07 13:36:49,776 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 35.0 (TID 50, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:36:49,776 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 35.0 (TID 51, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:36:49,776 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 35.0 (TID 52, localhost, executor driver, partition 2, ANY, 7649 bytes)
2021-12-07 13:36:49,776 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 35.0 (TID 53, localhost, executor driver, partition 3, ANY, 7649 bytes)
2021-12-07 13:36:49,776 [Executor task launch worker for task 50] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 35.0 (TID 50)
2021-12-07 13:36:49,776 [Executor task launch worker for task 51] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 35.0 (TID 51)
2021-12-07 13:36:49,776 [Executor task launch worker for task 53] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 35.0 (TID 53)
2021-12-07 13:36:49,776 [Executor task launch worker for task 52] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 35.0 (TID 52)
2021-12-07 13:36:49,782 [Executor task launch worker for task 51] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:49,782 [Executor task launch worker for task 51] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:49,784 [Executor task launch worker for task 50] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:49,784 [Executor task launch worker for task 50] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:49,791 [Executor task launch worker for task 53] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:49,791 [Executor task launch worker for task 52] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:49,791 [Executor task launch worker for task 53] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:49,791 [Executor task launch worker for task 52] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:49,832 [Executor task launch worker for task 52] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 35.0 (TID 52). 1055 bytes result sent to driver
2021-12-07 13:36:49,832 [Executor task launch worker for task 51] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 35.0 (TID 51). 1098 bytes result sent to driver
2021-12-07 13:36:49,832 [Executor task launch worker for task 50] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 35.0 (TID 50). 1055 bytes result sent to driver
2021-12-07 13:36:49,832 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 35.0 (TID 52) in 56 ms on localhost (executor driver) (1/4)
2021-12-07 13:36:49,832 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 35.0 (TID 51) in 56 ms on localhost (executor driver) (2/4)
2021-12-07 13:36:49,832 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 35.0 (TID 50) in 56 ms on localhost (executor driver) (3/4)
2021-12-07 13:36:49,835 [Executor task launch worker for task 53] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 35.0 (TID 53). 1055 bytes result sent to driver
2021-12-07 13:36:49,835 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 35.0 (TID 53) in 59 ms on localhost (executor driver) (4/4)
2021-12-07 13:36:49,835 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 35.0, whose tasks have all completed, from pool 
2021-12-07 13:36:49,835 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 35 (count at PaidPromotionAdjustParameter.scala:167) finished in 0.063 s
2021-12-07 13:36:49,835 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 14 finished: count at PaidPromotionAdjustParameter.scala:167, took 1.972458 s
2021-12-07 13:36:49,836 [main] INFO [PaidPromotion$] - 最终训练集用户数 = 95323
2021-12-07 13:36:49,838 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:168
2021-12-07 13:36:49,839 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 41 (distinct at PaidPromotionAdjustParameter.scala:159)
2021-12-07 13:36:49,839 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 15 (count at PaidPromotionAdjustParameter.scala:168) with 2 output partitions
2021-12-07 13:36:49,839 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 37 (count at PaidPromotionAdjustParameter.scala:168)
2021-12-07 13:36:49,839 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 36)
2021-12-07 13:36:49,839 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 36)
2021-12-07 13:36:49,839 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 36 (MapPartitionsRDD[41] at distinct at PaidPromotionAdjustParameter.scala:159), which has no missing parents
2021-12-07 13:36:49,841 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25 stored as values in memory (estimated size 186.5 KB, free 1990.0 MB)
2021-12-07 13:36:49,843 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25_piece0 stored as bytes in memory (estimated size 66.4 KB, free 1990.0 MB)
2021-12-07 13:36:49,844 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_25_piece0 in memory on qb:50664 (size: 66.4 KB, free: 1990.6 MB)
2021-12-07 13:36:49,844 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 25 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:49,844 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[41] at distinct at PaidPromotionAdjustParameter.scala:159) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:49,844 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 36.0 with 2 tasks
2021-12-07 13:36:49,845 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 36.0 (TID 54, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:36:49,845 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 36.0 (TID 55, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:36:49,845 [Executor task launch worker for task 54] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 36.0 (TID 54)
2021-12-07 13:36:49,845 [Executor task launch worker for task 55] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 36.0 (TID 55)
2021-12-07 13:36:49,847 [Executor task launch worker for task 54] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:49,847 [Executor task launch worker for task 55] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:51,444 [Executor task launch worker for task 55] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 36.0 (TID 55). 989 bytes result sent to driver
2021-12-07 13:36:51,445 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 36.0 (TID 55) in 1600 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 562
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 598
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 587
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 583
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 564
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 597
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 594
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 559
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 573
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 555
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 593
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 567
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 551
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 571
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 574
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 554
2021-12-07 13:36:51,480 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 557
2021-12-07 13:36:51,481 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_23_piece0 on qb:50664 in memory (size: 66.8 KB, free: 1990.7 MB)
2021-12-07 13:36:51,481 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 578
2021-12-07 13:36:51,481 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 577
2021-12-07 13:36:51,481 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 550
2021-12-07 13:36:51,481 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 588
2021-12-07 13:36:51,481 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 596
2021-12-07 13:36:51,481 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 565
2021-12-07 13:36:51,481 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 580
2021-12-07 13:36:51,481 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 558
2021-12-07 13:36:51,481 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 553
2021-12-07 13:36:51,481 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 563
2021-12-07 13:36:51,482 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_24_piece0 on qb:50664 in memory (size: 2.2 KB, free: 1990.7 MB)
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 589
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 566
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 576
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 585
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 569
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 560
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 552
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 579
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 582
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 592
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 599
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 570
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 584
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 561
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 591
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 590
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 568
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 581
2021-12-07 13:36:51,482 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 575
2021-12-07 13:36:51,483 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 595
2021-12-07 13:36:51,483 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 572
2021-12-07 13:36:51,483 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 586
2021-12-07 13:36:51,483 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 556
2021-12-07 13:36:51,647 [Executor task launch worker for task 54] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 36.0 (TID 54). 1032 bytes result sent to driver
2021-12-07 13:36:51,647 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 36.0 (TID 54) in 1803 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:51,647 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 36.0, whose tasks have all completed, from pool 
2021-12-07 13:36:51,648 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 36 (distinct at PaidPromotionAdjustParameter.scala:159) finished in 1.808 s
2021-12-07 13:36:51,648 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:51,648 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:36:51,648 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 37)
2021-12-07 13:36:51,648 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:51,648 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 37 (MapPartitionsRDD[43] at distinct at PaidPromotionAdjustParameter.scala:159), which has no missing parents
2021-12-07 13:36:51,649 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26 stored as values in memory (estimated size 3.7 KB, free 1990.2 MB)
2021-12-07 13:36:51,651 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.2 MB)
2021-12-07 13:36:51,651 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_26_piece0 in memory on qb:50664 (size: 2.2 KB, free: 1990.7 MB)
2021-12-07 13:36:51,651 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 26 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:51,652 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 37 (MapPartitionsRDD[43] at distinct at PaidPromotionAdjustParameter.scala:159) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:51,652 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 37.0 with 2 tasks
2021-12-07 13:36:51,652 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 37.0 (TID 56, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:36:51,652 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 37.0 (TID 57, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:36:51,652 [Executor task launch worker for task 56] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 37.0 (TID 56)
2021-12-07 13:36:51,652 [Executor task launch worker for task 57] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 37.0 (TID 57)
2021-12-07 13:36:51,653 [Executor task launch worker for task 56] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:51,653 [Executor task launch worker for task 57] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:51,653 [Executor task launch worker for task 56] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,653 [Executor task launch worker for task 57] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,672 [Executor task launch worker for task 56] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 37.0 (TID 56). 1054 bytes result sent to driver
2021-12-07 13:36:51,672 [Executor task launch worker for task 57] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 37.0 (TID 57). 1054 bytes result sent to driver
2021-12-07 13:36:51,673 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 37.0 (TID 56) in 21 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:51,673 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 37.0 (TID 57) in 21 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:51,673 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 37.0, whose tasks have all completed, from pool 
2021-12-07 13:36:51,673 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 37 (count at PaidPromotionAdjustParameter.scala:168) finished in 0.024 s
2021-12-07 13:36:51,673 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 15 finished: count at PaidPromotionAdjustParameter.scala:168, took 1.835176 s
2021-12-07 13:36:51,674 [main] INFO [PaidPromotion$] - 最终验证集用户数 = 5194
2021-12-07 13:36:51,676 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:169
2021-12-07 13:36:51,676 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 45 (intersection at PaidPromotionAdjustParameter.scala:160)
2021-12-07 13:36:51,676 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 44 (intersection at PaidPromotionAdjustParameter.scala:160)
2021-12-07 13:36:51,676 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 16 (count at PaidPromotionAdjustParameter.scala:169) with 4 output partitions
2021-12-07 13:36:51,676 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 42 (count at PaidPromotionAdjustParameter.scala:169)
2021-12-07 13:36:51,676 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 39, ShuffleMapStage 41)
2021-12-07 13:36:51,676 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 39, ShuffleMapStage 41)
2021-12-07 13:36:51,677 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 39 (MapPartitionsRDD[45] at intersection at PaidPromotionAdjustParameter.scala:160), which has no missing parents
2021-12-07 13:36:51,677 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27 stored as values in memory (estimated size 4.0 KB, free 1990.2 MB)
2021-12-07 13:36:51,680 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.2 MB)
2021-12-07 13:36:51,681 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_27_piece0 in memory on qb:50664 (size: 2.3 KB, free: 1990.7 MB)
2021-12-07 13:36:51,681 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 27 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:51,681 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 39 (MapPartitionsRDD[45] at intersection at PaidPromotionAdjustParameter.scala:160) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:51,681 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 39.0 with 2 tasks
2021-12-07 13:36:51,682 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 41 (MapPartitionsRDD[44] at intersection at PaidPromotionAdjustParameter.scala:160), which has no missing parents
2021-12-07 13:36:51,682 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 39.0 (TID 58, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:36:51,682 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 39.0 (TID 59, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:36:51,682 [Executor task launch worker for task 59] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 39.0 (TID 59)
2021-12-07 13:36:51,682 [Executor task launch worker for task 58] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 39.0 (TID 58)
2021-12-07 13:36:51,682 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28 stored as values in memory (estimated size 4.0 KB, free 1990.2 MB)
2021-12-07 13:36:51,683 [Executor task launch worker for task 58] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:51,683 [Executor task launch worker for task 59] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:51,683 [Executor task launch worker for task 58] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,683 [Executor task launch worker for task 59] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,684 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.2 MB)
2021-12-07 13:36:51,685 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_28_piece0 in memory on qb:50664 (size: 2.3 KB, free: 1990.7 MB)
2021-12-07 13:36:51,685 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 28 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:51,685 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[44] at intersection at PaidPromotionAdjustParameter.scala:160) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:36:51,685 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 41.0 with 4 tasks
2021-12-07 13:36:51,686 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 41.0 (TID 60, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:36:51,686 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 41.0 (TID 61, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:36:51,686 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 41.0 (TID 62, localhost, executor driver, partition 2, ANY, 7638 bytes)
2021-12-07 13:36:51,686 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 41.0 (TID 63, localhost, executor driver, partition 3, ANY, 7638 bytes)
2021-12-07 13:36:51,686 [Executor task launch worker for task 61] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 41.0 (TID 61)
2021-12-07 13:36:51,686 [Executor task launch worker for task 60] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 41.0 (TID 60)
2021-12-07 13:36:51,687 [Executor task launch worker for task 60] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:51,687 [Executor task launch worker for task 61] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:51,687 [Executor task launch worker for task 60] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,687 [Executor task launch worker for task 61] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,693 [Executor task launch worker for task 62] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 41.0 (TID 62)
2021-12-07 13:36:51,695 [Executor task launch worker for task 62] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:51,696 [Executor task launch worker for task 62] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:36:51,698 [Executor task launch worker for task 63] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 41.0 (TID 63)
2021-12-07 13:36:51,699 [Executor task launch worker for task 63] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:51,699 [Executor task launch worker for task 63] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,724 [Executor task launch worker for task 59] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 39.0 (TID 59). 1206 bytes result sent to driver
2021-12-07 13:36:51,724 [Executor task launch worker for task 58] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 39.0 (TID 58). 1206 bytes result sent to driver
2021-12-07 13:36:51,724 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 39.0 (TID 59) in 42 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:51,724 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 39.0 (TID 58) in 42 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:51,724 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 39.0, whose tasks have all completed, from pool 
2021-12-07 13:36:51,724 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 39 (intersection at PaidPromotionAdjustParameter.scala:160) finished in 0.047 s
2021-12-07 13:36:51,724 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:51,725 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set(ShuffleMapStage 41)
2021-12-07 13:36:51,725 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 42)
2021-12-07 13:36:51,725 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:51,756 [Executor task launch worker for task 60] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 41.0 (TID 60). 1206 bytes result sent to driver
2021-12-07 13:36:51,757 [Executor task launch worker for task 61] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 41.0 (TID 61). 1206 bytes result sent to driver
2021-12-07 13:36:51,757 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 41.0 (TID 60) in 72 ms on localhost (executor driver) (1/4)
2021-12-07 13:36:51,757 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 41.0 (TID 61) in 71 ms on localhost (executor driver) (2/4)
2021-12-07 13:36:51,761 [Executor task launch worker for task 63] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 41.0 (TID 63). 1163 bytes result sent to driver
2021-12-07 13:36:51,761 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 41.0 (TID 63) in 75 ms on localhost (executor driver) (3/4)
2021-12-07 13:36:51,764 [Executor task launch worker for task 62] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 41.0 (TID 62). 1206 bytes result sent to driver
2021-12-07 13:36:51,765 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 41.0 (TID 62) in 79 ms on localhost (executor driver) (4/4)
2021-12-07 13:36:51,765 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 41.0, whose tasks have all completed, from pool 
2021-12-07 13:36:51,765 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 41 (intersection at PaidPromotionAdjustParameter.scala:160) finished in 0.083 s
2021-12-07 13:36:51,765 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:51,765 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:36:51,765 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 42)
2021-12-07 13:36:51,765 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:51,765 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 42 (MapPartitionsRDD[49] at intersection at PaidPromotionAdjustParameter.scala:160), which has no missing parents
2021-12-07 13:36:51,766 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29 stored as values in memory (estimated size 3.6 KB, free 1990.2 MB)
2021-12-07 13:36:51,768 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.2 MB)
2021-12-07 13:36:51,768 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_29_piece0 in memory on qb:50664 (size: 2.1 KB, free: 1990.7 MB)
2021-12-07 13:36:51,768 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 29 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:51,769 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ResultStage 42 (MapPartitionsRDD[49] at intersection at PaidPromotionAdjustParameter.scala:160) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:36:51,769 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 42.0 with 4 tasks
2021-12-07 13:36:51,769 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 42.0 (TID 64, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:51,769 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 42.0 (TID 65, localhost, executor driver, partition 1, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:51,769 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 42.0 (TID 66, localhost, executor driver, partition 2, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:51,769 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 42.0 (TID 67, localhost, executor driver, partition 3, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:51,769 [Executor task launch worker for task 65] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 42.0 (TID 65)
2021-12-07 13:36:51,769 [Executor task launch worker for task 67] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 42.0 (TID 67)
2021-12-07 13:36:51,769 [Executor task launch worker for task 64] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 42.0 (TID 64)
2021-12-07 13:36:51,769 [Executor task launch worker for task 66] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 42.0 (TID 66)
2021-12-07 13:36:51,771 [Executor task launch worker for task 64] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:36:51,771 [Executor task launch worker for task 66] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:36:51,771 [Executor task launch worker for task 64] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,771 [Executor task launch worker for task 66] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,771 [Executor task launch worker for task 67] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:36:51,771 [Executor task launch worker for task 65] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:36:51,771 [Executor task launch worker for task 67] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,771 [Executor task launch worker for task 65] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,773 [Executor task launch worker for task 65] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:51,773 [Executor task launch worker for task 64] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:51,773 [Executor task launch worker for task 65] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,773 [Executor task launch worker for task 64] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,773 [Executor task launch worker for task 66] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:51,773 [Executor task launch worker for task 66] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,773 [Executor task launch worker for task 67] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:51,773 [Executor task launch worker for task 67] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 629
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 635
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 608
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 625
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 605
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 606
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 616
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 623
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 632
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 631
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 611
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 640
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 643
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 633
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 607
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 648
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 621
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 647
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 610
2021-12-07 13:36:51,822 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 620
2021-12-07 13:36:51,823 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_28_piece0 on qb:50664 in memory (size: 2.3 KB, free: 1990.7 MB)
2021-12-07 13:36:51,823 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 601
2021-12-07 13:36:51,823 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 617
2021-12-07 13:36:51,823 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 637
2021-12-07 13:36:51,823 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 646
2021-12-07 13:36:51,823 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 636
2021-12-07 13:36:51,823 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 604
2021-12-07 13:36:51,823 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 614
2021-12-07 13:36:51,823 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 628
2021-12-07 13:36:51,823 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 622
2021-12-07 13:36:51,823 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 630
2021-12-07 13:36:51,823 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 615
2021-12-07 13:36:51,823 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 624
2021-12-07 13:36:51,824 [dispatcher-event-loop-5] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_26_piece0 on qb:50664 in memory (size: 2.2 KB, free: 1990.7 MB)
2021-12-07 13:36:51,824 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 645
2021-12-07 13:36:51,824 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 619
2021-12-07 13:36:51,824 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 644
2021-12-07 13:36:51,825 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_25_piece0 on qb:50664 in memory (size: 66.4 KB, free: 1990.8 MB)
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 639
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 649
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 600
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 603
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 612
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 613
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 641
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 638
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 642
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 634
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 609
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 602
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 626
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 627
2021-12-07 13:36:51,825 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 618
2021-12-07 13:36:51,826 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_27_piece0 on qb:50664 in memory (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:36:51,830 [Executor task launch worker for task 64] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 42.0 (TID 64). 1097 bytes result sent to driver
2021-12-07 13:36:51,831 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 42.0 (TID 64) in 62 ms on localhost (executor driver) (1/4)
2021-12-07 13:36:51,835 [Executor task launch worker for task 66] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 42.0 (TID 66). 1097 bytes result sent to driver
2021-12-07 13:36:51,835 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 42.0 (TID 66) in 66 ms on localhost (executor driver) (2/4)
2021-12-07 13:36:51,836 [Executor task launch worker for task 65] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 42.0 (TID 65). 1097 bytes result sent to driver
2021-12-07 13:36:51,836 [Executor task launch worker for task 67] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 42.0 (TID 67). 1097 bytes result sent to driver
2021-12-07 13:36:51,836 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 42.0 (TID 65) in 67 ms on localhost (executor driver) (3/4)
2021-12-07 13:36:51,836 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 42.0 (TID 67) in 67 ms on localhost (executor driver) (4/4)
2021-12-07 13:36:51,837 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 42.0, whose tasks have all completed, from pool 
2021-12-07 13:36:51,837 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 42 (count at PaidPromotionAdjustParameter.scala:169) finished in 0.072 s
2021-12-07 13:36:51,837 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 16 finished: count at PaidPromotionAdjustParameter.scala:169, took 0.160611 s
2021-12-07 13:36:51,837 [main] INFO [PaidPromotion$] - 最终共 同 用户数 = 5194
2021-12-07 13:36:51,839 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:170
2021-12-07 13:36:51,840 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 51 (distinct at PaidPromotionAdjustParameter.scala:162)
2021-12-07 13:36:51,840 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 17 (count at PaidPromotionAdjustParameter.scala:170) with 4 output partitions
2021-12-07 13:36:51,840 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 44 (count at PaidPromotionAdjustParameter.scala:170)
2021-12-07 13:36:51,840 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 43)
2021-12-07 13:36:51,840 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 43)
2021-12-07 13:36:51,840 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 43 (MapPartitionsRDD[51] at distinct at PaidPromotionAdjustParameter.scala:162), which has no missing parents
2021-12-07 13:36:51,842 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30 stored as values in memory (estimated size 187.1 KB, free 1990.3 MB)
2021-12-07 13:36:51,844 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30_piece0 stored as bytes in memory (estimated size 66.8 KB, free 1990.2 MB)
2021-12-07 13:36:51,845 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_30_piece0 in memory on qb:50664 (size: 66.8 KB, free: 1990.7 MB)
2021-12-07 13:36:51,845 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 30 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:51,845 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[51] at distinct at PaidPromotionAdjustParameter.scala:162) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:36:51,845 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 43.0 with 4 tasks
2021-12-07 13:36:51,846 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 43.0 (TID 68, localhost, executor driver, partition 0, ANY, 7994 bytes)
2021-12-07 13:36:51,846 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 43.0 (TID 69, localhost, executor driver, partition 1, ANY, 7994 bytes)
2021-12-07 13:36:51,846 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 43.0 (TID 70, localhost, executor driver, partition 2, ANY, 7994 bytes)
2021-12-07 13:36:51,847 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 43.0 (TID 71, localhost, executor driver, partition 3, ANY, 7994 bytes)
2021-12-07 13:36:51,847 [Executor task launch worker for task 69] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 43.0 (TID 69)
2021-12-07 13:36:51,847 [Executor task launch worker for task 71] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 43.0 (TID 71)
2021-12-07 13:36:51,847 [Executor task launch worker for task 70] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 43.0 (TID 70)
2021-12-07 13:36:51,847 [Executor task launch worker for task 68] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 43.0 (TID 68)
2021-12-07 13:36:51,850 [Executor task launch worker for task 70] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:51,850 [Executor task launch worker for task 71] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:51,850 [Executor task launch worker for task 68] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:51,851 [Executor task launch worker for task 69] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:52,358 [Executor task launch worker for task 71] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 43.0 (TID 71). 1034 bytes result sent to driver
2021-12-07 13:36:52,358 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 43.0 (TID 71) in 512 ms on localhost (executor driver) (1/4)
2021-12-07 13:36:53,263 [Executor task launch worker for task 69] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 43.0 (TID 69). 991 bytes result sent to driver
2021-12-07 13:36:53,264 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 43.0 (TID 69) in 1418 ms on localhost (executor driver) (2/4)
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 706
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 703
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 686
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 691
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 720
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 662
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 676
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 657
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 716
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 664
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 661
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 672
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 678
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 663
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 653
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 651
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 659
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 650
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 679
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 683
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 709
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 722
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 717
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 712
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 655
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 654
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 695
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 718
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 694
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 680
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 665
2021-12-07 13:36:53,615 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 667
2021-12-07 13:36:53,616 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_29_piece0 on qb:50664 in memory (size: 2.1 KB, free: 1990.7 MB)
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 701
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 674
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 675
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 702
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 688
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 682
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 658
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 713
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 714
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 656
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 707
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 699
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 660
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 710
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 721
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 700
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 685
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 693
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 719
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 671
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 724
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 697
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 681
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 684
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 715
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 708
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 692
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 677
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 689
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 698
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 690
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 670
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 668
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 711
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 723
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 669
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 673
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 687
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 704
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 652
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 696
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 705
2021-12-07 13:36:53,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 666
2021-12-07 13:36:54,666 [Executor task launch worker for task 70] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 43.0 (TID 70). 1034 bytes result sent to driver
2021-12-07 13:36:54,667 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 43.0 (TID 70) in 2821 ms on localhost (executor driver) (3/4)
2021-12-07 13:36:54,732 [Executor task launch worker for task 68] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 43.0 (TID 68). 1034 bytes result sent to driver
2021-12-07 13:36:54,732 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 43.0 (TID 68) in 2886 ms on localhost (executor driver) (4/4)
2021-12-07 13:36:54,732 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 43.0, whose tasks have all completed, from pool 
2021-12-07 13:36:54,733 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 43 (distinct at PaidPromotionAdjustParameter.scala:162) finished in 2.892 s
2021-12-07 13:36:54,733 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:54,733 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:36:54,733 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 44)
2021-12-07 13:36:54,733 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:54,733 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 44 (MapPartitionsRDD[53] at distinct at PaidPromotionAdjustParameter.scala:162), which has no missing parents
2021-12-07 13:36:54,734 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31 stored as values in memory (estimated size 3.7 KB, free 1990.2 MB)
2021-12-07 13:36:54,735 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.2 MB)
2021-12-07 13:36:54,736 [dispatcher-event-loop-5] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_31_piece0 in memory on qb:50664 (size: 2.2 KB, free: 1990.7 MB)
2021-12-07 13:36:54,736 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 31 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:54,736 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ResultStage 44 (MapPartitionsRDD[53] at distinct at PaidPromotionAdjustParameter.scala:162) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:36:54,736 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 44.0 with 4 tasks
2021-12-07 13:36:54,736 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 44.0 (TID 72, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:36:54,736 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 44.0 (TID 73, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:36:54,737 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 44.0 (TID 74, localhost, executor driver, partition 2, ANY, 7649 bytes)
2021-12-07 13:36:54,737 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 44.0 (TID 75, localhost, executor driver, partition 3, ANY, 7649 bytes)
2021-12-07 13:36:54,737 [Executor task launch worker for task 72] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 44.0 (TID 72)
2021-12-07 13:36:54,737 [Executor task launch worker for task 73] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 44.0 (TID 73)
2021-12-07 13:36:54,737 [Executor task launch worker for task 75] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 44.0 (TID 75)
2021-12-07 13:36:54,737 [Executor task launch worker for task 74] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 44.0 (TID 74)
2021-12-07 13:36:54,738 [Executor task launch worker for task 72] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:54,738 [Executor task launch worker for task 73] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:54,738 [Executor task launch worker for task 73] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:54,738 [Executor task launch worker for task 74] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:54,738 [Executor task launch worker for task 72] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:54,738 [Executor task launch worker for task 74] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:54,738 [Executor task launch worker for task 75] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:54,738 [Executor task launch worker for task 75] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:54,760 [Executor task launch worker for task 73] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 44.0 (TID 73). 967 bytes result sent to driver
2021-12-07 13:36:54,760 [Executor task launch worker for task 75] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 44.0 (TID 75). 967 bytes result sent to driver
2021-12-07 13:36:54,760 [Executor task launch worker for task 72] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 44.0 (TID 72). 967 bytes result sent to driver
2021-12-07 13:36:54,760 [Executor task launch worker for task 74] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 44.0 (TID 74). 1010 bytes result sent to driver
2021-12-07 13:36:54,760 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 44.0 (TID 73) in 24 ms on localhost (executor driver) (1/4)
2021-12-07 13:36:54,760 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 44.0 (TID 75) in 23 ms on localhost (executor driver) (2/4)
2021-12-07 13:36:54,760 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 44.0 (TID 72) in 24 ms on localhost (executor driver) (3/4)
2021-12-07 13:36:54,761 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 44.0 (TID 74) in 23 ms on localhost (executor driver) (4/4)
2021-12-07 13:36:54,761 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 44.0, whose tasks have all completed, from pool 
2021-12-07 13:36:54,761 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 44 (count at PaidPromotionAdjustParameter.scala:170) finished in 0.028 s
2021-12-07 13:36:54,761 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 17 finished: count at PaidPromotionAdjustParameter.scala:170, took 2.921781 s
2021-12-07 13:36:54,761 [main] INFO [PaidPromotion$] - 最终训练集节目数 = 132
2021-12-07 13:36:54,764 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:171
2021-12-07 13:36:54,764 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 55 (distinct at PaidPromotionAdjustParameter.scala:163)
2021-12-07 13:36:54,764 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 18 (count at PaidPromotionAdjustParameter.scala:171) with 2 output partitions
2021-12-07 13:36:54,764 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 46 (count at PaidPromotionAdjustParameter.scala:171)
2021-12-07 13:36:54,764 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 45)
2021-12-07 13:36:54,764 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 45)
2021-12-07 13:36:54,764 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 45 (MapPartitionsRDD[55] at distinct at PaidPromotionAdjustParameter.scala:163), which has no missing parents
2021-12-07 13:36:54,766 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32 stored as values in memory (estimated size 186.5 KB, free 1990.0 MB)
2021-12-07 13:36:54,768 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32_piece0 stored as bytes in memory (estimated size 66.4 KB, free 1990.0 MB)
2021-12-07 13:36:54,769 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_32_piece0 in memory on qb:50664 (size: 66.4 KB, free: 1990.6 MB)
2021-12-07 13:36:54,769 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 32 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:54,769 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[55] at distinct at PaidPromotionAdjustParameter.scala:163) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:54,769 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 45.0 with 2 tasks
2021-12-07 13:36:54,770 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 45.0 (TID 76, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:36:54,770 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 45.0 (TID 77, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:36:54,770 [Executor task launch worker for task 76] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 45.0 (TID 76)
2021-12-07 13:36:54,770 [Executor task launch worker for task 77] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 45.0 (TID 77)
2021-12-07 13:36:54,772 [Executor task launch worker for task 76] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:54,772 [Executor task launch worker for task 77] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:36:56,047 [Executor task launch worker for task 77] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 45.0 (TID 77). 989 bytes result sent to driver
2021-12-07 13:36:56,048 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 45.0 (TID 77) in 1278 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:56,106 [Executor task launch worker for task 76] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 45.0 (TID 76). 989 bytes result sent to driver
2021-12-07 13:36:56,106 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 45.0 (TID 76) in 1337 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:56,106 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 45.0, whose tasks have all completed, from pool 
2021-12-07 13:36:56,106 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 45 (distinct at PaidPromotionAdjustParameter.scala:163) finished in 1.341 s
2021-12-07 13:36:56,106 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:56,106 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:36:56,106 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 46)
2021-12-07 13:36:56,106 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:56,107 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 46 (MapPartitionsRDD[57] at distinct at PaidPromotionAdjustParameter.scala:163), which has no missing parents
2021-12-07 13:36:56,107 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33 stored as values in memory (estimated size 3.7 KB, free 1990.0 MB)
2021-12-07 13:36:56,109 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.0 MB)
2021-12-07 13:36:56,109 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_33_piece0 in memory on qb:50664 (size: 2.2 KB, free: 1990.6 MB)
2021-12-07 13:36:56,109 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 33 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:56,109 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 46 (MapPartitionsRDD[57] at distinct at PaidPromotionAdjustParameter.scala:163) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:56,109 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 46.0 with 2 tasks
2021-12-07 13:36:56,110 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 46.0 (TID 78, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:36:56,110 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 46.0 (TID 79, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:36:56,110 [Executor task launch worker for task 79] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 46.0 (TID 79)
2021-12-07 13:36:56,110 [Executor task launch worker for task 78] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 46.0 (TID 78)
2021-12-07 13:36:56,111 [Executor task launch worker for task 79] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:56,111 [Executor task launch worker for task 78] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:56,111 [Executor task launch worker for task 79] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,111 [Executor task launch worker for task 78] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,123 [Executor task launch worker for task 79] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 46.0 (TID 79). 1053 bytes result sent to driver
2021-12-07 13:36:56,123 [Executor task launch worker for task 78] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 46.0 (TID 78). 1053 bytes result sent to driver
2021-12-07 13:36:56,123 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 46.0 (TID 79) in 13 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:56,123 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 46.0 (TID 78) in 13 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:56,123 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 46.0, whose tasks have all completed, from pool 
2021-12-07 13:36:56,124 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 46 (count at PaidPromotionAdjustParameter.scala:171) finished in 0.017 s
2021-12-07 13:36:56,124 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 18 finished: count at PaidPromotionAdjustParameter.scala:171, took 1.360484 s
2021-12-07 13:36:56,124 [main] INFO [PaidPromotion$] - 最终验证集节目数 = 87
2021-12-07 13:36:56,127 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:172
2021-12-07 13:36:56,127 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 59 (intersection at PaidPromotionAdjustParameter.scala:164)
2021-12-07 13:36:56,127 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 58 (intersection at PaidPromotionAdjustParameter.scala:164)
2021-12-07 13:36:56,127 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 19 (count at PaidPromotionAdjustParameter.scala:172) with 4 output partitions
2021-12-07 13:36:56,127 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 51 (count at PaidPromotionAdjustParameter.scala:172)
2021-12-07 13:36:56,127 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 48, ShuffleMapStage 50)
2021-12-07 13:36:56,127 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 48, ShuffleMapStage 50)
2021-12-07 13:36:56,127 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 48 (MapPartitionsRDD[59] at intersection at PaidPromotionAdjustParameter.scala:164), which has no missing parents
2021-12-07 13:36:56,128 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_34 stored as values in memory (estimated size 4.0 KB, free 1990.0 MB)
2021-12-07 13:36:56,130 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_34_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.0 MB)
2021-12-07 13:36:56,130 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_34_piece0 in memory on qb:50664 (size: 2.3 KB, free: 1990.6 MB)
2021-12-07 13:36:56,130 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 34 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:56,130 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 48 (MapPartitionsRDD[59] at intersection at PaidPromotionAdjustParameter.scala:164) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:36:56,130 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 48.0 with 2 tasks
2021-12-07 13:36:56,131 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 50 (MapPartitionsRDD[58] at intersection at PaidPromotionAdjustParameter.scala:164), which has no missing parents
2021-12-07 13:36:56,131 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 48.0 (TID 80, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:36:56,131 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 48.0 (TID 81, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:36:56,131 [Executor task launch worker for task 80] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 48.0 (TID 80)
2021-12-07 13:36:56,131 [Executor task launch worker for task 81] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 48.0 (TID 81)
2021-12-07 13:36:56,132 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_35 stored as values in memory (estimated size 4.0 KB, free 1990.0 MB)
2021-12-07 13:36:56,132 [Executor task launch worker for task 81] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:56,132 [Executor task launch worker for task 80] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:36:56,132 [Executor task launch worker for task 81] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,132 [Executor task launch worker for task 80] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,134 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_35_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1989.9 MB)
2021-12-07 13:36:56,134 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_35_piece0 in memory on qb:50664 (size: 2.3 KB, free: 1990.6 MB)
2021-12-07 13:36:56,134 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 35 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:56,134 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[58] at intersection at PaidPromotionAdjustParameter.scala:164) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:36:56,134 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 50.0 with 4 tasks
2021-12-07 13:36:56,135 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 50.0 (TID 82, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:36:56,135 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 50.0 (TID 83, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:36:56,135 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 50.0 (TID 84, localhost, executor driver, partition 2, ANY, 7638 bytes)
2021-12-07 13:36:56,135 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 50.0 (TID 85, localhost, executor driver, partition 3, ANY, 7638 bytes)
2021-12-07 13:36:56,135 [Executor task launch worker for task 84] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 50.0 (TID 84)
2021-12-07 13:36:56,135 [Executor task launch worker for task 82] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 50.0 (TID 82)
2021-12-07 13:36:56,135 [Executor task launch worker for task 85] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 50.0 (TID 85)
2021-12-07 13:36:56,135 [Executor task launch worker for task 83] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 50.0 (TID 83)
2021-12-07 13:36:56,136 [Executor task launch worker for task 84] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:56,136 [Executor task launch worker for task 84] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,136 [Executor task launch worker for task 85] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:56,136 [Executor task launch worker for task 85] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,136 [Executor task launch worker for task 82] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:56,137 [Executor task launch worker for task 82] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:36:56,137 [Executor task launch worker for task 83] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:36:56,137 [Executor task launch worker for task 83] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,157 [Executor task launch worker for task 84] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 50.0 (TID 84). 1163 bytes result sent to driver
2021-12-07 13:36:56,158 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 50.0 (TID 84) in 23 ms on localhost (executor driver) (1/4)
2021-12-07 13:36:56,158 [Executor task launch worker for task 85] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 50.0 (TID 85). 1206 bytes result sent to driver
2021-12-07 13:36:56,158 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 50.0 (TID 85) in 23 ms on localhost (executor driver) (2/4)
2021-12-07 13:36:56,159 [Executor task launch worker for task 83] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 50.0 (TID 83). 1163 bytes result sent to driver
2021-12-07 13:36:56,159 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 50.0 (TID 83) in 24 ms on localhost (executor driver) (3/4)
2021-12-07 13:36:56,160 [Executor task launch worker for task 82] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 50.0 (TID 82). 1206 bytes result sent to driver
2021-12-07 13:36:56,160 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 50.0 (TID 82) in 25 ms on localhost (executor driver) (4/4)
2021-12-07 13:36:56,161 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 50.0, whose tasks have all completed, from pool 
2021-12-07 13:36:56,161 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 50 (intersection at PaidPromotionAdjustParameter.scala:164) finished in 0.030 s
2021-12-07 13:36:56,161 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:56,161 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set(ShuffleMapStage 48)
2021-12-07 13:36:56,161 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 51)
2021-12-07 13:36:56,161 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:56,161 [Executor task launch worker for task 80] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 48.0 (TID 80). 1163 bytes result sent to driver
2021-12-07 13:36:56,161 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 48.0 (TID 80) in 30 ms on localhost (executor driver) (1/2)
2021-12-07 13:36:56,162 [Executor task launch worker for task 81] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 48.0 (TID 81). 1206 bytes result sent to driver
2021-12-07 13:36:56,162 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 48.0 (TID 81) in 31 ms on localhost (executor driver) (2/2)
2021-12-07 13:36:56,162 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 48.0, whose tasks have all completed, from pool 
2021-12-07 13:36:56,163 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 48 (intersection at PaidPromotionAdjustParameter.scala:164) finished in 0.034 s
2021-12-07 13:36:56,163 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:36:56,163 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:36:56,163 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 51)
2021-12-07 13:36:56,163 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:36:56,163 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 51 (MapPartitionsRDD[63] at intersection at PaidPromotionAdjustParameter.scala:164), which has no missing parents
2021-12-07 13:36:56,164 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_36 stored as values in memory (estimated size 3.6 KB, free 1989.9 MB)
2021-12-07 13:36:56,165 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_36_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1989.9 MB)
2021-12-07 13:36:56,166 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_36_piece0 in memory on qb:50664 (size: 2.1 KB, free: 1990.6 MB)
2021-12-07 13:36:56,166 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 36 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:56,166 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ResultStage 51 (MapPartitionsRDD[63] at intersection at PaidPromotionAdjustParameter.scala:164) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:36:56,166 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 51.0 with 4 tasks
2021-12-07 13:36:56,166 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 51.0 (TID 86, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:56,167 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 51.0 (TID 87, localhost, executor driver, partition 1, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:56,167 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 51.0 (TID 88, localhost, executor driver, partition 2, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:56,167 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 51.0 (TID 89, localhost, executor driver, partition 3, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:36:56,167 [Executor task launch worker for task 87] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 51.0 (TID 87)
2021-12-07 13:36:56,167 [Executor task launch worker for task 89] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 51.0 (TID 89)
2021-12-07 13:36:56,167 [Executor task launch worker for task 86] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 51.0 (TID 86)
2021-12-07 13:36:56,167 [Executor task launch worker for task 88] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 51.0 (TID 88)
2021-12-07 13:36:56,168 [Executor task launch worker for task 88] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:36:56,168 [Executor task launch worker for task 87] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:36:56,168 [Executor task launch worker for task 88] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,168 [Executor task launch worker for task 87] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,168 [Executor task launch worker for task 89] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:36:56,168 [Executor task launch worker for task 86] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:36:56,168 [Executor task launch worker for task 89] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,168 [Executor task launch worker for task 86] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,170 [Executor task launch worker for task 86] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:56,170 [Executor task launch worker for task 86] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,170 [Executor task launch worker for task 89] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:56,170 [Executor task launch worker for task 89] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,170 [Executor task launch worker for task 87] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:56,170 [Executor task launch worker for task 88] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:36:56,170 [Executor task launch worker for task 88] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,170 [Executor task launch worker for task 87] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:36:56,180 [Executor task launch worker for task 89] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 51.0 (TID 89). 967 bytes result sent to driver
2021-12-07 13:36:56,180 [Executor task launch worker for task 87] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 51.0 (TID 87). 1010 bytes result sent to driver
2021-12-07 13:36:56,180 [Executor task launch worker for task 88] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 51.0 (TID 88). 967 bytes result sent to driver
2021-12-07 13:36:56,181 [Executor task launch worker for task 86] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 51.0 (TID 86). 967 bytes result sent to driver
2021-12-07 13:36:56,181 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 51.0 (TID 87) in 15 ms on localhost (executor driver) (1/4)
2021-12-07 13:36:56,181 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 51.0 (TID 89) in 14 ms on localhost (executor driver) (2/4)
2021-12-07 13:36:56,181 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 51.0 (TID 88) in 14 ms on localhost (executor driver) (3/4)
2021-12-07 13:36:56,181 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 51.0 (TID 86) in 15 ms on localhost (executor driver) (4/4)
2021-12-07 13:36:56,181 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 51.0, whose tasks have all completed, from pool 
2021-12-07 13:36:56,181 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 51 (count at PaidPromotionAdjustParameter.scala:172) finished in 0.018 s
2021-12-07 13:36:56,181 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 19 finished: count at PaidPromotionAdjustParameter.scala:172, took 0.054679 s
2021-12-07 13:36:56,182 [main] INFO [PaidPromotion$] - 最终共 同 节目数 = 87
2021-12-07 13:36:56,243 [main] INFO [org.apache.hadoop.conf.Configuration.deprecation] - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2021-12-07 13:36:56,245 [main] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:36:56,383 [main] INFO [org.apache.spark.SparkContext] - Starting job: runJob at SparkHadoopWriter.scala:78
2021-12-07 13:36:56,384 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 20 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2021-12-07 13:36:56,384 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 52 (runJob at SparkHadoopWriter.scala:78)
2021-12-07 13:36:56,384 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:36:56,384 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:36:56,384 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 52 (MapPartitionsRDD[66] at saveAsTextFile at PaidPromotionAdjustParameter.scala:177), which has no missing parents
2021-12-07 13:36:56,395 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_37 stored as values in memory (estimated size 262.2 KB, free 1989.7 MB)
2021-12-07 13:36:56,402 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 866
2021-12-07 13:36:56,403 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 795
2021-12-07 13:36:56,403 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 826
2021-12-07 13:36:56,403 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 858
2021-12-07 13:36:56,403 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 836
2021-12-07 13:36:56,404 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 889
2021-12-07 13:36:56,404 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_37_piece0 stored as bytes in memory (estimated size 94.9 KB, free 1989.6 MB)
2021-12-07 13:36:56,404 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 819
2021-12-07 13:36:56,404 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 781
2021-12-07 13:36:56,404 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 816
2021-12-07 13:36:56,404 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_37_piece0 in memory on qb:50664 (size: 94.9 KB, free: 1990.5 MB)
2021-12-07 13:36:56,404 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_33_piece0 on qb:50664 in memory (size: 2.2 KB, free: 1990.5 MB)
2021-12-07 13:36:56,404 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 37 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:36:56,404 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[66] at saveAsTextFile at PaidPromotionAdjustParameter.scala:177) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:36:56,404 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 52.0 with 1 tasks
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 750
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 827
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 734
2021-12-07 13:36:56,405 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_31_piece0 on qb:50664 in memory (size: 2.2 KB, free: 1990.5 MB)
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 870
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 838
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 792
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 818
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 899
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 780
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 778
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 787
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 731
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 772
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 794
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 886
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 847
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 747
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 843
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 748
2021-12-07 13:36:56,405 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 845
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 885
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 765
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 895
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 771
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 893
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 745
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 897
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 742
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 790
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 891
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 878
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 737
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 807
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 784
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 837
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 802
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 825
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 868
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 766
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 740
2021-12-07 13:36:56,406 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_30_piece0 on qb:50664 in memory (size: 66.8 KB, free: 1990.6 MB)
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 841
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 741
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 803
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 861
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 805
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 830
2021-12-07 13:36:56,406 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 799
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 850
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 815
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 873
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 822
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 776
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 872
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 817
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 853
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 863
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 727
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 809
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 857
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 860
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 769
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 881
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 824
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 874
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 728
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 736
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 755
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 844
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 846
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 754
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 800
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 812
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 773
2021-12-07 13:36:56,407 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_32_piece0 on qb:50664 in memory (size: 66.4 KB, free: 1990.7 MB)
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 753
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 862
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 767
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 760
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 887
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 768
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 869
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 758
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 725
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 883
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 746
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 777
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 762
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 785
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 739
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 829
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 821
2021-12-07 13:36:56,407 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 783
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 896
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 894
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 884
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 789
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 879
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 782
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 793
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 820
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 796
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 757
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 832
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 898
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 788
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 823
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 738
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 786
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 726
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 831
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 751
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 761
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 775
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 732
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 882
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 733
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 779
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 759
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 835
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 851
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 749
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 791
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 865
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 811
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 798
2021-12-07 13:36:56,408 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_34_piece0 on qb:50664 in memory (size: 2.3 KB, free: 1990.7 MB)
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 840
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 806
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 888
2021-12-07 13:36:56,408 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 743
2021-12-07 13:36:56,409 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_35_piece0 on qb:50664 in memory (size: 2.3 KB, free: 1990.7 MB)
2021-12-07 13:36:56,409 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 877
2021-12-07 13:36:56,409 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 730
2021-12-07 13:36:56,409 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 839
2021-12-07 13:36:56,409 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 774
2021-12-07 13:36:56,409 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 756
2021-12-07 13:36:56,409 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 855
2021-12-07 13:36:56,409 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 856
2021-12-07 13:36:56,409 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 890
2021-12-07 13:36:56,409 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 808
2021-12-07 13:36:56,409 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 814
2021-12-07 13:36:56,409 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 828
2021-12-07 13:36:56,409 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 810
2021-12-07 13:36:56,409 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 849
2021-12-07 13:36:56,409 [dispatcher-event-loop-5] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_36_piece0 on qb:50664 in memory (size: 2.1 KB, free: 1990.7 MB)
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 875
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 852
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 864
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 764
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 848
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 834
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 752
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 804
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 876
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 871
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 763
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 842
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 729
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 859
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 735
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 797
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 801
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 867
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 744
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 770
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 813
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 833
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 880
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 892
2021-12-07 13:36:56,410 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 854
2021-12-07 13:36:56,412 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 52.0 (TID 90, localhost, executor driver, partition 0, ANY, 8509 bytes)
2021-12-07 13:36:56,412 [Executor task launch worker for task 90] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 52.0 (TID 90)
2021-12-07 13:36:56,429 [Executor task launch worker for task 90] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:36:56,638 [Executor task launch worker for task 90] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:36:59,901 [Executor task launch worker for task 90] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:37:01,205 [Executor task launch worker for task 90] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:37:01,645 [Executor task launch worker for task 90] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:37:19,915 [Executor task launch worker for task 90] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - Saved output of task 'attempt_20211207133656_0066_m_000000_0' to hdfs://hdp1:8020/sk/chongqing/handle_data/set-train-device-production-data/_temporary/0/task_20211207133656_0066_m_000000
2021-12-07 13:37:19,915 [Executor task launch worker for task 90] INFO [org.apache.spark.mapred.SparkHadoopMapRedUtil] - attempt_20211207133656_0066_m_000000_0: Committed
2021-12-07 13:37:19,917 [Executor task launch worker for task 90] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 52.0 (TID 90). 1041 bytes result sent to driver
2021-12-07 13:37:19,922 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 52.0 (TID 90) in 23517 ms on localhost (executor driver) (1/1)
2021-12-07 13:37:19,922 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 52.0, whose tasks have all completed, from pool 
2021-12-07 13:37:19,922 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 52 (runJob at SparkHadoopWriter.scala:78) finished in 23.537 s
2021-12-07 13:37:19,922 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 20 finished: runJob at SparkHadoopWriter.scala:78, took 23.538866 s
2021-12-07 13:37:20,583 [main] INFO [org.apache.spark.internal.io.SparkHadoopWriter] - Job job_20211207133656_0066 committed.
2021-12-07 13:37:20,689 [main] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:37:20,733 [main] INFO [org.apache.spark.SparkContext] - Starting job: runJob at SparkHadoopWriter.scala:78
2021-12-07 13:37:20,733 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 21 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2021-12-07 13:37:20,733 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 53 (runJob at SparkHadoopWriter.scala:78)
2021-12-07 13:37:20,733 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:37:20,733 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:37:20,733 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 53 (MapPartitionsRDD[69] at saveAsTextFile at PaidPromotionAdjustParameter.scala:180), which has no missing parents
2021-12-07 13:37:20,745 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_38 stored as values in memory (estimated size 261.6 KB, free 1989.9 MB)
2021-12-07 13:37:20,747 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_38_piece0 stored as bytes in memory (estimated size 94.5 KB, free 1989.8 MB)
2021-12-07 13:37:20,747 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_38_piece0 in memory on qb:50664 (size: 94.5 KB, free: 1990.6 MB)
2021-12-07 13:37:20,748 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 38 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:37:20,748 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[69] at saveAsTextFile at PaidPromotionAdjustParameter.scala:180) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:37:20,748 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 53.0 with 1 tasks
2021-12-07 13:37:20,748 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 53.0 (TID 91, localhost, executor driver, partition 0, ANY, 8337 bytes)
2021-12-07 13:37:20,748 [Executor task launch worker for task 91] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 53.0 (TID 91)
2021-12-07 13:37:20,754 [Executor task launch worker for task 91] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:37:20,822 [Executor task launch worker for task 91] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:37:23,669 [Executor task launch worker for task 91] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:37:27,415 [Executor task launch worker for task 91] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - Saved output of task 'attempt_20211207133720_0069_m_000000_0' to hdfs://hdp1:8020/sk/chongqing/handle_data/set-validation-device-production-data/_temporary/0/task_20211207133720_0069_m_000000
2021-12-07 13:37:27,416 [Executor task launch worker for task 91] INFO [org.apache.spark.mapred.SparkHadoopMapRedUtil] - attempt_20211207133720_0069_m_000000_0: Committed
2021-12-07 13:37:27,417 [Executor task launch worker for task 91] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 53.0 (TID 91). 912 bytes result sent to driver
2021-12-07 13:37:27,418 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 53.0 (TID 91) in 6670 ms on localhost (executor driver) (1/1)
2021-12-07 13:37:27,418 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 53.0, whose tasks have all completed, from pool 
2021-12-07 13:37:27,418 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 53 (runJob at SparkHadoopWriter.scala:78) finished in 6.685 s
2021-12-07 13:37:27,419 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 21 finished: runJob at SparkHadoopWriter.scala:78, took 6.685192 s
2021-12-07 13:37:29,445 [main] INFO [org.apache.spark.internal.io.SparkHadoopWriter] - Job job_20211207133720_0069 committed.
2021-12-07 13:37:29,502 [main] INFO [PaidPromotion$] - 验证集用户订购列表-----------------------------------
2021-12-07 13:37:29,504 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:190
2021-12-07 13:37:29,504 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 33 (filter at PaidPromotionAdjustParameter.scala:148)
2021-12-07 13:37:29,504 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 22 (count at PaidPromotionAdjustParameter.scala:190) with 2 output partitions
2021-12-07 13:37:29,504 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 55 (count at PaidPromotionAdjustParameter.scala:190)
2021-12-07 13:37:29,504 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 54)
2021-12-07 13:37:29,504 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 54)
2021-12-07 13:37:29,504 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 54 (MapPartitionsRDD[33] at filter at PaidPromotionAdjustParameter.scala:148), which has no missing parents
2021-12-07 13:37:29,506 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_39 stored as values in memory (estimated size 187.0 KB, free 1989.6 MB)
2021-12-07 13:37:29,508 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_39_piece0 stored as bytes in memory (estimated size 66.5 KB, free 1989.5 MB)
2021-12-07 13:37:29,508 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_39_piece0 in memory on qb:50664 (size: 66.5 KB, free: 1990.5 MB)
2021-12-07 13:37:29,509 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 39 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:37:29,509 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[33] at filter at PaidPromotionAdjustParameter.scala:148) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:37:29,509 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 54.0 with 2 tasks
2021-12-07 13:37:29,509 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 54.0 (TID 92, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:37:29,509 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 54.0 (TID 93, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:37:29,509 [Executor task launch worker for task 93] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 54.0 (TID 93)
2021-12-07 13:37:29,509 [Executor task launch worker for task 92] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 54.0 (TID 92)
2021-12-07 13:37:29,511 [Executor task launch worker for task 92] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:37:29,511 [Executor task launch worker for task 93] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:37:30,590 [dispatcher-event-loop-8] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_38_piece0 on qb:50664 in memory (size: 94.5 KB, free: 1990.6 MB)
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 901
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 913
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 943
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 945
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 924
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 944
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 925
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 931
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 941
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 915
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 926
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 937
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 935
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 942
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 933
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 928
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 918
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 909
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 938
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 939
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 902
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 923
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 919
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 908
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 903
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 949
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 910
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 916
2021-12-07 13:37:30,591 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_37_piece0 on qb:50664 in memory (size: 94.9 KB, free: 1990.7 MB)
2021-12-07 13:37:30,591 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 948
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 906
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 917
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 936
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 946
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 914
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 911
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 940
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 932
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 927
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 922
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 905
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 904
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 912
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 920
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 934
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 929
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 907
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 921
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 930
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 900
2021-12-07 13:37:30,592 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 947
2021-12-07 13:37:30,966 [Executor task launch worker for task 92] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 54.0 (TID 92). 903 bytes result sent to driver
2021-12-07 13:37:30,966 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 54.0 (TID 92) in 1457 ms on localhost (executor driver) (1/2)
2021-12-07 13:37:34,571 [Executor task launch worker for task 93] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 54.0 (TID 93). 903 bytes result sent to driver
2021-12-07 13:37:34,571 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 54.0 (TID 93) in 5062 ms on localhost (executor driver) (2/2)
2021-12-07 13:37:34,571 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 54.0, whose tasks have all completed, from pool 
2021-12-07 13:37:34,571 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 54 (filter at PaidPromotionAdjustParameter.scala:148) finished in 5.066 s
2021-12-07 13:37:34,571 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:37:34,571 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:37:34,571 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 55)
2021-12-07 13:37:34,571 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:37:34,571 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 55 (MapPartitionsRDD[71] at map at PaidPromotionAdjustParameter.scala:184), which has no missing parents
2021-12-07 13:37:34,578 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_40 stored as values in memory (estimated size 187.9 KB, free 1990.0 MB)
2021-12-07 13:37:34,580 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_40_piece0 stored as bytes in memory (estimated size 67.0 KB, free 1990.0 MB)
2021-12-07 13:37:34,580 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_40_piece0 in memory on qb:50664 (size: 67.0 KB, free: 1990.6 MB)
2021-12-07 13:37:34,580 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 40 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:37:34,580 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 55 (MapPartitionsRDD[71] at map at PaidPromotionAdjustParameter.scala:184) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:37:34,580 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 55.0 with 2 tasks
2021-12-07 13:37:34,581 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 55.0 (TID 94, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:37:34,581 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 55.0 (TID 95, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:37:34,581 [Executor task launch worker for task 94] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 55.0 (TID 94)
2021-12-07 13:37:34,581 [Executor task launch worker for task 95] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 55.0 (TID 95)
2021-12-07 13:37:34,583 [Executor task launch worker for task 95] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:37:34,583 [Executor task launch worker for task 95] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:37:34,583 [Executor task launch worker for task 94] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:37:34,583 [Executor task launch worker for task 94] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:37:34,619 [Executor task launch worker for task 95] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 55.0 (TID 95). 1054 bytes result sent to driver
2021-12-07 13:37:34,619 [Executor task launch worker for task 94] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 55.0 (TID 94). 1054 bytes result sent to driver
2021-12-07 13:37:34,620 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 55.0 (TID 95) in 39 ms on localhost (executor driver) (1/2)
2021-12-07 13:37:34,620 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 55.0 (TID 94) in 39 ms on localhost (executor driver) (2/2)
2021-12-07 13:37:34,620 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 55.0, whose tasks have all completed, from pool 
2021-12-07 13:37:34,620 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 55 (count at PaidPromotionAdjustParameter.scala:190) finished in 0.048 s
2021-12-07 13:37:34,620 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 22 finished: count at PaidPromotionAdjustParameter.scala:190, took 5.117026 s
2021-12-07 13:37:34,620 [main] INFO [PaidPromotion$] - 验证集用户列表数量：5194
2021-12-07 13:37:34,631 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at PaidPromotionAdjustParameter.scala:191
2021-12-07 13:37:34,632 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 23 (take at PaidPromotionAdjustParameter.scala:191) with 1 output partitions
2021-12-07 13:37:34,632 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 57 (take at PaidPromotionAdjustParameter.scala:191)
2021-12-07 13:37:34,632 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 56)
2021-12-07 13:37:34,632 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:37:34,632 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 57 (MapPartitionsRDD[71] at map at PaidPromotionAdjustParameter.scala:184), which has no missing parents
2021-12-07 13:37:34,633 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_41 stored as values in memory (estimated size 188.1 KB, free 1989.8 MB)
2021-12-07 13:37:34,635 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_41_piece0 stored as bytes in memory (estimated size 67.0 KB, free 1989.7 MB)
2021-12-07 13:37:34,636 [dispatcher-event-loop-8] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_41_piece0 in memory on qb:50664 (size: 67.0 KB, free: 1990.6 MB)
2021-12-07 13:37:34,636 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 41 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:37:34,636 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[71] at map at PaidPromotionAdjustParameter.scala:184) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:37:34,636 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 57.0 with 1 tasks
2021-12-07 13:37:34,637 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 57.0 (TID 96, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:37:34,637 [Executor task launch worker for task 96] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 57.0 (TID 96)
2021-12-07 13:37:34,639 [Executor task launch worker for task 96] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:37:34,639 [Executor task launch worker for task 96] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:37:34,655 [Executor task launch worker for task 96] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 57.0 (TID 96). 2620 bytes result sent to driver
2021-12-07 13:37:34,655 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 57.0 (TID 96) in 19 ms on localhost (executor driver) (1/1)
2021-12-07 13:37:34,655 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 57.0, whose tasks have all completed, from pool 
2021-12-07 13:37:34,655 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 57 (take at PaidPromotionAdjustParameter.scala:191) finished in 0.023 s
2021-12-07 13:37:34,655 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 23 finished: take at PaidPromotionAdjustParameter.scala:191, took 0.024117 s
2021-12-07 13:37:34,904 [main] INFO [PaidPromotion$] - 验证集产品包列表-----------------------------------
2021-12-07 13:37:34,905 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:202
2021-12-07 13:37:34,906 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 72 (map at PaidPromotionAdjustParameter.scala:194)
2021-12-07 13:37:34,906 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 24 (count at PaidPromotionAdjustParameter.scala:202) with 2 output partitions
2021-12-07 13:37:34,906 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 59 (count at PaidPromotionAdjustParameter.scala:202)
2021-12-07 13:37:34,906 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 58)
2021-12-07 13:37:34,906 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 58)
2021-12-07 13:37:34,906 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 58 (MapPartitionsRDD[72] at map at PaidPromotionAdjustParameter.scala:194), which has no missing parents
2021-12-07 13:37:34,909 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_42 stored as values in memory (estimated size 187.1 KB, free 1989.5 MB)
2021-12-07 13:37:34,912 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_42_piece0 stored as bytes in memory (estimated size 66.6 KB, free 1989.5 MB)
2021-12-07 13:37:34,912 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_42_piece0 in memory on qb:50664 (size: 66.6 KB, free: 1990.5 MB)
2021-12-07 13:37:34,912 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 42 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:37:34,913 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 58 (MapPartitionsRDD[72] at map at PaidPromotionAdjustParameter.scala:194) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:37:34,913 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 58.0 with 2 tasks
2021-12-07 13:37:34,913 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 58.0 (TID 97, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:37:34,913 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 58.0 (TID 98, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:37:34,913 [Executor task launch worker for task 98] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 58.0 (TID 98)
2021-12-07 13:37:34,913 [Executor task launch worker for task 97] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 58.0 (TID 97)
2021-12-07 13:37:34,915 [Executor task launch worker for task 98] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:37:34,915 [Executor task launch worker for task 97] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:37:36,265 [Executor task launch worker for task 98] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 58.0 (TID 98). 860 bytes result sent to driver
2021-12-07 13:37:36,265 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 58.0 (TID 98) in 1352 ms on localhost (executor driver) (1/2)
2021-12-07 13:37:36,293 [Executor task launch worker for task 97] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 58.0 (TID 97). 860 bytes result sent to driver
2021-12-07 13:37:36,294 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 58.0 (TID 97) in 1381 ms on localhost (executor driver) (2/2)
2021-12-07 13:37:36,294 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 58.0, whose tasks have all completed, from pool 
2021-12-07 13:37:36,294 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 58 (map at PaidPromotionAdjustParameter.scala:194) finished in 1.387 s
2021-12-07 13:37:36,294 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:37:36,294 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:37:36,294 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 59)
2021-12-07 13:37:36,294 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:37:36,294 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 59 (MapPartitionsRDD[74] at map at PaidPromotionAdjustParameter.scala:196), which has no missing parents
2021-12-07 13:37:36,296 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_43 stored as values in memory (estimated size 188.0 KB, free 1989.3 MB)
2021-12-07 13:37:36,297 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_43_piece0 stored as bytes in memory (estimated size 67.0 KB, free 1989.2 MB)
2021-12-07 13:37:36,298 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_43_piece0 in memory on qb:50664 (size: 67.0 KB, free: 1990.4 MB)
2021-12-07 13:37:36,298 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 43 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:37:36,298 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 59 (MapPartitionsRDD[74] at map at PaidPromotionAdjustParameter.scala:196) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:37:36,298 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 59.0 with 2 tasks
2021-12-07 13:37:36,298 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 59.0 (TID 99, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:37:36,298 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 59.0 (TID 100, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:37:36,298 [Executor task launch worker for task 99] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 59.0 (TID 99)
2021-12-07 13:37:36,298 [Executor task launch worker for task 100] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 59.0 (TID 100)
2021-12-07 13:37:36,301 [Executor task launch worker for task 100] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:37:36,301 [Executor task launch worker for task 99] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:37:36,301 [Executor task launch worker for task 100] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:37:36,301 [Executor task launch worker for task 99] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:38:45,284 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 997
2021-12-07 13:38:45,284 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1019
2021-12-07 13:38:45,284 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1012
2021-12-07 13:38:45,284 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 987
2021-12-07 13:38:45,284 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 951
2021-12-07 13:38:45,284 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1009
2021-12-07 13:38:45,284 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1010
2021-12-07 13:38:45,286 [Executor task launch worker for task 99] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 59.0 (TID 99). 1096 bytes result sent to driver
2021-12-07 13:38:45,286 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 59.0 (TID 99) in 68988 ms on localhost (executor driver) (1/2)
2021-12-07 13:38:45,288 [Executor task launch worker for task 100] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 59.0 (TID 100). 1139 bytes result sent to driver
2021-12-07 13:38:45,288 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 59.0 (TID 100) in 68990 ms on localhost (executor driver) (2/2)
2021-12-07 13:38:45,288 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 59.0, whose tasks have all completed, from pool 
2021-12-07 13:38:45,288 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 59 (count at PaidPromotionAdjustParameter.scala:202) finished in 68.994 s
2021-12-07 13:38:45,288 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 24 finished: count at PaidPromotionAdjustParameter.scala:202, took 70.383701 s
2021-12-07 13:38:45,289 [main] INFO [PaidPromotion$] - 验证集产品包列表数量：87
2021-12-07 13:38:45,289 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_39_piece0 on qb:50664 in memory (size: 66.5 KB, free: 1990.5 MB)
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1022
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 957
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 996
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 968
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 992
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1002
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1003
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 959
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 973
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 954
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 980
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 976
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1016
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1015
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 956
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 969
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 960
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1000
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 979
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 963
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 958
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1011
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 988
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 970
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1004
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 975
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 982
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1023
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 977
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 953
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 961
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 972
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 998
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 986
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 990
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1021
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 967
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 950
2021-12-07 13:38:45,290 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 993
2021-12-07 13:38:45,291 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 985
2021-12-07 13:38:45,291 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 974
2021-12-07 13:38:45,291 [dispatcher-event-loop-8] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_42_piece0 on qb:50664 in memory (size: 66.6 KB, free: 1990.6 MB)
2021-12-07 13:38:45,291 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 995
2021-12-07 13:38:45,291 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 962
2021-12-07 13:38:45,291 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1014
2021-12-07 13:38:45,291 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1006
2021-12-07 13:38:45,291 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 971
2021-12-07 13:38:45,291 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 955
2021-12-07 13:38:45,291 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 991
2021-12-07 13:38:45,291 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 966
2021-12-07 13:38:45,292 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_41_piece0 on qb:50664 in memory (size: 67.0 KB, free: 1990.6 MB)
2021-12-07 13:38:45,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1001
2021-12-07 13:38:45,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1013
2021-12-07 13:38:45,293 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_40_piece0 on qb:50664 in memory (size: 67.0 KB, free: 1990.7 MB)
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 989
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 952
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1008
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 994
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1007
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1005
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1017
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 964
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 965
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 999
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1020
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 981
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1024
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 984
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1018
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 983
2021-12-07 13:38:45,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 978
2021-12-07 13:38:45,299 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at PaidPromotionAdjustParameter.scala:203
2021-12-07 13:38:45,299 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 25 (take at PaidPromotionAdjustParameter.scala:203) with 1 output partitions
2021-12-07 13:38:45,299 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 61 (take at PaidPromotionAdjustParameter.scala:203)
2021-12-07 13:38:45,299 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 60)
2021-12-07 13:38:45,299 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:38:45,299 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 61 (MapPartitionsRDD[74] at map at PaidPromotionAdjustParameter.scala:196), which has no missing parents
2021-12-07 13:38:45,301 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_44 stored as values in memory (estimated size 188.2 KB, free 1990.0 MB)
2021-12-07 13:38:45,303 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_44_piece0 stored as bytes in memory (estimated size 67.1 KB, free 1990.0 MB)
2021-12-07 13:38:45,303 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_44_piece0 in memory on qb:50664 (size: 67.1 KB, free: 1990.6 MB)
2021-12-07 13:38:45,304 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 44 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:38:45,304 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[74] at map at PaidPromotionAdjustParameter.scala:196) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:38:45,304 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 61.0 with 1 tasks
2021-12-07 13:38:45,304 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 61.0 (TID 101, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:38:45,304 [Executor task launch worker for task 101] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 61.0 (TID 101)
2021-12-07 13:38:45,306 [Executor task launch worker for task 101] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:38:45,306 [Executor task launch worker for task 101] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:38:45,317 [Executor task launch worker for task 101] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 61.0 (TID 101). 34419 bytes result sent to driver
2021-12-07 13:38:45,317 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 61.0 (TID 101) in 13 ms on localhost (executor driver) (1/1)
2021-12-07 13:38:45,317 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 61.0, whose tasks have all completed, from pool 
2021-12-07 13:38:45,317 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 61 (take at PaidPromotionAdjustParameter.scala:203) finished in 0.018 s
2021-12-07 13:38:45,317 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 25 finished: take at PaidPromotionAdjustParameter.scala:203, took 0.018602 s
2021-12-07 13:38:45,325 [main] INFO [PaidPromotion$] - 训练集用户订购列表-----------------------------------
2021-12-07 13:38:45,327 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:213
2021-12-07 13:38:45,327 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 35 (union at PaidPromotionAdjustParameter.scala:152)
2021-12-07 13:38:45,327 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 26 (count at PaidPromotionAdjustParameter.scala:213) with 4 output partitions
2021-12-07 13:38:45,327 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 63 (count at PaidPromotionAdjustParameter.scala:213)
2021-12-07 13:38:45,327 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 62)
2021-12-07 13:38:45,327 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 62)
2021-12-07 13:38:45,327 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 62 (UnionRDD[35] at union at PaidPromotionAdjustParameter.scala:152), which has no missing parents
2021-12-07 13:38:45,329 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_45 stored as values in memory (estimated size 187.6 KB, free 1989.8 MB)
2021-12-07 13:38:45,331 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_45_piece0 stored as bytes in memory (estimated size 66.9 KB, free 1989.7 MB)
2021-12-07 13:38:45,332 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_45_piece0 in memory on qb:50664 (size: 66.9 KB, free: 1990.6 MB)
2021-12-07 13:38:45,332 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 45 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:38:45,332 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ShuffleMapStage 62 (UnionRDD[35] at union at PaidPromotionAdjustParameter.scala:152) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:38:45,332 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 62.0 with 4 tasks
2021-12-07 13:38:45,332 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 62.0 (TID 102, localhost, executor driver, partition 0, ANY, 7994 bytes)
2021-12-07 13:38:45,333 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 62.0 (TID 103, localhost, executor driver, partition 1, ANY, 7994 bytes)
2021-12-07 13:38:45,333 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 62.0 (TID 104, localhost, executor driver, partition 2, ANY, 7994 bytes)
2021-12-07 13:38:45,333 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 62.0 (TID 105, localhost, executor driver, partition 3, ANY, 7994 bytes)
2021-12-07 13:38:45,333 [Executor task launch worker for task 102] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 62.0 (TID 102)
2021-12-07 13:38:45,333 [Executor task launch worker for task 103] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 62.0 (TID 103)
2021-12-07 13:38:45,333 [Executor task launch worker for task 105] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 62.0 (TID 105)
2021-12-07 13:38:45,334 [Executor task launch worker for task 104] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 62.0 (TID 104)
2021-12-07 13:38:45,335 [Executor task launch worker for task 102] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:38:45,335 [Executor task launch worker for task 103] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:38:45,335 [Executor task launch worker for task 104] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:38:45,335 [Executor task launch worker for task 105] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:38:46,132 [Thread-1] INFO [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2021-12-07 13:38:46,137 [Thread-1] INFO [org.spark_project.jetty.server.AbstractConnector] - Stopped Spark@3414a8c3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2021-12-07 13:38:46,139 [Thread-1] INFO [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://qb:4040
2021-12-07 13:38:46,145 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 26 failed: count at PaidPromotionAdjustParameter.scala:213, took 0.818175 s
2021-12-07 13:38:46,146 [Thread-1] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 62 (union at PaidPromotionAdjustParameter.scala:152) failed in 0.817 s due to Stage cancelled because SparkContext was shut down
2021-12-07 13:38:46,152 [dispatcher-event-loop-0] INFO [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2021-12-07 13:38:46,251 [Thread-1] ERROR [org.apache.spark.storage.DiskBlockManager] - Exception while deleting local spark dir: C:\Users\ACER\AppData\Local\Temp\blockmgr-6b1df013-ec25-4d9e-95cc-03165aa61401
java.io.IOException: Failed to delete: C:\Users\ACER\AppData\Local\Temp\blockmgr-6b1df013-ec25-4d9e-95cc-03165aa61401
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1070)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$org$apache$spark$storage$DiskBlockManager$$doStop$1.apply(DiskBlockManager.scala:178)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$org$apache$spark$storage$DiskBlockManager$$doStop$1.apply(DiskBlockManager.scala:174)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.storage.DiskBlockManager.org$apache$spark$storage$DiskBlockManager$$doStop(DiskBlockManager.scala:174)
	at org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:169)
	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1544)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:90)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1940)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1939)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:572)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1988)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2021-12-07 13:38:46,254 [Thread-1] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2021-12-07 13:38:46,255 [Thread-1] INFO [org.apache.spark.storage.BlockManager] - BlockManager stopped
2021-12-07 13:38:46,255 [Thread-1] INFO [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2021-12-07 13:38:46,257 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2021-12-07 13:38:46,260 [Thread-1] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2021-12-07 13:38:46,260 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2021-12-07 13:38:46,261 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\ACER\AppData\Local\Temp\spark-04eec369-1674-4781-98bf-a25852678b32
2021-12-07 13:38:46,274 [Executor task launch worker for task 102] ERROR [org.apache.spark.util.Utils] - Uncaught exception in thread Executor task launch worker for task 102
java.lang.NullPointerException
	at org.apache.spark.scheduler.Task$$anonfun$run$1.apply$mcV$sp(Task.scala:130)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.scheduler.Task.run(Task.scala:128)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2021-12-07 13:38:46,274 [Executor task launch worker for task 103] ERROR [org.apache.spark.TaskContextImpl] - Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_45 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$2.apply(BlockInfoManager.scala:293)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:292)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:769)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:265)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:265)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:128)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:118)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:118)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:131)
	at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:129)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:129)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:117)
	at org.apache.spark.scheduler.Task.run(Task.scala:119)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2021-12-07 13:38:46,274 [Executor task launch worker for task 103] ERROR [org.apache.spark.util.Utils] - Uncaught exception in thread Executor task launch worker for task 103
java.lang.NullPointerException
	at org.apache.spark.scheduler.Task$$anonfun$run$1.apply$mcV$sp(Task.scala:130)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.scheduler.Task.run(Task.scala:128)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2021-12-07 13:38:46,286 [Executor task launch worker for task 102] ERROR [org.apache.spark.executor.Executor] - Exception in task 0.0 in stage 62.0 (TID 102)
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:808)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:868)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2021-12-07 13:38:46,286 [Executor task launch worker for task 103] ERROR [org.apache.spark.executor.Executor] - Exception in task 1.0 in stage 62.0 (TID 103)
org.apache.spark.util.TaskCompletionListenerException: Block broadcast_45 not found

Previous exception in task: Filesystem closed
	org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:808)
	org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:868)
	org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	java.io.DataInputStream.read(DataInputStream.java:149)
	org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)
	org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	org.apache.spark.scheduler.Task.run(Task.scala:109)
	org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	java.lang.Thread.run(Thread.java:745)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:139)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:117)
	at org.apache.spark.scheduler.Task.run(Task.scala:119)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2021-12-07 13:38:46,287 [Executor task launch worker for task 103] INFO [org.apache.spark.executor.Executor] - Not reporting error to driver during JVM shutdown.
2021-12-07 13:38:46,286 [Executor task launch worker for task 102] INFO [org.apache.spark.executor.Executor] - Not reporting error to driver during JVM shutdown.
2021-12-07 13:38:46,309 [Executor task launch worker for task 105] ERROR [org.apache.spark.util.Utils] - Uncaught exception in thread Executor task launch worker for task 105
java.lang.NullPointerException
	at org.apache.spark.scheduler.Task$$anonfun$run$1.apply$mcV$sp(Task.scala:130)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.scheduler.Task.run(Task.scala:128)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2021-12-07 13:38:46,309 [Executor task launch worker for task 105] ERROR [org.apache.spark.executor.Executor] - Exception in task 3.0 in stage 62.0 (TID 105)
java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:808)
	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:868)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:934)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:248)
	at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:48)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:277)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:214)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2021-12-07 13:38:46,309 [Executor task launch worker for task 105] INFO [org.apache.spark.executor.Executor] - Not reporting error to driver during JVM shutdown.
2021-12-07 13:39:07,485 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.3.0
2021-12-07 13:39:07,765 [main] INFO [org.apache.spark.SparkContext] - Submitted application: PaidPromotionAdjustParameter
2021-12-07 13:39:07,822 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: ACER,root
2021-12-07 13:39:07,823 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: ACER,root
2021-12-07 13:39:07,823 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2021-12-07 13:39:07,823 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2021-12-07 13:39:07,824 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ACER, root); groups with view permissions: Set(); users  with modify permissions: Set(ACER, root); groups with modify permissions: Set()
2021-12-07 13:39:08,397 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 59355.
2021-12-07 13:39:08,413 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2021-12-07 13:39:08,427 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2021-12-07 13:39:08,429 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2021-12-07 13:39:08,429 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2021-12-07 13:39:08,436 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\ACER\AppData\Local\Temp\blockmgr-b1770f2d-8d56-4b00-9a87-3e4e867df076
2021-12-07 13:39:08,450 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1990.8 MB
2021-12-07 13:39:08,459 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2021-12-07 13:39:08,509 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @1858ms
2021-12-07 13:39:08,557 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2021-12-07 13:39:08,568 [main] INFO [org.spark_project.jetty.server.Server] - Started @1917ms
2021-12-07 13:39:08,591 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@5b800468{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2021-12-07 13:39:08,591 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2021-12-07 13:39:08,608 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1568159{/jobs,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,609 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@63cd604c{/jobs/json,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,610 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3954d008{/jobs/job,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,611 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6d8792db{/jobs/job/json,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,612 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3a1d593e{/stages,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,613 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@285d851a{/stages/json,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,614 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7876d598{/stages/stage,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,616 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4985cbcb{/stages/stage/json,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,617 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@54361a9{/stages/pool,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,618 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@293bb8a5{/stages/pool/json,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,619 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@290b1b2e{/storage,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,619 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5db4c359{/storage/json,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,620 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6fefce9e{/storage/rdd,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,621 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@8a589a2{/storage/rdd/json,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,622 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cc69cfe{/environment,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,623 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@11dee337{/environment/json,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,624 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@74bdc168{/executors,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,626 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7bb3a9fe{/executors/json,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,628 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f19c9d2{/executors/threadDump,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,629 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@a77614d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,636 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@523424b5{/static,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,637 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@12cd9150{/,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,639 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@32fe9d0a{/api,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,640 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@59fc684e{/jobs/job/kill,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,641 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@355e34c7{/stages/stage/kill,null,AVAILABLE,@Spark}
2021-12-07 13:39:08,643 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://qb:4040
2021-12-07 13:39:08,715 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2021-12-07 13:39:08,758 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59396.
2021-12-07 13:39:08,759 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on qb:59396
2021-12-07 13:39:08,760 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2021-12-07 13:39:08,761 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, qb, 59396, None)
2021-12-07 13:39:08,763 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager qb:59396 with 1990.8 MB RAM, BlockManagerId(driver, qb, 59396, None)
2021-12-07 13:39:08,765 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, qb, 59396, None)
2021-12-07 13:39:08,765 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, qb, 59396, None)
2021-12-07 13:39:08,881 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6fe46b62{/metrics/json,null,AVAILABLE,@Spark}
2021-12-07 13:39:09,279 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 312.7 KB, free 1990.5 MB)
2021-12-07 13:39:09,458 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KB, free 1990.5 MB)
2021-12-07 13:39:09,459 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on qb:59396 (size: 27.3 KB, free: 1990.8 MB)
2021-12-07 13:39:09,462 [main] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from textFile at PaidPromotionAdjustParameter.scala:96
2021-12-07 13:39:09,772 [main] WARN [org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
2021-12-07 13:39:09,839 [main] INFO [org.apache.hadoop.mapred.FileInputFormat] - Total input paths to process : 1
2021-12-07 13:39:09,876 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:98
2021-12-07 13:39:09,885 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (count at PaidPromotionAdjustParameter.scala:98) with 2 output partitions
2021-12-07 13:39:09,886 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (count at PaidPromotionAdjustParameter.scala:98)
2021-12-07 13:39:09,886 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:39:09,887 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:09,892 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (/sk/chongqing/data/order_data.bcp MapPartitionsRDD[1] at textFile at PaidPromotionAdjustParameter.scala:96), which has no missing parents
2021-12-07 13:39:09,920 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 1990.5 MB)
2021-12-07 13:39:09,924 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1900.0 B, free 1990.5 MB)
2021-12-07 13:39:09,925 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on qb:59396 (size: 1900.0 B, free: 1990.8 MB)
2021-12-07 13:39:09,925 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:09,934 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (/sk/chongqing/data/order_data.bcp MapPartitionsRDD[1] at textFile at PaidPromotionAdjustParameter.scala:96) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:09,935 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2021-12-07 13:39:09,965 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7896 bytes)
2021-12-07 13:39:09,965 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7896 bytes)
2021-12-07 13:39:09,970 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2021-12-07 13:39:09,970 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2021-12-07 13:39:10,010 [Executor task launch worker for task 0] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:10,010 [Executor task launch worker for task 1] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:10,805 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 711 bytes result sent to driver
2021-12-07 13:39:10,813 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 847 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:10,826 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 711 bytes result sent to driver
2021-12-07 13:39:10,830 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 874 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:10,831 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2021-12-07 13:39:10,831 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (count at PaidPromotionAdjustParameter.scala:98) finished in 0.926 s
2021-12-07 13:39:10,835 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: count at PaidPromotionAdjustParameter.scala:98, took 0.959615 s
2021-12-07 13:39:10,836 [main] INFO [PaidPromotion$] - 订购总数107294
2021-12-07 13:39:10,840 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:106
2021-12-07 13:39:10,841 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (count at PaidPromotionAdjustParameter.scala:106) with 2 output partitions
2021-12-07 13:39:10,841 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 1 (count at PaidPromotionAdjustParameter.scala:106)
2021-12-07 13:39:10,841 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:39:10,841 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:10,841 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 1 (MapPartitionsRDD[2] at map at PaidPromotionAdjustParameter.scala:102), which has no missing parents
2021-12-07 13:39:10,843 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 3.3 KB, free 1990.5 MB)
2021-12-07 13:39:10,847 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1985.0 B, free 1990.5 MB)
2021-12-07 13:39:10,848 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on qb:59396 (size: 1985.0 B, free: 1990.8 MB)
2021-12-07 13:39:10,848 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:10,849 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[2] at map at PaidPromotionAdjustParameter.scala:102) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:10,849 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2021-12-07 13:39:10,850 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7896 bytes)
2021-12-07 13:39:10,850 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7896 bytes)
2021-12-07 13:39:10,850 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2021-12-07 13:39:10,850 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2021-12-07 13:39:10,853 [Executor task launch worker for task 2] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:10,853 [Executor task launch worker for task 3] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 4
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 18
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 15
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 21
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 3
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 7
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 2
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 12
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 11
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 22
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 0
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 6
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 5
2021-12-07 13:39:11,525 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 19
2021-12-07 13:39:11,526 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 14
2021-12-07 13:39:11,538 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on qb:59396 in memory (size: 1900.0 B, free: 1990.8 MB)
2021-12-07 13:39:11,540 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 13
2021-12-07 13:39:11,540 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 17
2021-12-07 13:39:11,540 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 10
2021-12-07 13:39:11,540 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 24
2021-12-07 13:39:11,540 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 23
2021-12-07 13:39:11,540 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 16
2021-12-07 13:39:11,540 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 9
2021-12-07 13:39:11,540 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 8
2021-12-07 13:39:11,540 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 20
2021-12-07 13:39:11,540 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1
2021-12-07 13:39:12,287 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 754 bytes result sent to driver
2021-12-07 13:39:12,289 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 1439 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:13,205 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 754 bytes result sent to driver
2021-12-07 13:39:13,208 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 2358 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:13,208 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2021-12-07 13:39:13,208 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 1 (count at PaidPromotionAdjustParameter.scala:106) finished in 2.366 s
2021-12-07 13:39:13,209 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: count at PaidPromotionAdjustParameter.scala:106, took 2.367799 s
2021-12-07 13:39:13,209 [main] INFO [PaidPromotion$] - 订购总数：107294
2021-12-07 13:39:13,222 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:119
2021-12-07 13:39:13,222 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (count at PaidPromotionAdjustParameter.scala:119) with 2 output partitions
2021-12-07 13:39:13,222 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (count at PaidPromotionAdjustParameter.scala:119)
2021-12-07 13:39:13,222 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:39:13,222 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:13,223 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[3] at randomSplit at PaidPromotionAdjustParameter.scala:112), which has no missing parents
2021-12-07 13:39:13,224 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 3.6 KB, free 1990.5 MB)
2021-12-07 13:39:13,228 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.5 MB)
2021-12-07 13:39:13,229 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on qb:59396 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:39:13,229 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:13,230 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[3] at randomSplit at PaidPromotionAdjustParameter.scala:112) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:13,230 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2021-12-07 13:39:13,231 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 7896 bytes)
2021-12-07 13:39:13,231 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 7896 bytes)
2021-12-07 13:39:13,231 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2021-12-07 13:39:13,231 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2021-12-07 13:39:13,234 [Executor task launch worker for task 4] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:13,234 [Executor task launch worker for task 5] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:13,291 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 31
2021-12-07 13:39:13,292 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2_piece0 on qb:59396 in memory (size: 1985.0 B, free: 1990.8 MB)
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 37
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 29
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 33
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 41
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 35
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 42
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 47
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 30
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 43
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 34
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 46
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 39
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 25
2021-12-07 13:39:13,292 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 26
2021-12-07 13:39:13,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 38
2021-12-07 13:39:13,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 27
2021-12-07 13:39:13,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 36
2021-12-07 13:39:13,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 40
2021-12-07 13:39:13,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 48
2021-12-07 13:39:13,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 32
2021-12-07 13:39:13,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 45
2021-12-07 13:39:13,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 49
2021-12-07 13:39:13,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 44
2021-12-07 13:39:13,293 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 28
2021-12-07 13:39:13,616 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 797 bytes result sent to driver
2021-12-07 13:39:13,617 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 387 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:14,632 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2.0 (TID 5). 797 bytes result sent to driver
2021-12-07 13:39:14,632 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2.0 (TID 5) in 1401 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:14,632 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2021-12-07 13:39:14,633 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (count at PaidPromotionAdjustParameter.scala:119) finished in 1.409 s
2021-12-07 13:39:14,633 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: count at PaidPromotionAdjustParameter.scala:119, took 1.410878 s
2021-12-07 13:39:14,633 [main] INFO [PaidPromotion$] - 初次切分训练集数量：47173
2021-12-07 13:39:14,635 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:120
2021-12-07 13:39:14,635 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (count at PaidPromotionAdjustParameter.scala:120) with 2 output partitions
2021-12-07 13:39:14,635 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 3 (count at PaidPromotionAdjustParameter.scala:120)
2021-12-07 13:39:14,635 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:39:14,636 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:14,636 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 3 (MapPartitionsRDD[4] at randomSplit at PaidPromotionAdjustParameter.scala:112), which has no missing parents
2021-12-07 13:39:14,637 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 3.6 KB, free 1990.5 MB)
2021-12-07 13:39:14,640 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.5 MB)
2021-12-07 13:39:14,640 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on qb:59396 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:39:14,641 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:14,641 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[4] at randomSplit at PaidPromotionAdjustParameter.scala:112) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:14,641 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 3.0 with 2 tasks
2021-12-07 13:39:14,642 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 7896 bytes)
2021-12-07 13:39:14,642 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 7896 bytes)
2021-12-07 13:39:14,642 [Executor task launch worker for task 6] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 3.0 (TID 6)
2021-12-07 13:39:14,642 [Executor task launch worker for task 7] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 3.0 (TID 7)
2021-12-07 13:39:14,644 [Executor task launch worker for task 6] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:14,644 [Executor task launch worker for task 7] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:15,018 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 60
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 59
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 61
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 54
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 74
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 69
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 51
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 62
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 71
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 73
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 53
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 72
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 55
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 64
2021-12-07 13:39:15,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 57
2021-12-07 13:39:15,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 58
2021-12-07 13:39:15,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 66
2021-12-07 13:39:15,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 65
2021-12-07 13:39:15,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 67
2021-12-07 13:39:15,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 56
2021-12-07 13:39:15,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 68
2021-12-07 13:39:15,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 70
2021-12-07 13:39:15,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 52
2021-12-07 13:39:15,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 63
2021-12-07 13:39:15,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 50
2021-12-07 13:39:15,021 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_3_piece0 on qb:59396 in memory (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:39:15,238 [Executor task launch worker for task 6] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 3.0 (TID 6). 754 bytes result sent to driver
2021-12-07 13:39:15,238 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 3.0 (TID 6) in 596 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:15,665 [Executor task launch worker for task 7] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 3.0 (TID 7). 754 bytes result sent to driver
2021-12-07 13:39:15,666 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 3.0 (TID 7) in 1024 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:15,666 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2021-12-07 13:39:15,666 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 3 (count at PaidPromotionAdjustParameter.scala:120) finished in 1.030 s
2021-12-07 13:39:15,667 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: count at PaidPromotionAdjustParameter.scala:120, took 1.032290 s
2021-12-07 13:39:15,667 [main] INFO [PaidPromotion$] - 初次切分验证集数量：60121
2021-12-07 13:39:15,718 [main] INFO [PaidPromotion$] - -----------------------------------
2021-12-07 13:39:15,720 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:136
2021-12-07 13:39:15,726 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 6 (distinct at PaidPromotionAdjustParameter.scala:126)
2021-12-07 13:39:15,727 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 4 (count at PaidPromotionAdjustParameter.scala:136) with 2 output partitions
2021-12-07 13:39:15,727 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 5 (count at PaidPromotionAdjustParameter.scala:136)
2021-12-07 13:39:15,727 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 4)
2021-12-07 13:39:15,727 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 4)
2021-12-07 13:39:15,728 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 4 (MapPartitionsRDD[6] at distinct at PaidPromotionAdjustParameter.scala:126), which has no missing parents
2021-12-07 13:39:15,736 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 5.2 KB, free 1990.5 MB)
2021-12-07 13:39:15,738 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1990.5 MB)
2021-12-07 13:39:15,739 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on qb:59396 (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:39:15,739 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:15,741 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[6] at distinct at PaidPromotionAdjustParameter.scala:126) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:15,741 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 2 tasks
2021-12-07 13:39:15,742 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 8, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:39:15,742 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 4.0 (TID 9, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:39:15,742 [Executor task launch worker for task 8] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 8)
2021-12-07 13:39:15,742 [Executor task launch worker for task 9] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 4.0 (TID 9)
2021-12-07 13:39:15,746 [Executor task launch worker for task 8] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:15,746 [Executor task launch worker for task 9] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:16,101 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 87
2021-12-07 13:39:16,101 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 97
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 92
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 85
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 86
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 90
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 98
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 91
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 96
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 76
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 75
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 99
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 95
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 93
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 83
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 84
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 77
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 82
2021-12-07 13:39:16,102 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 88
2021-12-07 13:39:16,103 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 81
2021-12-07 13:39:16,103 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 79
2021-12-07 13:39:16,103 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 78
2021-12-07 13:39:16,103 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 94
2021-12-07 13:39:16,103 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 80
2021-12-07 13:39:16,103 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 89
2021-12-07 13:39:16,104 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_4_piece0 on qb:59396 in memory (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:39:16,377 [Executor task launch worker for task 8] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 8). 1032 bytes result sent to driver
2021-12-07 13:39:16,392 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 8) in 651 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:16,490 [Executor task launch worker for task 9] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 4.0 (TID 9). 1075 bytes result sent to driver
2021-12-07 13:39:16,491 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 4.0 (TID 9) in 749 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:16,491 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2021-12-07 13:39:16,491 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 4 (distinct at PaidPromotionAdjustParameter.scala:126) finished in 0.761 s
2021-12-07 13:39:16,492 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:16,492 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:16,492 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 5)
2021-12-07 13:39:16,493 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:16,495 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 5 (MapPartitionsRDD[8] at distinct at PaidPromotionAdjustParameter.scala:126), which has no missing parents
2021-12-07 13:39:16,499 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6 stored as values in memory (estimated size 3.7 KB, free 1990.5 MB)
2021-12-07 13:39:16,502 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.5 MB)
2021-12-07 13:39:16,502 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_6_piece0 in memory on qb:59396 (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:39:16,502 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 6 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:16,503 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at distinct at PaidPromotionAdjustParameter.scala:126) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:16,503 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 5.0 with 2 tasks
2021-12-07 13:39:16,504 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 5.0 (TID 10, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:16,504 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 5.0 (TID 11, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:39:16,505 [Executor task launch worker for task 10] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 5.0 (TID 10)
2021-12-07 13:39:16,505 [Executor task launch worker for task 11] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 5.0 (TID 11)
2021-12-07 13:39:16,515 [Executor task launch worker for task 11] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:16,515 [Executor task launch worker for task 10] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:16,516 [Executor task launch worker for task 10] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 4 ms
2021-12-07 13:39:16,516 [Executor task launch worker for task 11] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 4 ms
2021-12-07 13:39:16,637 [Executor task launch worker for task 10] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 5.0 (TID 10). 1098 bytes result sent to driver
2021-12-07 13:39:16,637 [Executor task launch worker for task 11] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 5.0 (TID 11). 1055 bytes result sent to driver
2021-12-07 13:39:16,637 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 5.0 (TID 10) in 134 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:16,638 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 5.0 (TID 11) in 134 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:16,638 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2021-12-07 13:39:16,638 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 5 (count at PaidPromotionAdjustParameter.scala:136) finished in 0.141 s
2021-12-07 13:39:16,639 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 4 finished: count at PaidPromotionAdjustParameter.scala:136, took 0.918064 s
2021-12-07 13:39:16,639 [main] INFO [PaidPromotion$] - 训练集用户数 = 44488
2021-12-07 13:39:16,642 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:137
2021-12-07 13:39:16,643 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 10 (distinct at PaidPromotionAdjustParameter.scala:127)
2021-12-07 13:39:16,643 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 5 (count at PaidPromotionAdjustParameter.scala:137) with 2 output partitions
2021-12-07 13:39:16,643 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 7 (count at PaidPromotionAdjustParameter.scala:137)
2021-12-07 13:39:16,643 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 6)
2021-12-07 13:39:16,643 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 6)
2021-12-07 13:39:16,643 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 6 (MapPartitionsRDD[10] at distinct at PaidPromotionAdjustParameter.scala:127), which has no missing parents
2021-12-07 13:39:16,645 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7 stored as values in memory (estimated size 5.2 KB, free 1990.4 MB)
2021-12-07 13:39:16,648 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1990.4 MB)
2021-12-07 13:39:16,649 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_7_piece0 in memory on qb:59396 (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:39:16,649 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 7 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:16,650 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[10] at distinct at PaidPromotionAdjustParameter.scala:127) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:16,650 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 6.0 with 2 tasks
2021-12-07 13:39:16,650 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 6.0 (TID 12, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:39:16,651 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 6.0 (TID 13, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:39:16,651 [Executor task launch worker for task 12] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 6.0 (TID 12)
2021-12-07 13:39:16,651 [Executor task launch worker for task 13] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 6.0 (TID 13)
2021-12-07 13:39:16,652 [Executor task launch worker for task 12] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:16,652 [Executor task launch worker for task 13] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:17,085 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 128
2021-12-07 13:39:17,086 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 129
2021-12-07 13:39:17,086 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 105
2021-12-07 13:39:17,086 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 101
2021-12-07 13:39:17,086 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 141
2021-12-07 13:39:17,086 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 102
2021-12-07 13:39:17,086 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 139
2021-12-07 13:39:17,086 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 104
2021-12-07 13:39:17,086 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 144
2021-12-07 13:39:17,086 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 123
2021-12-07 13:39:17,086 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 126
2021-12-07 13:39:17,086 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 118
2021-12-07 13:39:17,086 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 122
2021-12-07 13:39:17,087 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 110
2021-12-07 13:39:17,087 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 116
2021-12-07 13:39:17,087 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 131
2021-12-07 13:39:17,087 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 132
2021-12-07 13:39:17,087 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 147
2021-12-07 13:39:17,087 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 149
2021-12-07 13:39:17,087 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 140
2021-12-07 13:39:17,087 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 117
2021-12-07 13:39:17,087 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 107
2021-12-07 13:39:17,087 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_6_piece0 on qb:59396 in memory (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:39:17,088 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 142
2021-12-07 13:39:17,088 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 115
2021-12-07 13:39:17,088 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_5_piece0 on qb:59396 in memory (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 137
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 145
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 112
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 125
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 109
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 127
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 114
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 135
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 136
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 143
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 121
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 133
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 108
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 146
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 103
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 124
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 111
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 100
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 130
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 120
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 106
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 148
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 119
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 134
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 113
2021-12-07 13:39:17,089 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 138
2021-12-07 13:39:17,304 [Executor task launch worker for task 13] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 6.0 (TID 13). 1032 bytes result sent to driver
2021-12-07 13:39:17,305 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 6.0 (TID 13) in 654 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:17,369 [Executor task launch worker for task 12] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 6.0 (TID 12). 1032 bytes result sent to driver
2021-12-07 13:39:17,370 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 6.0 (TID 12) in 720 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:17,370 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2021-12-07 13:39:17,370 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 6 (distinct at PaidPromotionAdjustParameter.scala:127) finished in 0.726 s
2021-12-07 13:39:17,370 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:17,370 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:17,370 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 7)
2021-12-07 13:39:17,370 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:17,370 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 7 (MapPartitionsRDD[12] at distinct at PaidPromotionAdjustParameter.scala:127), which has no missing parents
2021-12-07 13:39:17,372 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8 stored as values in memory (estimated size 3.7 KB, free 1990.5 MB)
2021-12-07 13:39:17,376 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.5 MB)
2021-12-07 13:39:17,376 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_8_piece0 in memory on qb:59396 (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:39:17,377 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 8 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:17,377 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[12] at distinct at PaidPromotionAdjustParameter.scala:127) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:17,377 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 7.0 with 2 tasks
2021-12-07 13:39:17,378 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 7.0 (TID 14, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:17,378 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 7.0 (TID 15, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:39:17,378 [Executor task launch worker for task 15] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 7.0 (TID 15)
2021-12-07 13:39:17,378 [Executor task launch worker for task 14] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 7.0 (TID 14)
2021-12-07 13:39:17,379 [Executor task launch worker for task 15] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:17,379 [Executor task launch worker for task 15] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:17,380 [Executor task launch worker for task 14] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:17,380 [Executor task launch worker for task 14] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:17,436 [Executor task launch worker for task 15] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 7.0 (TID 15). 1055 bytes result sent to driver
2021-12-07 13:39:17,436 [Executor task launch worker for task 14] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 7.0 (TID 14). 1012 bytes result sent to driver
2021-12-07 13:39:17,436 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 7.0 (TID 15) in 58 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:17,436 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 7.0 (TID 14) in 58 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:17,437 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2021-12-07 13:39:17,437 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 7 (count at PaidPromotionAdjustParameter.scala:137) finished in 0.066 s
2021-12-07 13:39:17,437 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 5 finished: count at PaidPromotionAdjustParameter.scala:137, took 0.794877 s
2021-12-07 13:39:17,438 [main] INFO [PaidPromotion$] - 验证集用户数 = 55890
2021-12-07 13:39:17,442 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:138
2021-12-07 13:39:17,442 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 13 (intersection at PaidPromotionAdjustParameter.scala:128)
2021-12-07 13:39:17,442 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 14 (intersection at PaidPromotionAdjustParameter.scala:128)
2021-12-07 13:39:17,443 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 6 (count at PaidPromotionAdjustParameter.scala:138) with 2 output partitions
2021-12-07 13:39:17,443 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 12 (count at PaidPromotionAdjustParameter.scala:138)
2021-12-07 13:39:17,443 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 9, ShuffleMapStage 11)
2021-12-07 13:39:17,443 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 9, ShuffleMapStage 11)
2021-12-07 13:39:17,443 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 9 (MapPartitionsRDD[13] at intersection at PaidPromotionAdjustParameter.scala:128), which has no missing parents
2021-12-07 13:39:17,445 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9 stored as values in memory (estimated size 4.0 KB, free 1990.5 MB)
2021-12-07 13:39:17,447 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.4 MB)
2021-12-07 13:39:17,447 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_9_piece0 in memory on qb:59396 (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:39:17,448 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 9 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:17,448 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[13] at intersection at PaidPromotionAdjustParameter.scala:128) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:17,448 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 9.0 with 2 tasks
2021-12-07 13:39:17,448 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 11 (MapPartitionsRDD[14] at intersection at PaidPromotionAdjustParameter.scala:128), which has no missing parents
2021-12-07 13:39:17,448 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 9.0 (TID 16, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:39:17,449 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 9.0 (TID 17, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:39:17,449 [Executor task launch worker for task 16] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 9.0 (TID 16)
2021-12-07 13:39:17,449 [Executor task launch worker for task 17] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 9.0 (TID 17)
2021-12-07 13:39:17,449 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10 stored as values in memory (estimated size 4.0 KB, free 1990.4 MB)
2021-12-07 13:39:17,452 [Executor task launch worker for task 17] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:17,452 [Executor task launch worker for task 17] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:17,452 [Executor task launch worker for task 16] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:17,452 [Executor task launch worker for task 16] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:17,452 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.4 MB)
2021-12-07 13:39:17,453 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_10_piece0 in memory on qb:59396 (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:39:17,453 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 10 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:17,454 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[14] at intersection at PaidPromotionAdjustParameter.scala:128) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:17,454 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 11.0 with 2 tasks
2021-12-07 13:39:17,454 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 11.0 (TID 18, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:39:17,454 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 11.0 (TID 19, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:39:17,455 [Executor task launch worker for task 19] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 11.0 (TID 19)
2021-12-07 13:39:17,455 [Executor task launch worker for task 18] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 11.0 (TID 18)
2021-12-07 13:39:17,456 [Executor task launch worker for task 19] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:17,456 [Executor task launch worker for task 19] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:17,456 [Executor task launch worker for task 18] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:17,456 [Executor task launch worker for task 18] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:17,550 [Executor task launch worker for task 17] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 9.0 (TID 17). 1247 bytes result sent to driver
2021-12-07 13:39:17,550 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 9.0 (TID 17) in 101 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:17,561 [Executor task launch worker for task 16] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 9.0 (TID 16). 1247 bytes result sent to driver
2021-12-07 13:39:17,562 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 9.0 (TID 16) in 114 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:17,562 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2021-12-07 13:39:17,562 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 9 (intersection at PaidPromotionAdjustParameter.scala:128) finished in 0.118 s
2021-12-07 13:39:17,562 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:17,562 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set(ShuffleMapStage 11)
2021-12-07 13:39:17,562 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 12)
2021-12-07 13:39:17,562 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:17,566 [Executor task launch worker for task 18] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 11.0 (TID 18). 1161 bytes result sent to driver
2021-12-07 13:39:17,566 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 11.0 (TID 18) in 112 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:17,568 [Executor task launch worker for task 19] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 11.0 (TID 19). 1161 bytes result sent to driver
2021-12-07 13:39:17,569 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 11.0 (TID 19) in 115 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:17,569 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2021-12-07 13:39:17,569 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 11 (intersection at PaidPromotionAdjustParameter.scala:128) finished in 0.120 s
2021-12-07 13:39:17,569 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:17,569 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:17,569 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 12)
2021-12-07 13:39:17,569 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:17,569 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 12 (MapPartitionsRDD[18] at intersection at PaidPromotionAdjustParameter.scala:128), which has no missing parents
2021-12-07 13:39:17,571 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11 stored as values in memory (estimated size 3.6 KB, free 1990.4 MB)
2021-12-07 13:39:17,574 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.4 MB)
2021-12-07 13:39:17,575 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_11_piece0 in memory on qb:59396 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:39:17,575 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 11 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:17,576 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[18] at intersection at PaidPromotionAdjustParameter.scala:128) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:17,576 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 12.0 with 2 tasks
2021-12-07 13:39:17,577 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 12.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:17,577 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 12.0 (TID 21, localhost, executor driver, partition 1, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:17,577 [Executor task launch worker for task 20] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 12.0 (TID 20)
2021-12-07 13:39:17,577 [Executor task launch worker for task 21] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 12.0 (TID 21)
2021-12-07 13:39:17,580 [Executor task launch worker for task 21] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:17,580 [Executor task launch worker for task 20] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:17,580 [Executor task launch worker for task 21] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:17,580 [Executor task launch worker for task 20] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:17,582 [Executor task launch worker for task 20] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:17,582 [Executor task launch worker for task 21] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:17,582 [Executor task launch worker for task 20] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:17,582 [Executor task launch worker for task 21] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:17,704 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 196
2021-12-07 13:39:17,705 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 168
2021-12-07 13:39:17,705 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 191
2021-12-07 13:39:17,705 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 163
2021-12-07 13:39:17,705 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 197
2021-12-07 13:39:17,705 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 188
2021-12-07 13:39:17,705 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 154
2021-12-07 13:39:17,705 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 182
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 160
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 185
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 170
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 189
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 176
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 194
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 174
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 183
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 195
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 166
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 153
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 192
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 198
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 162
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 187
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 172
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 175
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 184
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 157
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 167
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 161
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 158
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 152
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 159
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 155
2021-12-07 13:39:17,706 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 181
2021-12-07 13:39:17,707 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 193
2021-12-07 13:39:17,707 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 171
2021-12-07 13:39:17,707 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_7_piece0 on qb:59396 in memory (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:39:17,708 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 190
2021-12-07 13:39:17,708 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 173
2021-12-07 13:39:17,708 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 165
2021-12-07 13:39:17,708 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 179
2021-12-07 13:39:17,708 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 169
2021-12-07 13:39:17,708 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 151
2021-12-07 13:39:17,708 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 150
2021-12-07 13:39:17,708 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_10_piece0 on qb:59396 in memory (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:39:17,709 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 177
2021-12-07 13:39:17,709 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 180
2021-12-07 13:39:17,709 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 178
2021-12-07 13:39:17,709 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 156
2021-12-07 13:39:17,709 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 186
2021-12-07 13:39:17,709 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 199
2021-12-07 13:39:17,709 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 164
2021-12-07 13:39:17,710 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_8_piece0 on qb:59396 in memory (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:39:17,710 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_9_piece0 on qb:59396 in memory (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:39:17,839 [Executor task launch worker for task 20] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 12.0 (TID 20). 1097 bytes result sent to driver
2021-12-07 13:39:17,839 [Executor task launch worker for task 21] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 12.0 (TID 21). 1097 bytes result sent to driver
2021-12-07 13:39:17,839 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 12.0 (TID 20) in 263 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:17,839 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 12.0 (TID 21) in 262 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:17,840 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 12.0, whose tasks have all completed, from pool 
2021-12-07 13:39:17,840 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 12 (count at PaidPromotionAdjustParameter.scala:138) finished in 0.270 s
2021-12-07 13:39:17,840 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 6 finished: count at PaidPromotionAdjustParameter.scala:138, took 0.398229 s
2021-12-07 13:39:17,841 [main] INFO [PaidPromotion$] - 共 同 用户数 = 5055
2021-12-07 13:39:17,844 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:139
2021-12-07 13:39:17,844 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 20 (distinct at PaidPromotionAdjustParameter.scala:131)
2021-12-07 13:39:17,844 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 7 (count at PaidPromotionAdjustParameter.scala:139) with 2 output partitions
2021-12-07 13:39:17,844 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 14 (count at PaidPromotionAdjustParameter.scala:139)
2021-12-07 13:39:17,844 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 13)
2021-12-07 13:39:17,844 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 13)
2021-12-07 13:39:17,845 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 13 (MapPartitionsRDD[20] at distinct at PaidPromotionAdjustParameter.scala:131), which has no missing parents
2021-12-07 13:39:17,846 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12 stored as values in memory (estimated size 5.2 KB, free 1990.5 MB)
2021-12-07 13:39:17,848 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1990.5 MB)
2021-12-07 13:39:17,848 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_12_piece0 in memory on qb:59396 (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:39:17,849 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 12 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:17,849 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[20] at distinct at PaidPromotionAdjustParameter.scala:131) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:17,849 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 13.0 with 2 tasks
2021-12-07 13:39:17,850 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 13.0 (TID 22, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:39:17,850 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 13.0 (TID 23, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:39:17,850 [Executor task launch worker for task 23] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 13.0 (TID 23)
2021-12-07 13:39:17,850 [Executor task launch worker for task 22] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 13.0 (TID 22)
2021-12-07 13:39:17,852 [Executor task launch worker for task 22] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:17,852 [Executor task launch worker for task 23] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:18,436 [Executor task launch worker for task 22] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 13.0 (TID 22). 989 bytes result sent to driver
2021-12-07 13:39:18,436 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 13.0 (TID 22) in 587 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:19,855 [Executor task launch worker for task 23] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 13.0 (TID 23). 989 bytes result sent to driver
2021-12-07 13:39:19,855 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 13.0 (TID 23) in 2005 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:19,855 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2021-12-07 13:39:19,856 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 13 (distinct at PaidPromotionAdjustParameter.scala:131) finished in 2.011 s
2021-12-07 13:39:19,856 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:19,856 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:19,856 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 14)
2021-12-07 13:39:19,856 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:19,856 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 14 (MapPartitionsRDD[22] at distinct at PaidPromotionAdjustParameter.scala:131), which has no missing parents
2021-12-07 13:39:19,857 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13 stored as values in memory (estimated size 3.7 KB, free 1990.5 MB)
2021-12-07 13:39:19,858 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.4 MB)
2021-12-07 13:39:19,859 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_13_piece0 in memory on qb:59396 (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:39:19,859 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 13 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:19,859 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[22] at distinct at PaidPromotionAdjustParameter.scala:131) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:19,859 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 14.0 with 2 tasks
2021-12-07 13:39:19,860 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 14.0 (TID 24, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:19,860 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 14.0 (TID 25, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:39:19,860 [Executor task launch worker for task 25] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 14.0 (TID 25)
2021-12-07 13:39:19,860 [Executor task launch worker for task 24] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 14.0 (TID 24)
2021-12-07 13:39:19,861 [Executor task launch worker for task 24] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:19,861 [Executor task launch worker for task 24] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:19,861 [Executor task launch worker for task 25] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:19,861 [Executor task launch worker for task 25] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:19,874 [Executor task launch worker for task 25] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 14.0 (TID 25). 967 bytes result sent to driver
2021-12-07 13:39:19,874 [Executor task launch worker for task 24] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 14.0 (TID 24). 967 bytes result sent to driver
2021-12-07 13:39:19,874 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 14.0 (TID 25) in 14 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:19,874 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 14.0 (TID 24) in 14 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:19,874 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2021-12-07 13:39:19,875 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 14 (count at PaidPromotionAdjustParameter.scala:139) finished in 0.019 s
2021-12-07 13:39:19,875 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 7 finished: count at PaidPromotionAdjustParameter.scala:139, took 2.031272 s
2021-12-07 13:39:19,875 [main] INFO [PaidPromotion$] - 训练集节目数 = 120
2021-12-07 13:39:19,878 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:140
2021-12-07 13:39:19,878 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 24 (distinct at PaidPromotionAdjustParameter.scala:132)
2021-12-07 13:39:19,878 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 8 (count at PaidPromotionAdjustParameter.scala:140) with 2 output partitions
2021-12-07 13:39:19,878 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 16 (count at PaidPromotionAdjustParameter.scala:140)
2021-12-07 13:39:19,878 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 15)
2021-12-07 13:39:19,878 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 15)
2021-12-07 13:39:19,878 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 15 (MapPartitionsRDD[24] at distinct at PaidPromotionAdjustParameter.scala:132), which has no missing parents
2021-12-07 13:39:19,880 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14 stored as values in memory (estimated size 5.2 KB, free 1990.4 MB)
2021-12-07 13:39:19,882 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1990.4 MB)
2021-12-07 13:39:19,883 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_14_piece0 in memory on qb:59396 (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:39:19,883 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 14 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:19,883 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[24] at distinct at PaidPromotionAdjustParameter.scala:132) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:19,883 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 15.0 with 2 tasks
2021-12-07 13:39:19,884 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 15.0 (TID 26, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:39:19,884 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 15.0 (TID 27, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:39:19,884 [Executor task launch worker for task 27] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 15.0 (TID 27)
2021-12-07 13:39:19,884 [Executor task launch worker for task 26] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 15.0 (TID 26)
2021-12-07 13:39:19,885 [Executor task launch worker for task 27] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:19,885 [Executor task launch worker for task 26] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 218
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 290
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 215
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 297
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 298
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 324
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 244
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 234
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 299
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 293
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 273
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 291
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 313
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 243
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 300
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 283
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 200
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 230
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 228
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 258
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 317
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 221
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 267
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 264
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 271
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 307
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 217
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 246
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 318
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 260
2021-12-07 13:39:20,123 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 237
2021-12-07 13:39:20,124 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_11_piece0 on qb:59396 in memory (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 203
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 249
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 301
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 309
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 227
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 321
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 262
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 266
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 279
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 292
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 207
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 224
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 277
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 240
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 306
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 288
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 305
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 210
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 296
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 212
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 302
2021-12-07 13:39:20,124 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 214
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 257
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 319
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 205
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 206
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 314
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 308
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 216
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 201
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 259
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 261
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 284
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 252
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 233
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 241
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 285
2021-12-07 13:39:20,125 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 295
2021-12-07 13:39:20,125 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_13_piece0 on qb:59396 in memory (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:39:20,126 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 322
2021-12-07 13:39:20,126 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_12_piece0 on qb:59396 in memory (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:39:20,126 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 278
2021-12-07 13:39:20,126 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 294
2021-12-07 13:39:20,126 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 276
2021-12-07 13:39:20,126 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 311
2021-12-07 13:39:20,126 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 282
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 315
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 304
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 250
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 287
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 204
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 286
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 280
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 323
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 225
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 226
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 256
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 289
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 202
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 265
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 209
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 232
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 245
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 211
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 268
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 274
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 269
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 208
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 275
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 229
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 239
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 270
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 303
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 312
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 231
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 223
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 238
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 272
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 219
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 254
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 320
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 316
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 248
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 220
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 247
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 263
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 235
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 242
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 251
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 222
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 255
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 310
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 236
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 253
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 281
2021-12-07 13:39:20,127 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 213
2021-12-07 13:39:20,540 [Executor task launch worker for task 26] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 15.0 (TID 26). 989 bytes result sent to driver
2021-12-07 13:39:20,541 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 15.0 (TID 26) in 658 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:20,844 [Executor task launch worker for task 27] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 15.0 (TID 27). 1032 bytes result sent to driver
2021-12-07 13:39:20,844 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 15.0 (TID 27) in 960 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:20,844 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 15.0, whose tasks have all completed, from pool 
2021-12-07 13:39:20,845 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 15 (distinct at PaidPromotionAdjustParameter.scala:132) finished in 0.966 s
2021-12-07 13:39:20,845 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:20,845 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:20,845 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 16)
2021-12-07 13:39:20,845 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:20,845 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 16 (MapPartitionsRDD[26] at distinct at PaidPromotionAdjustParameter.scala:132), which has no missing parents
2021-12-07 13:39:20,846 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15 stored as values in memory (estimated size 3.7 KB, free 1990.5 MB)
2021-12-07 13:39:20,848 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.5 MB)
2021-12-07 13:39:20,848 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_15_piece0 in memory on qb:59396 (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:39:20,848 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 15 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:20,849 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 16 (MapPartitionsRDD[26] at distinct at PaidPromotionAdjustParameter.scala:132) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:20,849 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 16.0 with 2 tasks
2021-12-07 13:39:20,849 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 16.0 (TID 28, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:20,849 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 16.0 (TID 29, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:39:20,849 [Executor task launch worker for task 28] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 16.0 (TID 28)
2021-12-07 13:39:20,849 [Executor task launch worker for task 29] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 16.0 (TID 29)
2021-12-07 13:39:20,851 [Executor task launch worker for task 29] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,851 [Executor task launch worker for task 28] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,851 [Executor task launch worker for task 28] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:20,851 [Executor task launch worker for task 29] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:20,861 [Executor task launch worker for task 29] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 16.0 (TID 29). 1011 bytes result sent to driver
2021-12-07 13:39:20,861 [Executor task launch worker for task 28] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 16.0 (TID 28). 1010 bytes result sent to driver
2021-12-07 13:39:20,862 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 16.0 (TID 29) in 13 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:20,862 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 16.0 (TID 28) in 13 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:20,862 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 16.0, whose tasks have all completed, from pool 
2021-12-07 13:39:20,862 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 16 (count at PaidPromotionAdjustParameter.scala:140) finished in 0.017 s
2021-12-07 13:39:20,862 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 8 finished: count at PaidPromotionAdjustParameter.scala:140, took 0.985137 s
2021-12-07 13:39:20,863 [main] INFO [PaidPromotion$] - 验证集节目数 = 127
2021-12-07 13:39:20,865 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:141
2021-12-07 13:39:20,865 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 27 (intersection at PaidPromotionAdjustParameter.scala:133)
2021-12-07 13:39:20,865 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 28 (intersection at PaidPromotionAdjustParameter.scala:133)
2021-12-07 13:39:20,866 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 9 (count at PaidPromotionAdjustParameter.scala:141) with 2 output partitions
2021-12-07 13:39:20,866 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 21 (count at PaidPromotionAdjustParameter.scala:141)
2021-12-07 13:39:20,866 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 20, ShuffleMapStage 18)
2021-12-07 13:39:20,866 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 20, ShuffleMapStage 18)
2021-12-07 13:39:20,866 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 18 (MapPartitionsRDD[27] at intersection at PaidPromotionAdjustParameter.scala:133), which has no missing parents
2021-12-07 13:39:20,867 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16 stored as values in memory (estimated size 4.0 KB, free 1990.5 MB)
2021-12-07 13:39:20,868 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.4 MB)
2021-12-07 13:39:20,869 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_16_piece0 in memory on qb:59396 (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:39:20,869 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 16 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:20,869 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[27] at intersection at PaidPromotionAdjustParameter.scala:133) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:20,869 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 18.0 with 2 tasks
2021-12-07 13:39:20,870 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 20 (MapPartitionsRDD[28] at intersection at PaidPromotionAdjustParameter.scala:133), which has no missing parents
2021-12-07 13:39:20,870 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 18.0 (TID 30, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:39:20,870 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 18.0 (TID 31, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:39:20,870 [Executor task launch worker for task 31] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 18.0 (TID 31)
2021-12-07 13:39:20,870 [Executor task launch worker for task 30] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 18.0 (TID 30)
2021-12-07 13:39:20,870 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17 stored as values in memory (estimated size 4.0 KB, free 1990.4 MB)
2021-12-07 13:39:20,871 [Executor task launch worker for task 30] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,871 [Executor task launch worker for task 30] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:20,871 [Executor task launch worker for task 31] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,871 [Executor task launch worker for task 31] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:20,872 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.4 MB)
2021-12-07 13:39:20,872 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_17_piece0 in memory on qb:59396 (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:39:20,873 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 17 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:20,873 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[28] at intersection at PaidPromotionAdjustParameter.scala:133) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:20,873 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 20.0 with 2 tasks
2021-12-07 13:39:20,874 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 20.0 (TID 32, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:39:20,874 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 20.0 (TID 33, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:39:20,874 [Executor task launch worker for task 32] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 20.0 (TID 32)
2021-12-07 13:39:20,874 [Executor task launch worker for task 33] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 20.0 (TID 33)
2021-12-07 13:39:20,875 [Executor task launch worker for task 32] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,875 [Executor task launch worker for task 32] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:20,875 [Executor task launch worker for task 33] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,875 [Executor task launch worker for task 33] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:20,887 [Executor task launch worker for task 30] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 18.0 (TID 30). 1118 bytes result sent to driver
2021-12-07 13:39:20,888 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 18.0 (TID 30) in 18 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:20,889 [Executor task launch worker for task 31] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 18.0 (TID 31). 1118 bytes result sent to driver
2021-12-07 13:39:20,889 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 18.0 (TID 31) in 19 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:20,889 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 18.0, whose tasks have all completed, from pool 
2021-12-07 13:39:20,889 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 18 (intersection at PaidPromotionAdjustParameter.scala:133) finished in 0.023 s
2021-12-07 13:39:20,889 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:20,889 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set(ShuffleMapStage 20)
2021-12-07 13:39:20,889 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 21)
2021-12-07 13:39:20,889 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:20,890 [Executor task launch worker for task 33] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 20.0 (TID 33). 1204 bytes result sent to driver
2021-12-07 13:39:20,891 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 20.0 (TID 33) in 17 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:20,891 [Executor task launch worker for task 32] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 20.0 (TID 32). 1161 bytes result sent to driver
2021-12-07 13:39:20,891 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 20.0 (TID 32) in 17 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:20,891 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 20.0, whose tasks have all completed, from pool 
2021-12-07 13:39:20,892 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 20 (intersection at PaidPromotionAdjustParameter.scala:133) finished in 0.022 s
2021-12-07 13:39:20,892 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:20,892 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:20,892 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 21)
2021-12-07 13:39:20,892 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:20,892 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 21 (MapPartitionsRDD[32] at intersection at PaidPromotionAdjustParameter.scala:133), which has no missing parents
2021-12-07 13:39:20,892 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18 stored as values in memory (estimated size 3.6 KB, free 1990.4 MB)
2021-12-07 13:39:20,894 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.4 MB)
2021-12-07 13:39:20,894 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_18_piece0 in memory on qb:59396 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:39:20,894 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 18 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:20,895 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 21 (MapPartitionsRDD[32] at intersection at PaidPromotionAdjustParameter.scala:133) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:20,895 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 21.0 with 2 tasks
2021-12-07 13:39:20,895 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 21.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:20,895 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 21.0 (TID 35, localhost, executor driver, partition 1, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:20,895 [Executor task launch worker for task 34] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 21.0 (TID 34)
2021-12-07 13:39:20,895 [Executor task launch worker for task 35] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 21.0 (TID 35)
2021-12-07 13:39:20,897 [Executor task launch worker for task 35] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,897 [Executor task launch worker for task 35] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:39:20,897 [Executor task launch worker for task 34] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,897 [Executor task launch worker for task 34] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:39:20,898 [Executor task launch worker for task 35] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,898 [Executor task launch worker for task 35] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:20,898 [Executor task launch worker for task 34] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,898 [Executor task launch worker for task 34] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:20,904 [Executor task launch worker for task 35] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 21.0 (TID 35). 1053 bytes result sent to driver
2021-12-07 13:39:20,904 [Executor task launch worker for task 34] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 21.0 (TID 34). 1053 bytes result sent to driver
2021-12-07 13:39:20,904 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 21.0 (TID 35) in 9 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:20,904 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 21.0 (TID 34) in 9 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:20,904 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 21.0, whose tasks have all completed, from pool 
2021-12-07 13:39:20,905 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 21 (count at PaidPromotionAdjustParameter.scala:141) finished in 0.012 s
2021-12-07 13:39:20,905 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 9 finished: count at PaidPromotionAdjustParameter.scala:141, took 0.039704 s
2021-12-07 13:39:20,905 [main] INFO [PaidPromotion$] - 共 同 节目数 = 115
2021-12-07 13:39:20,912 [main] INFO [org.apache.spark.SparkContext] - Starting job: collect at PaidPromotionAdjustParameter.scala:144
2021-12-07 13:39:20,912 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 10 (collect at PaidPromotionAdjustParameter.scala:144) with 2 output partitions
2021-12-07 13:39:20,912 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 26 (collect at PaidPromotionAdjustParameter.scala:144)
2021-12-07 13:39:20,912 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 25, ShuffleMapStage 23)
2021-12-07 13:39:20,912 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:20,912 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 26 (MapPartitionsRDD[18] at intersection at PaidPromotionAdjustParameter.scala:128), which has no missing parents
2021-12-07 13:39:20,913 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19 stored as values in memory (estimated size 3.8 KB, free 1990.4 MB)
2021-12-07 13:39:20,915 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.4 MB)
2021-12-07 13:39:20,915 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_19_piece0 in memory on qb:59396 (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:39:20,916 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 19 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:20,916 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 26 (MapPartitionsRDD[18] at intersection at PaidPromotionAdjustParameter.scala:128) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:20,916 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 26.0 with 2 tasks
2021-12-07 13:39:20,916 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 26.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:20,916 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 26.0 (TID 37, localhost, executor driver, partition 1, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:20,917 [Executor task launch worker for task 36] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 26.0 (TID 36)
2021-12-07 13:39:20,917 [Executor task launch worker for task 37] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 26.0 (TID 37)
2021-12-07 13:39:20,918 [Executor task launch worker for task 36] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,918 [Executor task launch worker for task 37] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,918 [Executor task launch worker for task 36] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:39:20,918 [Executor task launch worker for task 37] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:39:20,919 [Executor task launch worker for task 37] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,919 [Executor task launch worker for task 37] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:20,919 [Executor task launch worker for task 36] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:20,919 [Executor task launch worker for task 36] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 325
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 411
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 396
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 340
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 335
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 336
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 349
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 369
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 379
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 434
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 387
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 432
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 422
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 446
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 377
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 359
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 421
2021-12-07 13:39:21,003 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 429
2021-12-07 13:39:21,004 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_17_piece0 on qb:59396 in memory (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:39:21,004 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 388
2021-12-07 13:39:21,004 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 332
2021-12-07 13:39:21,004 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 428
2021-12-07 13:39:21,004 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 330
2021-12-07 13:39:21,004 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 438
2021-12-07 13:39:21,004 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 433
2021-12-07 13:39:21,004 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 409
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 441
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 445
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 448
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 374
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 404
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 398
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 426
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 395
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 344
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 405
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 363
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 367
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 383
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 425
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 437
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 392
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 385
2021-12-07 13:39:21,005 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 375
2021-12-07 13:39:21,006 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_15_piece0 on qb:59396 in memory (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 345
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 368
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 381
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 407
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 415
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 420
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 346
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 402
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 370
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 348
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 424
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 400
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 352
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 427
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 382
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 361
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 331
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 355
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 408
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 376
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 333
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 343
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 442
2021-12-07 13:39:21,006 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 397
2021-12-07 13:39:21,007 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_16_piece0 on qb:59396 in memory (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 356
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 393
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 413
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 443
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 436
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 410
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 373
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 447
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 364
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 365
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 334
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 435
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 341
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 389
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 350
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 371
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 419
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 372
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 351
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 412
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 399
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 386
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 357
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 358
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 430
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 327
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 444
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 439
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 338
2021-12-07 13:39:21,007 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 401
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 418
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 360
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 329
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 339
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 354
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 440
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 326
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 337
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 394
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 417
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 449
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 403
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 378
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 390
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 347
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 391
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 342
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 380
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 423
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 431
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 414
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 366
2021-12-07 13:39:21,008 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 416
2021-12-07 13:39:21,009 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_14_piece0 on qb:59396 in memory (size: 2.9 KB, free: 1990.8 MB)
2021-12-07 13:39:21,009 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 362
2021-12-07 13:39:21,009 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 406
2021-12-07 13:39:21,009 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 384
2021-12-07 13:39:21,010 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_18_piece0 on qb:59396 in memory (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:39:21,010 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 328
2021-12-07 13:39:21,010 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 353
2021-12-07 13:39:21,016 [Executor task launch worker for task 37] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 26.0 (TID 37). 86354 bytes result sent to driver
2021-12-07 13:39:21,018 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 26.0 (TID 37) in 102 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:21,018 [Executor task launch worker for task 36] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 26.0 (TID 36). 83470 bytes result sent to driver
2021-12-07 13:39:21,019 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 26.0 (TID 36) in 103 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:21,019 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 26.0, whose tasks have all completed, from pool 
2021-12-07 13:39:21,019 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 26 (collect at PaidPromotionAdjustParameter.scala:144) finished in 0.106 s
2021-12-07 13:39:21,020 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 10 finished: collect at PaidPromotionAdjustParameter.scala:144, took 0.107554 s
2021-12-07 13:39:21,024 [main] INFO [org.apache.spark.SparkContext] - Starting job: collect at PaidPromotionAdjustParameter.scala:145
2021-12-07 13:39:21,025 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 11 (collect at PaidPromotionAdjustParameter.scala:145) with 2 output partitions
2021-12-07 13:39:21,025 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 31 (collect at PaidPromotionAdjustParameter.scala:145)
2021-12-07 13:39:21,025 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 30, ShuffleMapStage 28)
2021-12-07 13:39:21,025 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:21,025 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 31 (MapPartitionsRDD[32] at intersection at PaidPromotionAdjustParameter.scala:133), which has no missing parents
2021-12-07 13:39:21,026 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20 stored as values in memory (estimated size 3.8 KB, free 1990.5 MB)
2021-12-07 13:39:21,028 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.5 MB)
2021-12-07 13:39:21,028 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_20_piece0 in memory on qb:59396 (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:39:21,029 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 20 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:21,029 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 31 (MapPartitionsRDD[32] at intersection at PaidPromotionAdjustParameter.scala:133) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:21,029 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 31.0 with 2 tasks
2021-12-07 13:39:21,029 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 31.0 (TID 38, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:21,029 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 31.0 (TID 39, localhost, executor driver, partition 1, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:21,030 [Executor task launch worker for task 38] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 31.0 (TID 38)
2021-12-07 13:39:21,030 [Executor task launch worker for task 39] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 31.0 (TID 39)
2021-12-07 13:39:21,031 [Executor task launch worker for task 39] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:21,031 [Executor task launch worker for task 38] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:21,031 [Executor task launch worker for task 38] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:39:21,031 [Executor task launch worker for task 39] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:39:21,032 [Executor task launch worker for task 39] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:21,032 [Executor task launch worker for task 39] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:21,032 [Executor task launch worker for task 38] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:21,032 [Executor task launch worker for task 38] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:21,037 [Executor task launch worker for task 38] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 31.0 (TID 38). 2741 bytes result sent to driver
2021-12-07 13:39:21,037 [Executor task launch worker for task 39] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 31.0 (TID 39). 2652 bytes result sent to driver
2021-12-07 13:39:21,037 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 31.0 (TID 38) in 8 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:21,038 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 31.0 (TID 39) in 8 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:21,038 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 31.0, whose tasks have all completed, from pool 
2021-12-07 13:39:21,038 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 31 (collect at PaidPromotionAdjustParameter.scala:145) finished in 0.013 s
2021-12-07 13:39:21,038 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 11 finished: collect at PaidPromotionAdjustParameter.scala:145, took 0.013705 s
2021-12-07 13:39:21,053 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:154
2021-12-07 13:39:21,054 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 12 (count at PaidPromotionAdjustParameter.scala:154) with 4 output partitions
2021-12-07 13:39:21,054 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 32 (count at PaidPromotionAdjustParameter.scala:154)
2021-12-07 13:39:21,054 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:39:21,055 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:21,055 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 32 (UnionRDD[35] at union at PaidPromotionAdjustParameter.scala:152), which has no missing parents
2021-12-07 13:39:21,063 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21 stored as values in memory (estimated size 180.7 KB, free 1990.3 MB)
2021-12-07 13:39:21,065 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_21_piece0 stored as bytes in memory (estimated size 64.4 KB, free 1990.2 MB)
2021-12-07 13:39:21,066 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_21_piece0 in memory on qb:59396 (size: 64.4 KB, free: 1990.7 MB)
2021-12-07 13:39:21,066 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 21 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:21,066 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ResultStage 32 (UnionRDD[35] at union at PaidPromotionAdjustParameter.scala:152) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:39:21,066 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 32.0 with 4 tasks
2021-12-07 13:39:21,068 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 32.0 (TID 40, localhost, executor driver, partition 0, ANY, 8005 bytes)
2021-12-07 13:39:21,068 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 32.0 (TID 41, localhost, executor driver, partition 1, ANY, 8005 bytes)
2021-12-07 13:39:21,068 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 32.0 (TID 42, localhost, executor driver, partition 2, ANY, 8005 bytes)
2021-12-07 13:39:21,068 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 32.0 (TID 43, localhost, executor driver, partition 3, ANY, 8005 bytes)
2021-12-07 13:39:21,069 [Executor task launch worker for task 40] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 32.0 (TID 40)
2021-12-07 13:39:21,069 [Executor task launch worker for task 41] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 32.0 (TID 41)
2021-12-07 13:39:21,069 [Executor task launch worker for task 43] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 32.0 (TID 43)
2021-12-07 13:39:21,069 [Executor task launch worker for task 42] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 32.0 (TID 42)
2021-12-07 13:39:21,072 [Executor task launch worker for task 41] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:21,072 [Executor task launch worker for task 40] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:21,072 [Executor task launch worker for task 42] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:21,073 [Executor task launch worker for task 43] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:22,080 [Executor task launch worker for task 42] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 32.0 (TID 42). 711 bytes result sent to driver
2021-12-07 13:39:22,080 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 32.0 (TID 42) in 1012 ms on localhost (executor driver) (1/4)
2021-12-07 13:39:22,604 [Executor task launch worker for task 41] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 32.0 (TID 41). 711 bytes result sent to driver
2021-12-07 13:39:22,604 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 32.0 (TID 41) in 1536 ms on localhost (executor driver) (2/4)
2021-12-07 13:39:22,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 479
2021-12-07 13:39:22,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 452
2021-12-07 13:39:22,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 486
2021-12-07 13:39:22,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 468
2021-12-07 13:39:22,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 498
2021-12-07 13:39:22,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 485
2021-12-07 13:39:22,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 489
2021-12-07 13:39:22,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 487
2021-12-07 13:39:22,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 456
2021-12-07 13:39:22,617 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 459
2021-12-07 13:39:22,618 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_20_piece0 on qb:59396 in memory (size: 2.2 KB, free: 1990.7 MB)
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 493
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 496
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 472
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 451
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 463
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 461
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 484
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 460
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 469
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 450
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 495
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 457
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 453
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 462
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 477
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 454
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 474
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 464
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 492
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 473
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 476
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 480
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 466
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 455
2021-12-07 13:39:22,618 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 481
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 488
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 458
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 471
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 475
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 465
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 497
2021-12-07 13:39:22,619 [dispatcher-event-loop-8] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_19_piece0 on qb:59396 in memory (size: 2.2 KB, free: 1990.7 MB)
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 490
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 491
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 482
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 499
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 483
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 478
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 494
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 467
2021-12-07 13:39:22,619 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 470
2021-12-07 13:39:22,621 [Executor task launch worker for task 43] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 32.0 (TID 43). 754 bytes result sent to driver
2021-12-07 13:39:22,621 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 32.0 (TID 43) in 1553 ms on localhost (executor driver) (3/4)
2021-12-07 13:39:22,963 [Executor task launch worker for task 40] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 32.0 (TID 40). 754 bytes result sent to driver
2021-12-07 13:39:22,963 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 32.0 (TID 40) in 1896 ms on localhost (executor driver) (4/4)
2021-12-07 13:39:22,963 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 32.0, whose tasks have all completed, from pool 
2021-12-07 13:39:22,964 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 32 (count at PaidPromotionAdjustParameter.scala:154) finished in 1.909 s
2021-12-07 13:39:22,964 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 12 finished: count at PaidPromotionAdjustParameter.scala:154, took 1.910881 s
2021-12-07 13:39:22,964 [main] INFO [PaidPromotion$] - 最终训练集数量：101055
2021-12-07 13:39:22,966 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:155
2021-12-07 13:39:22,966 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 13 (count at PaidPromotionAdjustParameter.scala:155) with 2 output partitions
2021-12-07 13:39:22,966 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 33 (count at PaidPromotionAdjustParameter.scala:155)
2021-12-07 13:39:22,966 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:39:22,966 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:22,966 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 33 (MapPartitionsRDD[33] at filter at PaidPromotionAdjustParameter.scala:148), which has no missing parents
2021-12-07 13:39:22,968 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22 stored as values in memory (estimated size 180.1 KB, free 1990.1 MB)
2021-12-07 13:39:22,970 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_22_piece0 stored as bytes in memory (estimated size 64.1 KB, free 1990.0 MB)
2021-12-07 13:39:22,970 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_22_piece0 in memory on qb:59396 (size: 64.1 KB, free: 1990.6 MB)
2021-12-07 13:39:22,971 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 22 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:22,972 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 33 (MapPartitionsRDD[33] at filter at PaidPromotionAdjustParameter.scala:148) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:22,972 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 33.0 with 2 tasks
2021-12-07 13:39:22,972 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 33.0 (TID 44, localhost, executor driver, partition 0, ANY, 7896 bytes)
2021-12-07 13:39:22,972 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 33.0 (TID 45, localhost, executor driver, partition 1, ANY, 7896 bytes)
2021-12-07 13:39:22,972 [Executor task launch worker for task 44] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 33.0 (TID 44)
2021-12-07 13:39:22,972 [Executor task launch worker for task 45] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 33.0 (TID 45)
2021-12-07 13:39:22,975 [Executor task launch worker for task 44] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:22,975 [Executor task launch worker for task 45] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:24,562 [Executor task launch worker for task 45] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 33.0 (TID 45). 710 bytes result sent to driver
2021-12-07 13:39:24,562 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 33.0 (TID 45) in 1590 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:26,028 [Executor task launch worker for task 44] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 33.0 (TID 44). 710 bytes result sent to driver
2021-12-07 13:39:26,028 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 33.0 (TID 44) in 3056 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:26,028 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 33.0, whose tasks have all completed, from pool 
2021-12-07 13:39:26,028 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 33 (count at PaidPromotionAdjustParameter.scala:155) finished in 3.062 s
2021-12-07 13:39:26,029 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 13 finished: count at PaidPromotionAdjustParameter.scala:155, took 3.062348 s
2021-12-07 13:39:26,029 [main] INFO [PaidPromotion$] - 最终验证集数量：6239
2021-12-07 13:39:26,060 [main] INFO [PaidPromotion$] - -----------------------------------
2021-12-07 13:39:26,061 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:167
2021-12-07 13:39:26,061 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 37 (distinct at PaidPromotionAdjustParameter.scala:158)
2021-12-07 13:39:26,061 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 14 (count at PaidPromotionAdjustParameter.scala:167) with 4 output partitions
2021-12-07 13:39:26,061 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 35 (count at PaidPromotionAdjustParameter.scala:167)
2021-12-07 13:39:26,061 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 34)
2021-12-07 13:39:26,061 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 34)
2021-12-07 13:39:26,062 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 34 (MapPartitionsRDD[37] at distinct at PaidPromotionAdjustParameter.scala:158), which has no missing parents
2021-12-07 13:39:26,063 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23 stored as values in memory (estimated size 182.3 KB, free 1989.8 MB)
2021-12-07 13:39:26,065 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_23_piece0 stored as bytes in memory (estimated size 65.3 KB, free 1989.7 MB)
2021-12-07 13:39:26,066 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_23_piece0 in memory on qb:59396 (size: 65.3 KB, free: 1990.6 MB)
2021-12-07 13:39:26,066 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 23 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:26,066 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[37] at distinct at PaidPromotionAdjustParameter.scala:158) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:39:26,066 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 34.0 with 4 tasks
2021-12-07 13:39:26,066 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 34.0 (TID 46, localhost, executor driver, partition 0, ANY, 7994 bytes)
2021-12-07 13:39:26,067 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 34.0 (TID 47, localhost, executor driver, partition 1, ANY, 7994 bytes)
2021-12-07 13:39:26,067 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 34.0 (TID 48, localhost, executor driver, partition 2, ANY, 7994 bytes)
2021-12-07 13:39:26,067 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 34.0 (TID 49, localhost, executor driver, partition 3, ANY, 7994 bytes)
2021-12-07 13:39:26,067 [Executor task launch worker for task 47] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 34.0 (TID 47)
2021-12-07 13:39:26,067 [Executor task launch worker for task 46] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 34.0 (TID 46)
2021-12-07 13:39:26,067 [Executor task launch worker for task 48] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 34.0 (TID 48)
2021-12-07 13:39:26,067 [Executor task launch worker for task 49] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 34.0 (TID 49)
2021-12-07 13:39:26,070 [Executor task launch worker for task 46] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:26,070 [Executor task launch worker for task 49] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:26,070 [Executor task launch worker for task 48] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:26,070 [Executor task launch worker for task 47] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 522
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 537
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 528
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 518
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 505
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 509
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 515
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 543
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 520
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 535
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 503
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 527
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 548
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 525
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 547
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 516
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 504
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 539
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 513
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 502
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 540
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 542
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 549
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 512
2021-12-07 13:39:26,527 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 529
2021-12-07 13:39:26,528 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_21_piece0 on qb:59396 in memory (size: 64.4 KB, free: 1990.6 MB)
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 506
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 521
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 524
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 517
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 523
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 508
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 511
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 526
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 514
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 533
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 507
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 544
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 532
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 500
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 531
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 501
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 545
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 534
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 538
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 519
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 530
2021-12-07 13:39:26,528 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 541
2021-12-07 13:39:26,529 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_22_piece0 on qb:59396 in memory (size: 64.1 KB, free: 1990.7 MB)
2021-12-07 13:39:26,529 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 536
2021-12-07 13:39:26,529 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 546
2021-12-07 13:39:26,529 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 510
2021-12-07 13:39:27,017 [Executor task launch worker for task 46] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 34.0 (TID 46). 1034 bytes result sent to driver
2021-12-07 13:39:27,017 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 34.0 (TID 46) in 951 ms on localhost (executor driver) (1/4)
2021-12-07 13:39:27,419 [Executor task launch worker for task 47] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 34.0 (TID 47). 1034 bytes result sent to driver
2021-12-07 13:39:27,419 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 34.0 (TID 47) in 1353 ms on localhost (executor driver) (2/4)
2021-12-07 13:39:27,581 [Executor task launch worker for task 49] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 34.0 (TID 49). 1034 bytes result sent to driver
2021-12-07 13:39:27,581 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 34.0 (TID 49) in 1514 ms on localhost (executor driver) (3/4)
2021-12-07 13:39:27,752 [Executor task launch worker for task 48] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 34.0 (TID 48). 1034 bytes result sent to driver
2021-12-07 13:39:27,753 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 34.0 (TID 48) in 1686 ms on localhost (executor driver) (4/4)
2021-12-07 13:39:27,753 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 34.0, whose tasks have all completed, from pool 
2021-12-07 13:39:27,753 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 34 (distinct at PaidPromotionAdjustParameter.scala:158) finished in 1.691 s
2021-12-07 13:39:27,753 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:27,753 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:27,753 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 35)
2021-12-07 13:39:27,753 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:27,753 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 35 (MapPartitionsRDD[39] at distinct at PaidPromotionAdjustParameter.scala:158), which has no missing parents
2021-12-07 13:39:27,754 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24 stored as values in memory (estimated size 3.7 KB, free 1990.2 MB)
2021-12-07 13:39:27,755 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.2 MB)
2021-12-07 13:39:27,756 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_24_piece0 in memory on qb:59396 (size: 2.2 KB, free: 1990.7 MB)
2021-12-07 13:39:27,756 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 24 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:27,756 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ResultStage 35 (MapPartitionsRDD[39] at distinct at PaidPromotionAdjustParameter.scala:158) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:39:27,756 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 35.0 with 4 tasks
2021-12-07 13:39:27,756 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 35.0 (TID 50, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:27,757 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 35.0 (TID 51, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:39:27,757 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 35.0 (TID 52, localhost, executor driver, partition 2, ANY, 7649 bytes)
2021-12-07 13:39:27,757 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 35.0 (TID 53, localhost, executor driver, partition 3, ANY, 7649 bytes)
2021-12-07 13:39:27,757 [Executor task launch worker for task 51] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 35.0 (TID 51)
2021-12-07 13:39:27,757 [Executor task launch worker for task 52] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 35.0 (TID 52)
2021-12-07 13:39:27,757 [Executor task launch worker for task 53] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 35.0 (TID 53)
2021-12-07 13:39:27,757 [Executor task launch worker for task 50] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 35.0 (TID 50)
2021-12-07 13:39:27,758 [Executor task launch worker for task 52] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:27,758 [Executor task launch worker for task 50] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:27,758 [Executor task launch worker for task 52] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:27,758 [Executor task launch worker for task 50] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:27,758 [Executor task launch worker for task 53] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:27,758 [Executor task launch worker for task 51] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:27,758 [Executor task launch worker for task 53] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:27,758 [Executor task launch worker for task 51] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:27,801 [Executor task launch worker for task 50] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 35.0 (TID 50). 1012 bytes result sent to driver
2021-12-07 13:39:27,801 [Executor task launch worker for task 52] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 35.0 (TID 52). 1012 bytes result sent to driver
2021-12-07 13:39:27,801 [Executor task launch worker for task 51] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 35.0 (TID 51). 1012 bytes result sent to driver
2021-12-07 13:39:27,801 [Executor task launch worker for task 53] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 35.0 (TID 53). 1012 bytes result sent to driver
2021-12-07 13:39:27,801 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 35.0 (TID 50) in 45 ms on localhost (executor driver) (1/4)
2021-12-07 13:39:27,801 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 35.0 (TID 51) in 45 ms on localhost (executor driver) (2/4)
2021-12-07 13:39:27,801 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 35.0 (TID 52) in 44 ms on localhost (executor driver) (3/4)
2021-12-07 13:39:27,802 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 35.0 (TID 53) in 45 ms on localhost (executor driver) (4/4)
2021-12-07 13:39:27,802 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 35.0, whose tasks have all completed, from pool 
2021-12-07 13:39:27,802 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 35 (count at PaidPromotionAdjustParameter.scala:167) finished in 0.049 s
2021-12-07 13:39:27,802 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 14 finished: count at PaidPromotionAdjustParameter.scala:167, took 1.742421 s
2021-12-07 13:39:27,803 [main] INFO [PaidPromotion$] - 最终训练集用户数 = 95323
2021-12-07 13:39:27,805 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:168
2021-12-07 13:39:27,805 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 41 (distinct at PaidPromotionAdjustParameter.scala:159)
2021-12-07 13:39:27,805 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 15 (count at PaidPromotionAdjustParameter.scala:168) with 2 output partitions
2021-12-07 13:39:27,805 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 37 (count at PaidPromotionAdjustParameter.scala:168)
2021-12-07 13:39:27,805 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 36)
2021-12-07 13:39:27,805 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 36)
2021-12-07 13:39:27,806 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 36 (MapPartitionsRDD[41] at distinct at PaidPromotionAdjustParameter.scala:159), which has no missing parents
2021-12-07 13:39:27,807 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25 stored as values in memory (estimated size 181.7 KB, free 1990.0 MB)
2021-12-07 13:39:27,810 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_25_piece0 stored as bytes in memory (estimated size 65.0 KB, free 1990.0 MB)
2021-12-07 13:39:27,810 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_25_piece0 in memory on qb:59396 (size: 65.0 KB, free: 1990.6 MB)
2021-12-07 13:39:27,810 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 25 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:27,810 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[41] at distinct at PaidPromotionAdjustParameter.scala:159) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:27,810 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 36.0 with 2 tasks
2021-12-07 13:39:27,810 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 36.0 (TID 54, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:39:27,811 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 36.0 (TID 55, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:39:27,811 [Executor task launch worker for task 55] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 36.0 (TID 55)
2021-12-07 13:39:27,811 [Executor task launch worker for task 54] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 36.0 (TID 54)
2021-12-07 13:39:27,814 [Executor task launch worker for task 54] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:27,814 [Executor task launch worker for task 55] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:28,232 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 572
2021-12-07 13:39:28,232 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 567
2021-12-07 13:39:28,232 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 594
2021-12-07 13:39:28,232 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 598
2021-12-07 13:39:28,232 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 553
2021-12-07 13:39:28,232 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 590
2021-12-07 13:39:28,232 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 564
2021-12-07 13:39:28,232 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 565
2021-12-07 13:39:28,233 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_24_piece0 on qb:59396 in memory (size: 2.2 KB, free: 1990.6 MB)
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 584
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 588
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 571
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 589
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 562
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 552
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 599
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 575
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 592
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 593
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 560
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 566
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 563
2021-12-07 13:39:28,233 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 579
2021-12-07 13:39:28,233 [dispatcher-event-loop-5] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_23_piece0 on qb:59396 in memory (size: 65.3 KB, free: 1990.7 MB)
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 573
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 568
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 554
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 597
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 595
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 580
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 555
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 583
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 577
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 570
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 551
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 556
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 550
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 576
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 591
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 557
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 585
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 582
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 569
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 574
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 586
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 596
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 587
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 578
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 581
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 559
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 558
2021-12-07 13:39:28,234 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 561
2021-12-07 13:39:29,048 [Executor task launch worker for task 54] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 36.0 (TID 54). 1032 bytes result sent to driver
2021-12-07 13:39:29,048 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 36.0 (TID 54) in 1238 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:29,089 [Executor task launch worker for task 55] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 36.0 (TID 55). 1032 bytes result sent to driver
2021-12-07 13:39:29,090 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 36.0 (TID 55) in 1280 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:29,090 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 36.0, whose tasks have all completed, from pool 
2021-12-07 13:39:29,090 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 36 (distinct at PaidPromotionAdjustParameter.scala:159) finished in 1.284 s
2021-12-07 13:39:29,090 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:29,090 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:29,090 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 37)
2021-12-07 13:39:29,090 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:29,090 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 37 (MapPartitionsRDD[43] at distinct at PaidPromotionAdjustParameter.scala:159), which has no missing parents
2021-12-07 13:39:29,091 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26 stored as values in memory (estimated size 3.7 KB, free 1990.2 MB)
2021-12-07 13:39:29,092 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_26_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.2 MB)
2021-12-07 13:39:29,092 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_26_piece0 in memory on qb:59396 (size: 2.2 KB, free: 1990.7 MB)
2021-12-07 13:39:29,093 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 26 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:29,093 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 37 (MapPartitionsRDD[43] at distinct at PaidPromotionAdjustParameter.scala:159) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:29,093 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 37.0 with 2 tasks
2021-12-07 13:39:29,093 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 37.0 (TID 56, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:29,093 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 37.0 (TID 57, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:39:29,093 [Executor task launch worker for task 56] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 37.0 (TID 56)
2021-12-07 13:39:29,093 [Executor task launch worker for task 57] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 37.0 (TID 57)
2021-12-07 13:39:29,094 [Executor task launch worker for task 56] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:29,094 [Executor task launch worker for task 57] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:29,094 [Executor task launch worker for task 56] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,094 [Executor task launch worker for task 57] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,112 [Executor task launch worker for task 56] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 37.0 (TID 56). 968 bytes result sent to driver
2021-12-07 13:39:29,112 [Executor task launch worker for task 57] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 37.0 (TID 57). 968 bytes result sent to driver
2021-12-07 13:39:29,113 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 37.0 (TID 56) in 20 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:29,113 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 37.0 (TID 57) in 20 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:29,113 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 37.0, whose tasks have all completed, from pool 
2021-12-07 13:39:29,113 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 37 (count at PaidPromotionAdjustParameter.scala:168) finished in 0.023 s
2021-12-07 13:39:29,113 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 15 finished: count at PaidPromotionAdjustParameter.scala:168, took 1.307846 s
2021-12-07 13:39:29,113 [main] INFO [PaidPromotion$] - 最终验证集用户数 = 5055
2021-12-07 13:39:29,115 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:169
2021-12-07 13:39:29,116 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 44 (intersection at PaidPromotionAdjustParameter.scala:160)
2021-12-07 13:39:29,116 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 45 (intersection at PaidPromotionAdjustParameter.scala:160)
2021-12-07 13:39:29,116 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 16 (count at PaidPromotionAdjustParameter.scala:169) with 4 output partitions
2021-12-07 13:39:29,116 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 42 (count at PaidPromotionAdjustParameter.scala:169)
2021-12-07 13:39:29,116 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 39, ShuffleMapStage 41)
2021-12-07 13:39:29,116 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 39, ShuffleMapStage 41)
2021-12-07 13:39:29,116 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 39 (MapPartitionsRDD[44] at intersection at PaidPromotionAdjustParameter.scala:160), which has no missing parents
2021-12-07 13:39:29,117 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27 stored as values in memory (estimated size 4.0 KB, free 1990.2 MB)
2021-12-07 13:39:29,119 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_27_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.2 MB)
2021-12-07 13:39:29,119 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_27_piece0 in memory on qb:59396 (size: 2.3 KB, free: 1990.7 MB)
2021-12-07 13:39:29,119 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 27 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:29,119 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ShuffleMapStage 39 (MapPartitionsRDD[44] at intersection at PaidPromotionAdjustParameter.scala:160) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:39:29,119 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 39.0 with 4 tasks
2021-12-07 13:39:29,119 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 41 (MapPartitionsRDD[45] at intersection at PaidPromotionAdjustParameter.scala:160), which has no missing parents
2021-12-07 13:39:29,120 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 39.0 (TID 58, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:39:29,120 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 39.0 (TID 59, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:39:29,120 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 39.0 (TID 60, localhost, executor driver, partition 2, ANY, 7638 bytes)
2021-12-07 13:39:29,120 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 39.0 (TID 61, localhost, executor driver, partition 3, ANY, 7638 bytes)
2021-12-07 13:39:29,120 [Executor task launch worker for task 59] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 39.0 (TID 59)
2021-12-07 13:39:29,120 [Executor task launch worker for task 60] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 39.0 (TID 60)
2021-12-07 13:39:29,120 [Executor task launch worker for task 61] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 39.0 (TID 61)
2021-12-07 13:39:29,120 [Executor task launch worker for task 58] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 39.0 (TID 58)
2021-12-07 13:39:29,120 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28 stored as values in memory (estimated size 4.0 KB, free 1990.2 MB)
2021-12-07 13:39:29,121 [Executor task launch worker for task 59] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:29,121 [Executor task launch worker for task 59] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,121 [Executor task launch worker for task 60] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:29,121 [Executor task launch worker for task 60] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,121 [Executor task launch worker for task 61] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:29,121 [Executor task launch worker for task 61] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,121 [Executor task launch worker for task 58] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:29,121 [Executor task launch worker for task 58] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,122 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_28_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.2 MB)
2021-12-07 13:39:29,122 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_28_piece0 in memory on qb:59396 (size: 2.3 KB, free: 1990.7 MB)
2021-12-07 13:39:29,122 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 28 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:29,123 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[45] at intersection at PaidPromotionAdjustParameter.scala:160) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:29,123 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 41.0 with 2 tasks
2021-12-07 13:39:29,123 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 41.0 (TID 62, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:39:29,123 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 41.0 (TID 63, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:39:29,124 [Executor task launch worker for task 62] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 41.0 (TID 62)
2021-12-07 13:39:29,124 [Executor task launch worker for task 63] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 41.0 (TID 63)
2021-12-07 13:39:29,125 [Executor task launch worker for task 63] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:29,125 [Executor task launch worker for task 62] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:29,125 [Executor task launch worker for task 63] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,125 [Executor task launch worker for task 62] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,155 [Executor task launch worker for task 63] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 41.0 (TID 63). 1206 bytes result sent to driver
2021-12-07 13:39:29,156 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 41.0 (TID 63) in 33 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:29,156 [Executor task launch worker for task 62] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 41.0 (TID 62). 1206 bytes result sent to driver
2021-12-07 13:39:29,156 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 41.0 (TID 62) in 33 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:29,156 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 41.0, whose tasks have all completed, from pool 
2021-12-07 13:39:29,156 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 41 (intersection at PaidPromotionAdjustParameter.scala:160) finished in 0.036 s
2021-12-07 13:39:29,157 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:29,157 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set(ShuffleMapStage 39)
2021-12-07 13:39:29,157 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 42)
2021-12-07 13:39:29,157 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:29,172 [Executor task launch worker for task 59] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 39.0 (TID 59). 1249 bytes result sent to driver
2021-12-07 13:39:29,172 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 39.0 (TID 59) in 52 ms on localhost (executor driver) (1/4)
2021-12-07 13:39:29,173 [Executor task launch worker for task 61] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 39.0 (TID 61). 1206 bytes result sent to driver
2021-12-07 13:39:29,173 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 39.0 (TID 61) in 53 ms on localhost (executor driver) (2/4)
2021-12-07 13:39:29,177 [Executor task launch worker for task 60] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 39.0 (TID 60). 1206 bytes result sent to driver
2021-12-07 13:39:29,177 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 39.0 (TID 60) in 57 ms on localhost (executor driver) (3/4)
2021-12-07 13:39:29,181 [Executor task launch worker for task 58] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 39.0 (TID 58). 1206 bytes result sent to driver
2021-12-07 13:39:29,181 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 39.0 (TID 58) in 61 ms on localhost (executor driver) (4/4)
2021-12-07 13:39:29,181 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 39.0, whose tasks have all completed, from pool 
2021-12-07 13:39:29,182 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 39 (intersection at PaidPromotionAdjustParameter.scala:160) finished in 0.065 s
2021-12-07 13:39:29,182 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:29,182 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:29,182 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 42)
2021-12-07 13:39:29,182 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:29,182 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 42 (MapPartitionsRDD[49] at intersection at PaidPromotionAdjustParameter.scala:160), which has no missing parents
2021-12-07 13:39:29,182 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29 stored as values in memory (estimated size 3.6 KB, free 1990.2 MB)
2021-12-07 13:39:29,183 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.2 MB)
2021-12-07 13:39:29,184 [dispatcher-event-loop-8] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_29_piece0 in memory on qb:59396 (size: 2.1 KB, free: 1990.7 MB)
2021-12-07 13:39:29,184 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 29 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:29,184 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ResultStage 42 (MapPartitionsRDD[49] at intersection at PaidPromotionAdjustParameter.scala:160) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:39:29,184 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 42.0 with 4 tasks
2021-12-07 13:39:29,184 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 42.0 (TID 64, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:29,185 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 42.0 (TID 65, localhost, executor driver, partition 1, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:29,185 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 42.0 (TID 66, localhost, executor driver, partition 2, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:29,185 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 42.0 (TID 67, localhost, executor driver, partition 3, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:29,185 [Executor task launch worker for task 66] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 42.0 (TID 66)
2021-12-07 13:39:29,185 [Executor task launch worker for task 67] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 42.0 (TID 67)
2021-12-07 13:39:29,185 [Executor task launch worker for task 65] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 42.0 (TID 65)
2021-12-07 13:39:29,185 [Executor task launch worker for task 64] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 42.0 (TID 64)
2021-12-07 13:39:29,186 [Executor task launch worker for task 66] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:39:29,186 [Executor task launch worker for task 66] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,186 [Executor task launch worker for task 64] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:39:29,186 [Executor task launch worker for task 64] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,186 [Executor task launch worker for task 67] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:39:29,186 [Executor task launch worker for task 65] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:39:29,186 [Executor task launch worker for task 67] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,186 [Executor task launch worker for task 65] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,188 [Executor task launch worker for task 66] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:29,188 [Executor task launch worker for task 64] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:29,188 [Executor task launch worker for task 66] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,188 [Executor task launch worker for task 64] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,188 [Executor task launch worker for task 65] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:29,188 [Executor task launch worker for task 65] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,188 [Executor task launch worker for task 67] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:29,188 [Executor task launch worker for task 67] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:29,242 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 649
2021-12-07 13:39:29,242 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 623
2021-12-07 13:39:29,242 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 631
2021-12-07 13:39:29,243 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_28_piece0 on qb:59396 in memory (size: 2.3 KB, free: 1990.7 MB)
2021-12-07 13:39:29,244 [dispatcher-event-loop-5] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_25_piece0 on qb:59396 in memory (size: 65.0 KB, free: 1990.8 MB)
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 646
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 613
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 617
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 600
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 605
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 618
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 616
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 619
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 621
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 624
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 637
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 632
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 641
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 648
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 607
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 602
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 630
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 620
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 633
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 636
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 614
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 603
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 609
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 626
2021-12-07 13:39:29,244 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 625
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 635
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 627
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 629
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 611
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 612
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 642
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 640
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 647
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 622
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 644
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 615
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 638
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 645
2021-12-07 13:39:29,245 [Executor task launch worker for task 65] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 42.0 (TID 65). 1054 bytes result sent to driver
2021-12-07 13:39:29,245 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_27_piece0 on qb:59396 in memory (size: 2.3 KB, free: 1990.8 MB)
2021-12-07 13:39:29,245 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 42.0 (TID 65) in 61 ms on localhost (executor driver) (1/4)
2021-12-07 13:39:29,245 [Executor task launch worker for task 66] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 42.0 (TID 66). 1054 bytes result sent to driver
2021-12-07 13:39:29,245 [Executor task launch worker for task 64] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 42.0 (TID 64). 1097 bytes result sent to driver
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 610
2021-12-07 13:39:29,245 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 608
2021-12-07 13:39:29,246 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 42.0 (TID 66) in 61 ms on localhost (executor driver) (2/4)
2021-12-07 13:39:29,246 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 42.0 (TID 64) in 62 ms on localhost (executor driver) (3/4)
2021-12-07 13:39:29,246 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_26_piece0 on qb:59396 in memory (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 13:39:29,246 [Executor task launch worker for task 67] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 42.0 (TID 67). 1097 bytes result sent to driver
2021-12-07 13:39:29,246 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 42.0 (TID 67) in 61 ms on localhost (executor driver) (4/4)
2021-12-07 13:39:29,246 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 42.0, whose tasks have all completed, from pool 
2021-12-07 13:39:29,246 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 601
2021-12-07 13:39:29,246 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 643
2021-12-07 13:39:29,246 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 634
2021-12-07 13:39:29,246 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 639
2021-12-07 13:39:29,246 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 604
2021-12-07 13:39:29,246 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 628
2021-12-07 13:39:29,246 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 606
2021-12-07 13:39:29,246 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 42 (count at PaidPromotionAdjustParameter.scala:169) finished in 0.064 s
2021-12-07 13:39:29,247 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 16 finished: count at PaidPromotionAdjustParameter.scala:169, took 0.130691 s
2021-12-07 13:39:29,247 [main] INFO [PaidPromotion$] - 最终共 同 用户数 = 5055
2021-12-07 13:39:29,249 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:170
2021-12-07 13:39:29,249 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 51 (distinct at PaidPromotionAdjustParameter.scala:162)
2021-12-07 13:39:29,249 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 17 (count at PaidPromotionAdjustParameter.scala:170) with 4 output partitions
2021-12-07 13:39:29,249 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 44 (count at PaidPromotionAdjustParameter.scala:170)
2021-12-07 13:39:29,249 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 43)
2021-12-07 13:39:29,249 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 43)
2021-12-07 13:39:29,249 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 43 (MapPartitionsRDD[51] at distinct at PaidPromotionAdjustParameter.scala:162), which has no missing parents
2021-12-07 13:39:29,251 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30 stored as values in memory (estimated size 182.3 KB, free 1990.3 MB)
2021-12-07 13:39:29,253 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_30_piece0 stored as bytes in memory (estimated size 65.3 KB, free 1990.2 MB)
2021-12-07 13:39:29,254 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_30_piece0 in memory on qb:59396 (size: 65.3 KB, free: 1990.7 MB)
2021-12-07 13:39:29,254 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 30 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:29,254 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[51] at distinct at PaidPromotionAdjustParameter.scala:162) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:39:29,254 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 43.0 with 4 tasks
2021-12-07 13:39:29,255 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 43.0 (TID 68, localhost, executor driver, partition 0, ANY, 7994 bytes)
2021-12-07 13:39:29,255 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 43.0 (TID 69, localhost, executor driver, partition 1, ANY, 7994 bytes)
2021-12-07 13:39:29,255 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 43.0 (TID 70, localhost, executor driver, partition 2, ANY, 7994 bytes)
2021-12-07 13:39:29,255 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 43.0 (TID 71, localhost, executor driver, partition 3, ANY, 7994 bytes)
2021-12-07 13:39:29,255 [Executor task launch worker for task 68] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 43.0 (TID 68)
2021-12-07 13:39:29,255 [Executor task launch worker for task 70] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 43.0 (TID 70)
2021-12-07 13:39:29,255 [Executor task launch worker for task 71] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 43.0 (TID 71)
2021-12-07 13:39:29,255 [Executor task launch worker for task 69] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 43.0 (TID 69)
2021-12-07 13:39:29,258 [Executor task launch worker for task 69] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:29,258 [Executor task launch worker for task 70] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:29,258 [Executor task launch worker for task 68] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:29,258 [Executor task launch worker for task 71] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:29,766 [Executor task launch worker for task 70] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 43.0 (TID 70). 991 bytes result sent to driver
2021-12-07 13:39:29,766 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 43.0 (TID 70) in 511 ms on localhost (executor driver) (1/4)
2021-12-07 13:39:30,508 [Executor task launch worker for task 68] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 43.0 (TID 68). 991 bytes result sent to driver
2021-12-07 13:39:30,509 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 43.0 (TID 68) in 1255 ms on localhost (executor driver) (2/4)
2021-12-07 13:39:32,635 [Executor task launch worker for task 71] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 43.0 (TID 71). 991 bytes result sent to driver
2021-12-07 13:39:32,636 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 43.0 (TID 71) in 3381 ms on localhost (executor driver) (3/4)
2021-12-07 13:39:32,644 [Executor task launch worker for task 69] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 43.0 (TID 69). 991 bytes result sent to driver
2021-12-07 13:39:32,644 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 43.0 (TID 69) in 3389 ms on localhost (executor driver) (4/4)
2021-12-07 13:39:32,644 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 43.0, whose tasks have all completed, from pool 
2021-12-07 13:39:32,644 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 43 (distinct at PaidPromotionAdjustParameter.scala:162) finished in 3.394 s
2021-12-07 13:39:32,645 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:32,645 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:32,645 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 44)
2021-12-07 13:39:32,645 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:32,645 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 44 (MapPartitionsRDD[53] at distinct at PaidPromotionAdjustParameter.scala:162), which has no missing parents
2021-12-07 13:39:32,645 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31 stored as values in memory (estimated size 3.7 KB, free 1990.2 MB)
2021-12-07 13:39:32,647 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_31_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.2 MB)
2021-12-07 13:39:32,647 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_31_piece0 in memory on qb:59396 (size: 2.2 KB, free: 1990.7 MB)
2021-12-07 13:39:32,647 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 31 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:32,648 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ResultStage 44 (MapPartitionsRDD[53] at distinct at PaidPromotionAdjustParameter.scala:162) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:39:32,648 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 44.0 with 4 tasks
2021-12-07 13:39:32,648 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 44.0 (TID 72, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:32,648 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 44.0 (TID 73, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:39:32,648 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 44.0 (TID 74, localhost, executor driver, partition 2, ANY, 7649 bytes)
2021-12-07 13:39:32,648 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 44.0 (TID 75, localhost, executor driver, partition 3, ANY, 7649 bytes)
2021-12-07 13:39:32,648 [Executor task launch worker for task 75] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 44.0 (TID 75)
2021-12-07 13:39:32,648 [Executor task launch worker for task 73] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 44.0 (TID 73)
2021-12-07 13:39:32,648 [Executor task launch worker for task 72] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 44.0 (TID 72)
2021-12-07 13:39:32,648 [Executor task launch worker for task 74] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 44.0 (TID 74)
2021-12-07 13:39:32,649 [Executor task launch worker for task 74] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:32,649 [Executor task launch worker for task 72] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:32,649 [Executor task launch worker for task 74] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:32,649 [Executor task launch worker for task 73] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:32,650 [Executor task launch worker for task 73] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:39:32,649 [Executor task launch worker for task 72] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:32,649 [Executor task launch worker for task 75] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:32,650 [Executor task launch worker for task 75] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:39:32,669 [Executor task launch worker for task 75] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 44.0 (TID 75). 1010 bytes result sent to driver
2021-12-07 13:39:32,669 [Executor task launch worker for task 74] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 44.0 (TID 74). 1053 bytes result sent to driver
2021-12-07 13:39:32,669 [Executor task launch worker for task 73] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 44.0 (TID 73). 1010 bytes result sent to driver
2021-12-07 13:39:32,669 [Executor task launch worker for task 72] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 44.0 (TID 72). 1053 bytes result sent to driver
2021-12-07 13:39:32,669 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 44.0 (TID 75) in 21 ms on localhost (executor driver) (1/4)
2021-12-07 13:39:32,669 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 44.0 (TID 74) in 21 ms on localhost (executor driver) (2/4)
2021-12-07 13:39:32,669 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 44.0 (TID 73) in 21 ms on localhost (executor driver) (3/4)
2021-12-07 13:39:32,669 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 44.0 (TID 72) in 21 ms on localhost (executor driver) (4/4)
2021-12-07 13:39:32,669 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 44.0, whose tasks have all completed, from pool 
2021-12-07 13:39:32,670 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 44 (count at PaidPromotionAdjustParameter.scala:170) finished in 0.025 s
2021-12-07 13:39:32,670 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 17 finished: count at PaidPromotionAdjustParameter.scala:170, took 3.421201 s
2021-12-07 13:39:32,670 [main] INFO [PaidPromotion$] - 最终训练集节目数 = 132
2021-12-07 13:39:32,672 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:171
2021-12-07 13:39:32,672 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 55 (distinct at PaidPromotionAdjustParameter.scala:163)
2021-12-07 13:39:32,672 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 18 (count at PaidPromotionAdjustParameter.scala:171) with 2 output partitions
2021-12-07 13:39:32,672 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 46 (count at PaidPromotionAdjustParameter.scala:171)
2021-12-07 13:39:32,672 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 45)
2021-12-07 13:39:32,672 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 45)
2021-12-07 13:39:32,673 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 45 (MapPartitionsRDD[55] at distinct at PaidPromotionAdjustParameter.scala:163), which has no missing parents
2021-12-07 13:39:32,674 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32 stored as values in memory (estimated size 181.7 KB, free 1990.0 MB)
2021-12-07 13:39:32,676 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_32_piece0 stored as bytes in memory (estimated size 64.9 KB, free 1990.0 MB)
2021-12-07 13:39:32,676 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_32_piece0 in memory on qb:59396 (size: 64.9 KB, free: 1990.6 MB)
2021-12-07 13:39:32,676 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 32 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:32,677 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[55] at distinct at PaidPromotionAdjustParameter.scala:163) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:32,677 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 45.0 with 2 tasks
2021-12-07 13:39:32,677 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 45.0 (TID 76, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:39:32,677 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 45.0 (TID 77, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:39:32,677 [Executor task launch worker for task 77] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 45.0 (TID 77)
2021-12-07 13:39:32,677 [Executor task launch worker for task 76] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 45.0 (TID 76)
2021-12-07 13:39:32,679 [Executor task launch worker for task 76] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:32,679 [Executor task launch worker for task 77] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 655
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 735
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 718
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 683
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 688
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 668
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 693
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 757
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 661
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 774
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 731
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 697
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 766
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 723
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 675
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 747
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 676
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 739
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 657
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 740
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 712
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 674
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 672
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 743
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 719
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 770
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 728
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 709
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 698
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 720
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 710
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 762
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 682
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 692
2021-12-07 13:39:33,019 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 716
2021-12-07 13:39:33,019 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_29_piece0 on qb:59396 in memory (size: 2.1 KB, free: 1990.6 MB)
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 745
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 741
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 654
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 665
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 673
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 656
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 755
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 724
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 763
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 751
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 714
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 717
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 756
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 708
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 679
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 765
2021-12-07 13:39:33,020 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 715
2021-12-07 13:39:33,020 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_31_piece0 on qb:59396 in memory (size: 2.2 KB, free: 1990.6 MB)
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 771
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 722
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 737
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 686
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 760
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 754
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 734
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 662
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 704
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 711
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 772
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 650
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 700
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 702
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 773
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 738
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 652
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 695
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 659
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 691
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 694
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 705
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 703
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 678
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 753
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 701
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 670
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 687
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 706
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 768
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 699
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 713
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 671
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 721
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 725
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 764
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 660
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 730
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 744
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 742
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 681
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 759
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 726
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 684
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 733
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 664
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 727
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 658
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 651
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 667
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 761
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 752
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 653
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 729
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 696
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 736
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 663
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 666
2021-12-07 13:39:33,021 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 690
2021-12-07 13:39:33,022 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_30_piece0 on qb:59396 in memory (size: 65.3 KB, free: 1990.7 MB)
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 750
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 677
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 769
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 680
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 685
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 758
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 732
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 669
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 707
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 746
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 767
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 689
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 748
2021-12-07 13:39:33,022 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 749
2021-12-07 13:39:33,883 [Executor task launch worker for task 77] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 45.0 (TID 77). 1032 bytes result sent to driver
2021-12-07 13:39:33,883 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 45.0 (TID 77) in 1206 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:33,897 [Executor task launch worker for task 76] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 45.0 (TID 76). 1032 bytes result sent to driver
2021-12-07 13:39:33,897 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 45.0 (TID 76) in 1220 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:33,897 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 45.0, whose tasks have all completed, from pool 
2021-12-07 13:39:33,897 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 45 (distinct at PaidPromotionAdjustParameter.scala:163) finished in 1.224 s
2021-12-07 13:39:33,898 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:33,898 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:33,898 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 46)
2021-12-07 13:39:33,898 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:33,898 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 46 (MapPartitionsRDD[57] at distinct at PaidPromotionAdjustParameter.scala:163), which has no missing parents
2021-12-07 13:39:33,898 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33 stored as values in memory (estimated size 3.7 KB, free 1990.2 MB)
2021-12-07 13:39:33,899 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_33_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.2 MB)
2021-12-07 13:39:33,900 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_33_piece0 in memory on qb:59396 (size: 2.2 KB, free: 1990.7 MB)
2021-12-07 13:39:33,900 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 33 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:33,900 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 46 (MapPartitionsRDD[57] at distinct at PaidPromotionAdjustParameter.scala:163) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:33,900 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 46.0 with 2 tasks
2021-12-07 13:39:33,900 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 46.0 (TID 78, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:33,901 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 46.0 (TID 79, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:39:33,901 [Executor task launch worker for task 79] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 46.0 (TID 79)
2021-12-07 13:39:33,903 [Executor task launch worker for task 78] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 46.0 (TID 78)
2021-12-07 13:39:33,903 [Executor task launch worker for task 79] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:33,903 [Executor task launch worker for task 79] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:33,904 [Executor task launch worker for task 78] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:33,904 [Executor task launch worker for task 78] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:33,915 [Executor task launch worker for task 79] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 46.0 (TID 79). 967 bytes result sent to driver
2021-12-07 13:39:33,915 [Executor task launch worker for task 78] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 46.0 (TID 78). 1053 bytes result sent to driver
2021-12-07 13:39:33,915 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 46.0 (TID 79) in 15 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:33,915 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 46.0 (TID 78) in 15 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:33,915 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 46.0, whose tasks have all completed, from pool 
2021-12-07 13:39:33,915 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 46 (count at PaidPromotionAdjustParameter.scala:171) finished in 0.017 s
2021-12-07 13:39:33,916 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 18 finished: count at PaidPromotionAdjustParameter.scala:171, took 1.242821 s
2021-12-07 13:39:33,916 [main] INFO [PaidPromotion$] - 最终验证集节目数 = 80
2021-12-07 13:39:33,918 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:172
2021-12-07 13:39:33,918 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 58 (intersection at PaidPromotionAdjustParameter.scala:164)
2021-12-07 13:39:33,918 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 59 (intersection at PaidPromotionAdjustParameter.scala:164)
2021-12-07 13:39:33,918 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 19 (count at PaidPromotionAdjustParameter.scala:172) with 4 output partitions
2021-12-07 13:39:33,918 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 51 (count at PaidPromotionAdjustParameter.scala:172)
2021-12-07 13:39:33,918 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 48, ShuffleMapStage 50)
2021-12-07 13:39:33,919 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 48, ShuffleMapStage 50)
2021-12-07 13:39:33,919 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 48 (MapPartitionsRDD[58] at intersection at PaidPromotionAdjustParameter.scala:164), which has no missing parents
2021-12-07 13:39:33,919 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_34 stored as values in memory (estimated size 4.0 KB, free 1990.2 MB)
2021-12-07 13:39:33,921 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_34_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.2 MB)
2021-12-07 13:39:33,922 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_34_piece0 in memory on qb:59396 (size: 2.3 KB, free: 1990.7 MB)
2021-12-07 13:39:33,922 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 34 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:33,922 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ShuffleMapStage 48 (MapPartitionsRDD[58] at intersection at PaidPromotionAdjustParameter.scala:164) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:39:33,922 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 48.0 with 4 tasks
2021-12-07 13:39:33,923 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 50 (MapPartitionsRDD[59] at intersection at PaidPromotionAdjustParameter.scala:164), which has no missing parents
2021-12-07 13:39:33,923 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 48.0 (TID 80, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:39:33,923 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 48.0 (TID 81, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:39:33,923 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 48.0 (TID 82, localhost, executor driver, partition 2, ANY, 7638 bytes)
2021-12-07 13:39:33,923 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 48.0 (TID 83, localhost, executor driver, partition 3, ANY, 7638 bytes)
2021-12-07 13:39:33,923 [Executor task launch worker for task 82] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 48.0 (TID 82)
2021-12-07 13:39:33,923 [Executor task launch worker for task 83] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 48.0 (TID 83)
2021-12-07 13:39:33,923 [Executor task launch worker for task 81] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 48.0 (TID 81)
2021-12-07 13:39:33,923 [Executor task launch worker for task 80] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 48.0 (TID 80)
2021-12-07 13:39:33,923 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_35 stored as values in memory (estimated size 4.0 KB, free 1990.2 MB)
2021-12-07 13:39:33,924 [Executor task launch worker for task 83] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:33,924 [Executor task launch worker for task 83] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:33,924 [Executor task launch worker for task 82] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:33,924 [Executor task launch worker for task 82] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:33,924 [Executor task launch worker for task 80] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:33,924 [Executor task launch worker for task 80] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:33,924 [Executor task launch worker for task 81] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:33,924 [Executor task launch worker for task 81] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:33,925 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_35_piece0 stored as bytes in memory (estimated size 2.3 KB, free 1990.2 MB)
2021-12-07 13:39:33,926 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_35_piece0 in memory on qb:59396 (size: 2.3 KB, free: 1990.7 MB)
2021-12-07 13:39:33,926 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 35 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:33,927 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[59] at intersection at PaidPromotionAdjustParameter.scala:164) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:33,927 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 50.0 with 2 tasks
2021-12-07 13:39:33,927 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 50.0 (TID 84, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 13:39:33,927 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 50.0 (TID 85, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 13:39:33,927 [Executor task launch worker for task 85] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 50.0 (TID 85)
2021-12-07 13:39:33,927 [Executor task launch worker for task 84] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 50.0 (TID 84)
2021-12-07 13:39:33,928 [Executor task launch worker for task 85] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:33,928 [Executor task launch worker for task 85] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:33,928 [Executor task launch worker for task 84] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:33,928 [Executor task launch worker for task 84] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:33,937 [Executor task launch worker for task 81] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 48.0 (TID 81). 1206 bytes result sent to driver
2021-12-07 13:39:33,938 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 48.0 (TID 81) in 15 ms on localhost (executor driver) (1/4)
2021-12-07 13:39:33,941 [Executor task launch worker for task 83] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 48.0 (TID 83). 1163 bytes result sent to driver
2021-12-07 13:39:33,942 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 48.0 (TID 83) in 19 ms on localhost (executor driver) (2/4)
2021-12-07 13:39:33,942 [Executor task launch worker for task 80] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 48.0 (TID 80). 1163 bytes result sent to driver
2021-12-07 13:39:33,943 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 48.0 (TID 80) in 20 ms on localhost (executor driver) (3/4)
2021-12-07 13:39:33,944 [Executor task launch worker for task 82] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 48.0 (TID 82). 1163 bytes result sent to driver
2021-12-07 13:39:33,944 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 48.0 (TID 82) in 21 ms on localhost (executor driver) (4/4)
2021-12-07 13:39:33,944 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 48.0, whose tasks have all completed, from pool 
2021-12-07 13:39:33,944 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 48 (intersection at PaidPromotionAdjustParameter.scala:164) finished in 0.025 s
2021-12-07 13:39:33,945 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:33,945 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set(ShuffleMapStage 50)
2021-12-07 13:39:33,945 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 51)
2021-12-07 13:39:33,945 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:33,950 [Executor task launch worker for task 84] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 50.0 (TID 84). 1163 bytes result sent to driver
2021-12-07 13:39:33,950 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 50.0 (TID 84) in 23 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:33,951 [Executor task launch worker for task 85] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 50.0 (TID 85). 1163 bytes result sent to driver
2021-12-07 13:39:33,951 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 50.0 (TID 85) in 24 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:33,952 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 50.0, whose tasks have all completed, from pool 
2021-12-07 13:39:33,952 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 50 (intersection at PaidPromotionAdjustParameter.scala:164) finished in 0.029 s
2021-12-07 13:39:33,952 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:33,952 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:33,952 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 51)
2021-12-07 13:39:33,952 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:33,952 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 51 (MapPartitionsRDD[63] at intersection at PaidPromotionAdjustParameter.scala:164), which has no missing parents
2021-12-07 13:39:33,952 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_36 stored as values in memory (estimated size 3.6 KB, free 1990.2 MB)
2021-12-07 13:39:33,954 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_36_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.2 MB)
2021-12-07 13:39:33,954 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_36_piece0 in memory on qb:59396 (size: 2.1 KB, free: 1990.7 MB)
2021-12-07 13:39:33,954 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 36 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:33,955 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ResultStage 51 (MapPartitionsRDD[63] at intersection at PaidPromotionAdjustParameter.scala:164) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:39:33,955 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 51.0 with 4 tasks
2021-12-07 13:39:33,955 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 51.0 (TID 86, localhost, executor driver, partition 0, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:33,955 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 51.0 (TID 87, localhost, executor driver, partition 1, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:33,955 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 51.0 (TID 88, localhost, executor driver, partition 2, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:33,955 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 51.0 (TID 89, localhost, executor driver, partition 3, PROCESS_LOCAL, 7712 bytes)
2021-12-07 13:39:33,955 [Executor task launch worker for task 86] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 51.0 (TID 86)
2021-12-07 13:39:33,955 [Executor task launch worker for task 89] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 51.0 (TID 89)
2021-12-07 13:39:33,955 [Executor task launch worker for task 88] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 51.0 (TID 88)
2021-12-07 13:39:33,955 [Executor task launch worker for task 87] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 51.0 (TID 87)
2021-12-07 13:39:33,956 [Executor task launch worker for task 87] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:39:33,956 [Executor task launch worker for task 88] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:39:33,956 [Executor task launch worker for task 88] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:33,956 [Executor task launch worker for task 86] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:39:33,956 [Executor task launch worker for task 87] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:33,957 [Executor task launch worker for task 86] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:39:33,957 [Executor task launch worker for task 89] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 4 blocks
2021-12-07 13:39:33,957 [Executor task launch worker for task 89] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:39:33,958 [Executor task launch worker for task 86] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:33,958 [Executor task launch worker for task 88] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:33,958 [Executor task launch worker for task 88] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:33,958 [Executor task launch worker for task 87] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:33,959 [Executor task launch worker for task 87] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:39:33,958 [Executor task launch worker for task 86] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:33,959 [Executor task launch worker for task 89] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 13:39:33,959 [Executor task launch worker for task 89] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:39:33,968 [Executor task launch worker for task 88] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 51.0 (TID 88). 1053 bytes result sent to driver
2021-12-07 13:39:33,968 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 51.0 (TID 88) in 13 ms on localhost (executor driver) (1/4)
2021-12-07 13:39:33,968 [Executor task launch worker for task 89] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 51.0 (TID 89). 1053 bytes result sent to driver
2021-12-07 13:39:33,968 [Executor task launch worker for task 86] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 51.0 (TID 86). 1053 bytes result sent to driver
2021-12-07 13:39:33,968 [Executor task launch worker for task 87] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 51.0 (TID 87). 1053 bytes result sent to driver
2021-12-07 13:39:33,969 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 51.0 (TID 89) in 14 ms on localhost (executor driver) (2/4)
2021-12-07 13:39:33,969 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 51.0 (TID 86) in 14 ms on localhost (executor driver) (3/4)
2021-12-07 13:39:33,969 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 51.0 (TID 87) in 14 ms on localhost (executor driver) (4/4)
2021-12-07 13:39:33,969 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 51.0, whose tasks have all completed, from pool 
2021-12-07 13:39:33,969 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 51 (count at PaidPromotionAdjustParameter.scala:172) finished in 0.017 s
2021-12-07 13:39:33,969 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 19 finished: count at PaidPromotionAdjustParameter.scala:172, took 0.051329 s
2021-12-07 13:39:33,970 [main] INFO [PaidPromotion$] - 最终共 同 节目数 = 80
2021-12-07 13:39:33,988 [main] INFO [org.apache.hadoop.conf.Configuration.deprecation] - mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2021-12-07 13:39:33,990 [main] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:39:34,139 [main] INFO [org.apache.spark.SparkContext] - Starting job: runJob at SparkHadoopWriter.scala:78
2021-12-07 13:39:34,140 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 20 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2021-12-07 13:39:34,140 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 52 (runJob at SparkHadoopWriter.scala:78)
2021-12-07 13:39:34,140 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:39:34,140 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:34,140 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 52 (MapPartitionsRDD[66] at saveAsTextFile at PaidPromotionAdjustParameter.scala:177), which has no missing parents
2021-12-07 13:39:34,151 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_37 stored as values in memory (estimated size 257.3 KB, free 1990.0 MB)
2021-12-07 13:39:34,153 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_37_piece0 stored as bytes in memory (estimated size 93.4 KB, free 1989.9 MB)
2021-12-07 13:39:34,153 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_37_piece0 in memory on qb:59396 (size: 93.4 KB, free: 1990.6 MB)
2021-12-07 13:39:34,154 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 37 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:34,154 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[66] at saveAsTextFile at PaidPromotionAdjustParameter.scala:177) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:39:34,154 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 52.0 with 1 tasks
2021-12-07 13:39:34,157 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 52.0 (TID 90, localhost, executor driver, partition 0, ANY, 8509 bytes)
2021-12-07 13:39:34,157 [Executor task launch worker for task 90] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 52.0 (TID 90)
2021-12-07 13:39:34,173 [Executor task launch worker for task 90] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:39:34,272 [Executor task launch worker for task 90] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:35,145 [Executor task launch worker for task 90] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:35,785 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 806
2021-12-07 13:39:35,785 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 834
2021-12-07 13:39:35,785 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 800
2021-12-07 13:39:35,785 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 851
2021-12-07 13:39:35,785 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 796
2021-12-07 13:39:35,785 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 863
2021-12-07 13:39:35,785 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 871
2021-12-07 13:39:35,785 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 869
2021-12-07 13:39:35,785 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 872
2021-12-07 13:39:35,787 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_34_piece0 on qb:59396 in memory (size: 2.3 KB, free: 1990.6 MB)
2021-12-07 13:39:35,787 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 824
2021-12-07 13:39:35,787 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 835
2021-12-07 13:39:35,787 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 829
2021-12-07 13:39:35,787 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 866
2021-12-07 13:39:35,787 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 825
2021-12-07 13:39:35,787 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 855
2021-12-07 13:39:35,787 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 841
2021-12-07 13:39:35,787 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 777
2021-12-07 13:39:35,787 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 787
2021-12-07 13:39:35,787 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 856
2021-12-07 13:39:35,787 [dispatcher-event-loop-8] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_33_piece0 on qb:59396 in memory (size: 2.2 KB, free: 1990.6 MB)
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 808
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 809
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 861
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 791
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 788
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 799
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 783
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 882
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 821
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 875
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 830
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 858
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 833
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 843
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 888
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 870
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 785
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 873
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 842
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 867
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 846
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 828
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 807
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 897
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 827
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 845
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 853
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 877
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 892
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 801
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 857
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 832
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 813
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 814
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 780
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 782
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 812
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 781
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 898
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 868
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 899
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 895
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 820
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 890
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 884
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 794
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 795
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 879
2021-12-07 13:39:35,788 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 784
2021-12-07 13:39:35,789 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_32_piece0 on qb:59396 in memory (size: 64.9 KB, free: 1990.7 MB)
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 864
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 880
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 790
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 793
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 817
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 883
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 859
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 802
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 805
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 837
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 881
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 818
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 889
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 822
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 779
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 815
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 885
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 840
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 848
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 823
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 849
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 811
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 836
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 838
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 878
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 887
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 786
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 789
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 816
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 852
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 778
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 826
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 876
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 844
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 839
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 865
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 819
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 798
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 831
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 776
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 891
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 886
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 893
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 894
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 792
2021-12-07 13:39:35,789 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 854
2021-12-07 13:39:35,790 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_35_piece0 on qb:59396 in memory (size: 2.3 KB, free: 1990.7 MB)
2021-12-07 13:39:35,790 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 775
2021-12-07 13:39:35,790 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 896
2021-12-07 13:39:35,790 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 850
2021-12-07 13:39:35,790 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 874
2021-12-07 13:39:35,790 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 804
2021-12-07 13:39:35,790 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 862
2021-12-07 13:39:35,790 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_36_piece0 on qb:59396 in memory (size: 2.1 KB, free: 1990.7 MB)
2021-12-07 13:39:35,791 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 797
2021-12-07 13:39:35,791 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 810
2021-12-07 13:39:35,791 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 860
2021-12-07 13:39:35,791 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 847
2021-12-07 13:39:35,791 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 803
2021-12-07 13:39:36,016 [Executor task launch worker for task 90] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:36,659 [Executor task launch worker for task 90] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:39,899 [Executor task launch worker for task 90] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - Saved output of task 'attempt_20211207133933_0066_m_000000_0' to hdfs://hdp1:8020/sk/chongqing/handle_data/set-train-device-production-data/_temporary/0/task_20211207133933_0066_m_000000
2021-12-07 13:39:39,900 [Executor task launch worker for task 90] INFO [org.apache.spark.mapred.SparkHadoopMapRedUtil] - attempt_20211207133933_0066_m_000000_0: Committed
2021-12-07 13:39:39,902 [Executor task launch worker for task 90] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 52.0 (TID 90). 998 bytes result sent to driver
2021-12-07 13:39:39,905 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 52.0 (TID 90) in 5750 ms on localhost (executor driver) (1/1)
2021-12-07 13:39:39,905 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 52.0, whose tasks have all completed, from pool 
2021-12-07 13:39:39,905 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 52 (runJob at SparkHadoopWriter.scala:78) finished in 5.765 s
2021-12-07 13:39:39,905 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 20 finished: runJob at SparkHadoopWriter.scala:78, took 5.766087 s
2021-12-07 13:39:40,035 [main] INFO [org.apache.spark.internal.io.SparkHadoopWriter] - Job job_20211207133933_0066 committed.
2021-12-07 13:39:40,039 [main] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:39:40,058 [main] INFO [org.apache.spark.SparkContext] - Starting job: runJob at SparkHadoopWriter.scala:78
2021-12-07 13:39:40,058 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 21 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2021-12-07 13:39:40,058 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 53 (runJob at SparkHadoopWriter.scala:78)
2021-12-07 13:39:40,058 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:39:40,058 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:40,059 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 53 (MapPartitionsRDD[69] at saveAsTextFile at PaidPromotionAdjustParameter.scala:180), which has no missing parents
2021-12-07 13:39:40,069 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_38 stored as values in memory (estimated size 256.8 KB, free 1989.9 MB)
2021-12-07 13:39:40,071 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_38_piece0 stored as bytes in memory (estimated size 92.9 KB, free 1989.8 MB)
2021-12-07 13:39:40,071 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_38_piece0 in memory on qb:59396 (size: 92.9 KB, free: 1990.6 MB)
2021-12-07 13:39:40,071 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 38 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:40,071 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[69] at saveAsTextFile at PaidPromotionAdjustParameter.scala:180) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:39:40,071 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 53.0 with 1 tasks
2021-12-07 13:39:40,072 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 53.0 (TID 91, localhost, executor driver, partition 0, ANY, 8337 bytes)
2021-12-07 13:39:40,072 [Executor task launch worker for task 91] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 53.0 (TID 91)
2021-12-07 13:39:40,077 [Executor task launch worker for task 91] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:39:40,099 [Executor task launch worker for task 91] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:42,942 [Executor task launch worker for task 91] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:44,372 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 900
2021-12-07 13:39:44,372 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 918
2021-12-07 13:39:44,372 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 916
2021-12-07 13:39:44,372 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 902
2021-12-07 13:39:44,373 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 911
2021-12-07 13:39:44,373 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 907
2021-12-07 13:39:44,373 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 914
2021-12-07 13:39:44,373 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 915
2021-12-07 13:39:44,373 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 905
2021-12-07 13:39:44,373 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 919
2021-12-07 13:39:44,373 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 903
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 910
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 913
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 904
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 909
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 924
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 906
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 922
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 912
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 908
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 921
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 917
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 923
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 920
2021-12-07 13:39:44,374 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 901
2021-12-07 13:39:44,375 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_37_piece0 on qb:59396 in memory (size: 93.4 KB, free: 1990.7 MB)
2021-12-07 13:39:45,016 [Executor task launch worker for task 91] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - Saved output of task 'attempt_20211207133940_0069_m_000000_0' to hdfs://hdp1:8020/sk/chongqing/handle_data/set-validation-device-production-data/_temporary/0/task_20211207133940_0069_m_000000
2021-12-07 13:39:45,016 [Executor task launch worker for task 91] INFO [org.apache.spark.mapred.SparkHadoopMapRedUtil] - attempt_20211207133940_0069_m_000000_0: Committed
2021-12-07 13:39:45,018 [Executor task launch worker for task 91] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 53.0 (TID 91). 998 bytes result sent to driver
2021-12-07 13:39:45,019 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 53.0 (TID 91) in 4947 ms on localhost (executor driver) (1/1)
2021-12-07 13:39:45,019 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 53.0, whose tasks have all completed, from pool 
2021-12-07 13:39:45,019 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 53 (runJob at SparkHadoopWriter.scala:78) finished in 4.960 s
2021-12-07 13:39:45,019 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 21 finished: runJob at SparkHadoopWriter.scala:78, took 4.961145 s
2021-12-07 13:39:45,552 [main] INFO [org.apache.spark.internal.io.SparkHadoopWriter] - Job job_20211207133940_0069 committed.
2021-12-07 13:39:45,564 [main] INFO [PaidPromotion$] - 验证集用户订购列表-----------------------------------
2021-12-07 13:39:45,566 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:190
2021-12-07 13:39:45,566 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 33 (filter at PaidPromotionAdjustParameter.scala:148)
2021-12-07 13:39:45,566 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 22 (count at PaidPromotionAdjustParameter.scala:190) with 2 output partitions
2021-12-07 13:39:45,566 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 55 (count at PaidPromotionAdjustParameter.scala:190)
2021-12-07 13:39:45,566 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 54)
2021-12-07 13:39:45,566 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 54)
2021-12-07 13:39:45,566 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 54 (MapPartitionsRDD[33] at filter at PaidPromotionAdjustParameter.scala:148), which has no missing parents
2021-12-07 13:39:45,568 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_39 stored as values in memory (estimated size 182.2 KB, free 1989.9 MB)
2021-12-07 13:39:45,569 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_39_piece0 stored as bytes in memory (estimated size 65.1 KB, free 1989.9 MB)
2021-12-07 13:39:45,570 [dispatcher-event-loop-5] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_39_piece0 in memory on qb:59396 (size: 65.1 KB, free: 1990.6 MB)
2021-12-07 13:39:45,570 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 39 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:45,570 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[33] at filter at PaidPromotionAdjustParameter.scala:148) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:45,570 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 54.0 with 2 tasks
2021-12-07 13:39:45,571 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 54.0 (TID 92, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:39:45,571 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 54.0 (TID 93, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:39:45,571 [Executor task launch worker for task 93] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 54.0 (TID 93)
2021-12-07 13:39:45,571 [Executor task launch worker for task 92] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 54.0 (TID 92)
2021-12-07 13:39:45,573 [Executor task launch worker for task 92] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:45,573 [Executor task launch worker for task 93] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:46,722 [Executor task launch worker for task 92] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 54.0 (TID 92). 860 bytes result sent to driver
2021-12-07 13:39:46,722 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 54.0 (TID 92) in 1152 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:46,832 [Executor task launch worker for task 93] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 54.0 (TID 93). 860 bytes result sent to driver
2021-12-07 13:39:46,832 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 54.0 (TID 93) in 1261 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:46,832 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 54.0, whose tasks have all completed, from pool 
2021-12-07 13:39:46,832 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 54 (filter at PaidPromotionAdjustParameter.scala:148) finished in 1.265 s
2021-12-07 13:39:46,832 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:46,832 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:46,832 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 55)
2021-12-07 13:39:46,832 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:46,833 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 55 (MapPartitionsRDD[71] at map at PaidPromotionAdjustParameter.scala:184), which has no missing parents
2021-12-07 13:39:46,835 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_40 stored as values in memory (estimated size 183.1 KB, free 1989.7 MB)
2021-12-07 13:39:46,838 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_40_piece0 stored as bytes in memory (estimated size 65.5 KB, free 1989.6 MB)
2021-12-07 13:39:46,838 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_40_piece0 in memory on qb:59396 (size: 65.5 KB, free: 1990.6 MB)
2021-12-07 13:39:46,838 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 40 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:46,838 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 55 (MapPartitionsRDD[71] at map at PaidPromotionAdjustParameter.scala:184) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:46,838 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 55.0 with 2 tasks
2021-12-07 13:39:46,839 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 55.0 (TID 94, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:46,839 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 55.0 (TID 95, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:39:46,839 [Executor task launch worker for task 95] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 55.0 (TID 95)
2021-12-07 13:39:46,839 [Executor task launch worker for task 94] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 55.0 (TID 94)
2021-12-07 13:39:46,841 [Executor task launch worker for task 94] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:46,841 [Executor task launch worker for task 95] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:46,841 [Executor task launch worker for task 94] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:46,841 [Executor task launch worker for task 95] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:46,877 [Executor task launch worker for task 94] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 55.0 (TID 94). 1097 bytes result sent to driver
2021-12-07 13:39:46,877 [Executor task launch worker for task 95] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 55.0 (TID 95). 1097 bytes result sent to driver
2021-12-07 13:39:46,877 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 55.0 (TID 94) in 38 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:46,877 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 55.0 (TID 95) in 38 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:46,877 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 55.0, whose tasks have all completed, from pool 
2021-12-07 13:39:46,877 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 55 (count at PaidPromotionAdjustParameter.scala:190) finished in 0.044 s
2021-12-07 13:39:46,877 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 22 finished: count at PaidPromotionAdjustParameter.scala:190, took 1.312210 s
2021-12-07 13:39:46,878 [main] INFO [PaidPromotion$] - 验证集用户列表数量：5055
2021-12-07 13:39:46,886 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at PaidPromotionAdjustParameter.scala:191
2021-12-07 13:39:46,886 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 23 (take at PaidPromotionAdjustParameter.scala:191) with 1 output partitions
2021-12-07 13:39:46,886 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 57 (take at PaidPromotionAdjustParameter.scala:191)
2021-12-07 13:39:46,886 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 56)
2021-12-07 13:39:46,886 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:46,886 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 57 (MapPartitionsRDD[71] at map at PaidPromotionAdjustParameter.scala:184), which has no missing parents
2021-12-07 13:39:46,888 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_41 stored as values in memory (estimated size 183.2 KB, free 1989.5 MB)
2021-12-07 13:39:46,890 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_41_piece0 stored as bytes in memory (estimated size 65.6 KB, free 1989.4 MB)
2021-12-07 13:39:46,890 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_41_piece0 in memory on qb:59396 (size: 65.6 KB, free: 1990.5 MB)
2021-12-07 13:39:46,890 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 41 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:46,890 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[71] at map at PaidPromotionAdjustParameter.scala:184) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:39:46,891 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 57.0 with 1 tasks
2021-12-07 13:39:46,891 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 57.0 (TID 96, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:46,891 [Executor task launch worker for task 96] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 57.0 (TID 96)
2021-12-07 13:39:46,893 [Executor task launch worker for task 96] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:46,893 [Executor task launch worker for task 96] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:46,907 [Executor task launch worker for task 96] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 57.0 (TID 96). 2714 bytes result sent to driver
2021-12-07 13:39:46,908 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 57.0 (TID 96) in 17 ms on localhost (executor driver) (1/1)
2021-12-07 13:39:46,908 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 57.0, whose tasks have all completed, from pool 
2021-12-07 13:39:46,908 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 57 (take at PaidPromotionAdjustParameter.scala:191) finished in 0.021 s
2021-12-07 13:39:46,908 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 23 finished: take at PaidPromotionAdjustParameter.scala:191, took 0.022020 s
2021-12-07 13:39:46,919 [main] INFO [PaidPromotion$] - 验证集产品包列表-----------------------------------
2021-12-07 13:39:46,920 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:202
2021-12-07 13:39:46,921 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 72 (map at PaidPromotionAdjustParameter.scala:194)
2021-12-07 13:39:46,921 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 24 (count at PaidPromotionAdjustParameter.scala:202) with 2 output partitions
2021-12-07 13:39:46,921 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 59 (count at PaidPromotionAdjustParameter.scala:202)
2021-12-07 13:39:46,921 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 58)
2021-12-07 13:39:46,921 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 58)
2021-12-07 13:39:46,921 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 58 (MapPartitionsRDD[72] at map at PaidPromotionAdjustParameter.scala:194), which has no missing parents
2021-12-07 13:39:46,923 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_42 stored as values in memory (estimated size 182.3 KB, free 1989.2 MB)
2021-12-07 13:39:46,925 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_42_piece0 stored as bytes in memory (estimated size 65.2 KB, free 1989.2 MB)
2021-12-07 13:39:46,925 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_42_piece0 in memory on qb:59396 (size: 65.2 KB, free: 1990.4 MB)
2021-12-07 13:39:46,925 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 42 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:46,926 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 58 (MapPartitionsRDD[72] at map at PaidPromotionAdjustParameter.scala:194) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:46,926 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 58.0 with 2 tasks
2021-12-07 13:39:46,927 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 58.0 (TID 97, localhost, executor driver, partition 0, ANY, 7885 bytes)
2021-12-07 13:39:46,927 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 58.0 (TID 98, localhost, executor driver, partition 1, ANY, 7885 bytes)
2021-12-07 13:39:46,927 [Executor task launch worker for task 97] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 58.0 (TID 97)
2021-12-07 13:39:46,927 [Executor task launch worker for task 98] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 58.0 (TID 98)
2021-12-07 13:39:46,929 [Executor task launch worker for task 98] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:46,929 [Executor task launch worker for task 97] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 990
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 979
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 942
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 993
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 999
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 948
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 971
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1014
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1022
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1009
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 970
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 931
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 946
2021-12-07 13:39:47,692 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 956
2021-12-07 13:39:47,693 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_39_piece0 on qb:59396 in memory (size: 65.1 KB, free: 1990.5 MB)
2021-12-07 13:39:47,693 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1004
2021-12-07 13:39:47,693 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 955
2021-12-07 13:39:47,693 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 997
2021-12-07 13:39:47,693 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 989
2021-12-07 13:39:47,693 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 984
2021-12-07 13:39:47,693 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 968
2021-12-07 13:39:47,693 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 927
2021-12-07 13:39:47,693 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1020
2021-12-07 13:39:47,693 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1021
2021-12-07 13:39:47,693 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 936
2021-12-07 13:39:47,693 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 941
2021-12-07 13:39:47,693 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 980
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1002
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 937
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 952
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 987
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1018
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 939
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1017
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 977
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 932
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 938
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 953
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 967
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 954
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 961
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 976
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 957
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 966
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1015
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1001
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 930
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 965
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 994
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 934
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1011
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 959
2021-12-07 13:39:47,694 [dispatcher-event-loop-8] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_40_piece0 on qb:59396 in memory (size: 65.5 KB, free: 1990.6 MB)
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 929
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 945
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 992
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 996
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1006
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1007
2021-12-07 13:39:47,694 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 943
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 975
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 991
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 963
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 969
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 926
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1013
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1016
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 949
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 944
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 928
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 973
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1005
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1019
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 972
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 974
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 960
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 962
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 964
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1012
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 983
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 950
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1000
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1008
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1024
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 940
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 933
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 988
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 986
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 951
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 998
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1010
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1023
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 947
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 925
2021-12-07 13:39:47,695 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_41_piece0 on qb:59396 in memory (size: 65.6 KB, free: 1990.6 MB)
2021-12-07 13:39:47,695 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 958
2021-12-07 13:39:47,696 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 981
2021-12-07 13:39:47,696 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_38_piece0 on qb:59396 in memory (size: 92.9 KB, free: 1990.7 MB)
2021-12-07 13:39:47,696 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 985
2021-12-07 13:39:47,696 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 995
2021-12-07 13:39:47,696 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1003
2021-12-07 13:39:47,696 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 978
2021-12-07 13:39:47,696 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 935
2021-12-07 13:39:47,696 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 982
2021-12-07 13:39:48,073 [Executor task launch worker for task 98] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 58.0 (TID 98). 903 bytes result sent to driver
2021-12-07 13:39:48,074 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 58.0 (TID 98) in 1147 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:48,075 [Executor task launch worker for task 97] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 58.0 (TID 97). 903 bytes result sent to driver
2021-12-07 13:39:48,076 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 58.0 (TID 97) in 1150 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:48,076 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 58.0, whose tasks have all completed, from pool 
2021-12-07 13:39:48,076 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 58 (map at PaidPromotionAdjustParameter.scala:194) finished in 1.155 s
2021-12-07 13:39:48,076 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:48,076 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:48,076 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 59)
2021-12-07 13:39:48,076 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:48,076 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 59 (MapPartitionsRDD[74] at map at PaidPromotionAdjustParameter.scala:196), which has no missing parents
2021-12-07 13:39:48,078 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_43 stored as values in memory (estimated size 183.2 KB, free 1990.0 MB)
2021-12-07 13:39:48,079 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_43_piece0 stored as bytes in memory (estimated size 65.5 KB, free 1990.0 MB)
2021-12-07 13:39:48,080 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_43_piece0 in memory on qb:59396 (size: 65.5 KB, free: 1990.6 MB)
2021-12-07 13:39:48,080 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 43 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:48,080 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 59 (MapPartitionsRDD[74] at map at PaidPromotionAdjustParameter.scala:196) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:39:48,080 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 59.0 with 2 tasks
2021-12-07 13:39:48,080 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 59.0 (TID 99, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:48,081 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 59.0 (TID 100, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:39:48,081 [Executor task launch worker for task 100] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 59.0 (TID 100)
2021-12-07 13:39:48,081 [Executor task launch worker for task 99] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 59.0 (TID 99)
2021-12-07 13:39:48,083 [Executor task launch worker for task 100] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:48,083 [Executor task launch worker for task 99] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:48,083 [Executor task launch worker for task 100] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:48,083 [Executor task launch worker for task 99] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:48,101 [Executor task launch worker for task 99] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 59.0 (TID 99). 1053 bytes result sent to driver
2021-12-07 13:39:48,102 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 59.0 (TID 99) in 22 ms on localhost (executor driver) (1/2)
2021-12-07 13:39:48,104 [Executor task launch worker for task 100] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 59.0 (TID 100). 1053 bytes result sent to driver
2021-12-07 13:39:48,104 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 59.0 (TID 100) in 24 ms on localhost (executor driver) (2/2)
2021-12-07 13:39:48,104 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 59.0, whose tasks have all completed, from pool 
2021-12-07 13:39:48,104 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 59 (count at PaidPromotionAdjustParameter.scala:202) finished in 0.028 s
2021-12-07 13:39:48,104 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 24 finished: count at PaidPromotionAdjustParameter.scala:202, took 1.183744 s
2021-12-07 13:39:48,105 [main] INFO [PaidPromotion$] - 验证集产品包列表数量：80
2021-12-07 13:39:48,111 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at PaidPromotionAdjustParameter.scala:203
2021-12-07 13:39:48,111 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 25 (take at PaidPromotionAdjustParameter.scala:203) with 1 output partitions
2021-12-07 13:39:48,111 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 61 (take at PaidPromotionAdjustParameter.scala:203)
2021-12-07 13:39:48,111 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 60)
2021-12-07 13:39:48,111 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:48,111 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 61 (MapPartitionsRDD[74] at map at PaidPromotionAdjustParameter.scala:196), which has no missing parents
2021-12-07 13:39:48,112 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_44 stored as values in memory (estimated size 183.3 KB, free 1989.8 MB)
2021-12-07 13:39:48,114 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_44_piece0 stored as bytes in memory (estimated size 65.6 KB, free 1989.7 MB)
2021-12-07 13:39:48,114 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_44_piece0 in memory on qb:59396 (size: 65.6 KB, free: 1990.6 MB)
2021-12-07 13:39:48,115 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 44 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:48,115 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[74] at map at PaidPromotionAdjustParameter.scala:196) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:39:48,115 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 61.0 with 1 tasks
2021-12-07 13:39:48,115 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 61.0 (TID 101, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:48,115 [Executor task launch worker for task 101] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 61.0 (TID 101)
2021-12-07 13:39:48,117 [Executor task launch worker for task 101] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:48,117 [Executor task launch worker for task 101] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:48,126 [Executor task launch worker for task 101] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 61.0 (TID 101). 34761 bytes result sent to driver
2021-12-07 13:39:48,126 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 61.0 (TID 101) in 11 ms on localhost (executor driver) (1/1)
2021-12-07 13:39:48,127 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 61.0, whose tasks have all completed, from pool 
2021-12-07 13:39:48,127 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 61 (take at PaidPromotionAdjustParameter.scala:203) finished in 0.016 s
2021-12-07 13:39:48,127 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 25 finished: take at PaidPromotionAdjustParameter.scala:203, took 0.016130 s
2021-12-07 13:39:48,134 [main] INFO [PaidPromotion$] - 训练集用户订购列表-----------------------------------
2021-12-07 13:39:48,135 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:213
2021-12-07 13:39:48,136 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 35 (union at PaidPromotionAdjustParameter.scala:152)
2021-12-07 13:39:48,136 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 26 (count at PaidPromotionAdjustParameter.scala:213) with 4 output partitions
2021-12-07 13:39:48,136 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 63 (count at PaidPromotionAdjustParameter.scala:213)
2021-12-07 13:39:48,136 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 62)
2021-12-07 13:39:48,136 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 62)
2021-12-07 13:39:48,136 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 62 (UnionRDD[35] at union at PaidPromotionAdjustParameter.scala:152), which has no missing parents
2021-12-07 13:39:48,137 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_45 stored as values in memory (estimated size 182.7 KB, free 1989.6 MB)
2021-12-07 13:39:48,139 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_45_piece0 stored as bytes in memory (estimated size 65.5 KB, free 1989.5 MB)
2021-12-07 13:39:48,139 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_45_piece0 in memory on qb:59396 (size: 65.5 KB, free: 1990.5 MB)
2021-12-07 13:39:48,139 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 45 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:48,140 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ShuffleMapStage 62 (UnionRDD[35] at union at PaidPromotionAdjustParameter.scala:152) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:39:48,140 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 62.0 with 4 tasks
2021-12-07 13:39:48,140 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 62.0 (TID 102, localhost, executor driver, partition 0, ANY, 7994 bytes)
2021-12-07 13:39:48,140 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 62.0 (TID 103, localhost, executor driver, partition 1, ANY, 7994 bytes)
2021-12-07 13:39:48,140 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 62.0 (TID 104, localhost, executor driver, partition 2, ANY, 7994 bytes)
2021-12-07 13:39:48,140 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 62.0 (TID 105, localhost, executor driver, partition 3, ANY, 7994 bytes)
2021-12-07 13:39:48,141 [Executor task launch worker for task 102] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 62.0 (TID 102)
2021-12-07 13:39:48,141 [Executor task launch worker for task 104] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 62.0 (TID 104)
2021-12-07 13:39:48,141 [Executor task launch worker for task 105] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 62.0 (TID 105)
2021-12-07 13:39:48,141 [Executor task launch worker for task 103] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 62.0 (TID 103)
2021-12-07 13:39:48,142 [Executor task launch worker for task 104] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:48,142 [Executor task launch worker for task 102] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:48,142 [Executor task launch worker for task 105] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:48,142 [Executor task launch worker for task 103] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:49,226 [Executor task launch worker for task 103] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 62.0 (TID 103). 862 bytes result sent to driver
2021-12-07 13:39:49,226 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 62.0 (TID 103) in 1086 ms on localhost (executor driver) (1/4)
2021-12-07 13:39:49,474 [Executor task launch worker for task 104] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 62.0 (TID 104). 862 bytes result sent to driver
2021-12-07 13:39:49,474 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 62.0 (TID 104) in 1334 ms on localhost (executor driver) (2/4)
2021-12-07 13:39:49,837 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1063
2021-12-07 13:39:49,837 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1081
2021-12-07 13:39:49,837 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1082
2021-12-07 13:39:49,837 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1065
2021-12-07 13:39:49,837 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1026
2021-12-07 13:39:49,837 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1078
2021-12-07 13:39:49,837 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1084
2021-12-07 13:39:49,837 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1085
2021-12-07 13:39:49,837 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1091
2021-12-07 13:39:49,837 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1049
2021-12-07 13:39:49,837 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1094
2021-12-07 13:39:49,838 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_43_piece0 on qb:59396 in memory (size: 65.5 KB, free: 1990.6 MB)
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1075
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1060
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1067
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1047
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1043
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1058
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1055
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1079
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1052
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1041
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1062
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1086
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1037
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1035
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1098
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1039
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1066
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1099
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1071
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1072
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1040
2021-12-07 13:39:49,838 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1051
2021-12-07 13:39:49,839 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_42_piece0 on qb:59396 in memory (size: 65.2 KB, free: 1990.6 MB)
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1080
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1053
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1038
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1089
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1077
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1027
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1028
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1029
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1073
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1032
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1030
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1087
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1031
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1064
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1076
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1093
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1034
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1042
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1048
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1092
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1045
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1054
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1025
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1033
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1036
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1083
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1090
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1074
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1069
2021-12-07 13:39:49,839 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1057
2021-12-07 13:39:49,840 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1070
2021-12-07 13:39:49,840 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1046
2021-12-07 13:39:49,840 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1061
2021-12-07 13:39:49,840 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1068
2021-12-07 13:39:49,840 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1096
2021-12-07 13:39:49,840 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1044
2021-12-07 13:39:49,840 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1095
2021-12-07 13:39:49,840 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1097
2021-12-07 13:39:49,840 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1050
2021-12-07 13:39:49,840 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1059
2021-12-07 13:39:49,840 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1088
2021-12-07 13:39:49,840 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_44_piece0 on qb:59396 in memory (size: 65.6 KB, free: 1990.7 MB)
2021-12-07 13:39:49,840 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1056
2021-12-07 13:39:50,637 [Executor task launch worker for task 105] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 62.0 (TID 105). 948 bytes result sent to driver
2021-12-07 13:39:50,637 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 62.0 (TID 105) in 2497 ms on localhost (executor driver) (3/4)
2021-12-07 13:39:51,101 [Executor task launch worker for task 102] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 62.0 (TID 102). 905 bytes result sent to driver
2021-12-07 13:39:51,101 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 62.0 (TID 102) in 2961 ms on localhost (executor driver) (4/4)
2021-12-07 13:39:51,101 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 62.0, whose tasks have all completed, from pool 
2021-12-07 13:39:51,101 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 62 (union at PaidPromotionAdjustParameter.scala:152) finished in 2.965 s
2021-12-07 13:39:51,101 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:51,101 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:51,101 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 63)
2021-12-07 13:39:51,101 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:51,102 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 63 (MapPartitionsRDD[76] at map at PaidPromotionAdjustParameter.scala:207), which has no missing parents
2021-12-07 13:39:51,103 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_46 stored as values in memory (estimated size 183.6 KB, free 1990.0 MB)
2021-12-07 13:39:51,105 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_46_piece0 stored as bytes in memory (estimated size 65.7 KB, free 1990.0 MB)
2021-12-07 13:39:51,105 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_46_piece0 in memory on qb:59396 (size: 65.7 KB, free: 1990.6 MB)
2021-12-07 13:39:51,105 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 46 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:51,105 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ResultStage 63 (MapPartitionsRDD[76] at map at PaidPromotionAdjustParameter.scala:207) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:39:51,105 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 63.0 with 4 tasks
2021-12-07 13:39:51,106 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 63.0 (TID 106, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:51,106 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 63.0 (TID 107, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:39:51,106 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 63.0 (TID 108, localhost, executor driver, partition 2, ANY, 7649 bytes)
2021-12-07 13:39:51,106 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 63.0 (TID 109, localhost, executor driver, partition 3, ANY, 7649 bytes)
2021-12-07 13:39:51,106 [Executor task launch worker for task 106] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 63.0 (TID 106)
2021-12-07 13:39:51,106 [Executor task launch worker for task 107] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 63.0 (TID 107)
2021-12-07 13:39:51,106 [Executor task launch worker for task 108] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 63.0 (TID 108)
2021-12-07 13:39:51,106 [Executor task launch worker for task 109] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 63.0 (TID 109)
2021-12-07 13:39:51,108 [Executor task launch worker for task 108] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:51,108 [Executor task launch worker for task 106] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:51,108 [Executor task launch worker for task 108] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:51,108 [Executor task launch worker for task 106] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:51,108 [Executor task launch worker for task 107] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:51,108 [Executor task launch worker for task 107] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:51,108 [Executor task launch worker for task 109] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:51,108 [Executor task launch worker for task 109] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:51,228 [dispatcher-event-loop-5] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_45_piece0 on qb:59396 in memory (size: 65.5 KB, free: 1990.7 MB)
2021-12-07 13:39:51,232 [Executor task launch worker for task 108] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 63.0 (TID 108). 1098 bytes result sent to driver
2021-12-07 13:39:51,232 [Executor task launch worker for task 106] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 63.0 (TID 106). 1098 bytes result sent to driver
2021-12-07 13:39:51,233 [Executor task launch worker for task 109] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 63.0 (TID 109). 1098 bytes result sent to driver
2021-12-07 13:39:51,233 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 63.0 (TID 108) in 127 ms on localhost (executor driver) (1/4)
2021-12-07 13:39:51,233 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 63.0 (TID 106) in 128 ms on localhost (executor driver) (2/4)
2021-12-07 13:39:51,233 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 63.0 (TID 109) in 127 ms on localhost (executor driver) (3/4)
2021-12-07 13:39:51,233 [Executor task launch worker for task 107] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 63.0 (TID 107). 1098 bytes result sent to driver
2021-12-07 13:39:51,233 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 63.0 (TID 107) in 127 ms on localhost (executor driver) (4/4)
2021-12-07 13:39:51,233 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 63.0, whose tasks have all completed, from pool 
2021-12-07 13:39:51,234 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 63 (count at PaidPromotionAdjustParameter.scala:213) finished in 0.132 s
2021-12-07 13:39:51,234 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 26 finished: count at PaidPromotionAdjustParameter.scala:213, took 3.098728 s
2021-12-07 13:39:51,234 [main] INFO [PaidPromotion$] - 训练集用户列表数量：95323
2021-12-07 13:39:51,242 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at PaidPromotionAdjustParameter.scala:214
2021-12-07 13:39:51,242 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 27 (take at PaidPromotionAdjustParameter.scala:214) with 1 output partitions
2021-12-07 13:39:51,242 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 65 (take at PaidPromotionAdjustParameter.scala:214)
2021-12-07 13:39:51,242 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 64)
2021-12-07 13:39:51,242 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:51,242 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 65 (MapPartitionsRDD[76] at map at PaidPromotionAdjustParameter.scala:207), which has no missing parents
2021-12-07 13:39:51,244 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_47 stored as values in memory (estimated size 183.8 KB, free 1990.0 MB)
2021-12-07 13:39:51,246 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_47_piece0 stored as bytes in memory (estimated size 65.8 KB, free 1990.0 MB)
2021-12-07 13:39:51,246 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_47_piece0 in memory on qb:59396 (size: 65.8 KB, free: 1990.6 MB)
2021-12-07 13:39:51,247 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 47 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:51,247 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[76] at map at PaidPromotionAdjustParameter.scala:207) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:39:51,247 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 65.0 with 1 tasks
2021-12-07 13:39:51,247 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 65.0 (TID 110, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:51,247 [Executor task launch worker for task 110] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 65.0 (TID 110)
2021-12-07 13:39:51,249 [Executor task launch worker for task 110] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:51,249 [Executor task launch worker for task 110] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:51,287 [Executor task launch worker for task 110] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 65.0 (TID 110). 2386 bytes result sent to driver
2021-12-07 13:39:51,287 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 65.0 (TID 110) in 40 ms on localhost (executor driver) (1/1)
2021-12-07 13:39:51,287 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 65.0, whose tasks have all completed, from pool 
2021-12-07 13:39:51,287 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 65 (take at PaidPromotionAdjustParameter.scala:214) finished in 0.044 s
2021-12-07 13:39:51,288 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 27 finished: take at PaidPromotionAdjustParameter.scala:214, took 0.045541 s
2021-12-07 13:39:51,296 [main] INFO [PaidPromotion$] - 训练集产品包列表-----------------------------------
2021-12-07 13:39:51,297 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:225
2021-12-07 13:39:51,297 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 77 (map at PaidPromotionAdjustParameter.scala:217)
2021-12-07 13:39:51,297 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 28 (count at PaidPromotionAdjustParameter.scala:225) with 4 output partitions
2021-12-07 13:39:51,297 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 67 (count at PaidPromotionAdjustParameter.scala:225)
2021-12-07 13:39:51,297 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 66)
2021-12-07 13:39:51,297 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 66)
2021-12-07 13:39:51,297 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 66 (MapPartitionsRDD[77] at map at PaidPromotionAdjustParameter.scala:217), which has no missing parents
2021-12-07 13:39:51,299 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_48 stored as values in memory (estimated size 182.8 KB, free 1989.8 MB)
2021-12-07 13:39:51,301 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_48_piece0 stored as bytes in memory (estimated size 65.5 KB, free 1989.7 MB)
2021-12-07 13:39:51,301 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_48_piece0 in memory on qb:59396 (size: 65.5 KB, free: 1990.6 MB)
2021-12-07 13:39:51,301 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 48 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:51,301 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ShuffleMapStage 66 (MapPartitionsRDD[77] at map at PaidPromotionAdjustParameter.scala:217) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:39:51,301 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 66.0 with 4 tasks
2021-12-07 13:39:51,302 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 66.0 (TID 111, localhost, executor driver, partition 0, ANY, 7994 bytes)
2021-12-07 13:39:51,302 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 66.0 (TID 112, localhost, executor driver, partition 1, ANY, 7994 bytes)
2021-12-07 13:39:51,302 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 66.0 (TID 113, localhost, executor driver, partition 2, ANY, 7994 bytes)
2021-12-07 13:39:51,302 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 66.0 (TID 114, localhost, executor driver, partition 3, ANY, 7994 bytes)
2021-12-07 13:39:51,302 [Executor task launch worker for task 112] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 66.0 (TID 112)
2021-12-07 13:39:51,302 [Executor task launch worker for task 114] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 66.0 (TID 114)
2021-12-07 13:39:51,302 [Executor task launch worker for task 113] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 66.0 (TID 113)
2021-12-07 13:39:51,302 [Executor task launch worker for task 111] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 66.0 (TID 111)
2021-12-07 13:39:51,304 [Executor task launch worker for task 111] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:51,304 [Executor task launch worker for task 113] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:0+3442194
2021-12-07 13:39:51,304 [Executor task launch worker for task 112] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:51,304 [Executor task launch worker for task 114] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/order_data.bcp:3442194+3442195
2021-12-07 13:39:52,323 [Executor task launch worker for task 111] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 66.0 (TID 111). 862 bytes result sent to driver
2021-12-07 13:39:52,324 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 66.0 (TID 111) in 1023 ms on localhost (executor driver) (1/4)
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1149
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1146
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1167
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1158
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1171
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1116
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1142
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1102
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1172
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1147
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1114
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1135
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1155
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1152
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1136
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1153
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1156
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1141
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1162
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1166
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1150
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1151
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1118
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1128
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1160
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1127
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1144
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1105
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1132
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1108
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1168
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1174
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1157
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1131
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1104
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1123
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1124
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1125
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1103
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1161
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1165
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1106
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1173
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1113
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1110
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1107
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1145
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1148
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1109
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1126
2021-12-07 13:39:53,578 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1122
2021-12-07 13:39:53,579 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_47_piece0 on qb:59396 in memory (size: 65.8 KB, free: 1990.6 MB)
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1100
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1164
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1133
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1130
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1111
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1138
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1101
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1137
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1143
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1169
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1115
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1140
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1119
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1134
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1139
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1159
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1121
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1154
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1170
2021-12-07 13:39:53,580 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1117
2021-12-07 13:39:53,581 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_46_piece0 on qb:59396 in memory (size: 65.7 KB, free: 1990.7 MB)
2021-12-07 13:39:53,581 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1112
2021-12-07 13:39:53,581 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1163
2021-12-07 13:39:53,581 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1120
2021-12-07 13:39:53,581 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1129
2021-12-07 13:39:53,738 [Executor task launch worker for task 114] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 66.0 (TID 114). 948 bytes result sent to driver
2021-12-07 13:39:53,738 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 66.0 (TID 114) in 2436 ms on localhost (executor driver) (2/4)
2021-12-07 13:39:53,788 [Executor task launch worker for task 113] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 66.0 (TID 113). 905 bytes result sent to driver
2021-12-07 13:39:53,788 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 66.0 (TID 113) in 2486 ms on localhost (executor driver) (3/4)
2021-12-07 13:39:53,941 [Executor task launch worker for task 112] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 66.0 (TID 112). 905 bytes result sent to driver
2021-12-07 13:39:53,941 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 66.0 (TID 112) in 2639 ms on localhost (executor driver) (4/4)
2021-12-07 13:39:53,941 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 66.0, whose tasks have all completed, from pool 
2021-12-07 13:39:53,941 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 66 (map at PaidPromotionAdjustParameter.scala:217) finished in 2.643 s
2021-12-07 13:39:53,941 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:39:53,941 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:39:53,941 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 67)
2021-12-07 13:39:53,941 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:39:53,941 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 67 (MapPartitionsRDD[79] at map at PaidPromotionAdjustParameter.scala:219), which has no missing parents
2021-12-07 13:39:53,943 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_49 stored as values in memory (estimated size 183.7 KB, free 1990.0 MB)
2021-12-07 13:39:53,944 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_49_piece0 stored as bytes in memory (estimated size 65.8 KB, free 1990.0 MB)
2021-12-07 13:39:53,944 [dispatcher-event-loop-8] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_49_piece0 in memory on qb:59396 (size: 65.8 KB, free: 1990.6 MB)
2021-12-07 13:39:53,945 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 49 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:53,945 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 4 missing tasks from ResultStage 67 (MapPartitionsRDD[79] at map at PaidPromotionAdjustParameter.scala:219) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
2021-12-07 13:39:53,945 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 67.0 with 4 tasks
2021-12-07 13:39:53,945 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 67.0 (TID 115, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:53,945 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 67.0 (TID 116, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:39:53,945 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 67.0 (TID 117, localhost, executor driver, partition 2, ANY, 7649 bytes)
2021-12-07 13:39:53,945 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 67.0 (TID 118, localhost, executor driver, partition 3, ANY, 7649 bytes)
2021-12-07 13:39:53,945 [Executor task launch worker for task 117] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 67.0 (TID 117)
2021-12-07 13:39:53,945 [Executor task launch worker for task 116] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 67.0 (TID 116)
2021-12-07 13:39:53,945 [Executor task launch worker for task 118] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 67.0 (TID 118)
2021-12-07 13:39:53,945 [Executor task launch worker for task 115] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 67.0 (TID 115)
2021-12-07 13:39:53,948 [Executor task launch worker for task 118] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:53,948 [Executor task launch worker for task 117] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:53,948 [Executor task launch worker for task 118] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:53,948 [Executor task launch worker for task 117] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:53,948 [Executor task launch worker for task 115] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:53,948 [Executor task launch worker for task 115] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:53,948 [Executor task launch worker for task 116] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:53,948 [Executor task launch worker for task 116] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:53,980 [Executor task launch worker for task 115] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 67.0 (TID 115). 1053 bytes result sent to driver
2021-12-07 13:39:53,980 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 67.0 (TID 115) in 35 ms on localhost (executor driver) (1/4)
2021-12-07 13:39:53,987 [Executor task launch worker for task 117] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 67.0 (TID 117). 1053 bytes result sent to driver
2021-12-07 13:39:53,988 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 67.0 (TID 117) in 43 ms on localhost (executor driver) (2/4)
2021-12-07 13:39:53,988 [Executor task launch worker for task 116] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 67.0 (TID 116). 1053 bytes result sent to driver
2021-12-07 13:39:53,988 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 67.0 (TID 116) in 43 ms on localhost (executor driver) (3/4)
2021-12-07 13:39:54,019 [Executor task launch worker for task 118] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 67.0 (TID 118). 1053 bytes result sent to driver
2021-12-07 13:39:54,020 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 67.0 (TID 118) in 75 ms on localhost (executor driver) (4/4)
2021-12-07 13:39:54,020 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 67.0, whose tasks have all completed, from pool 
2021-12-07 13:39:54,020 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 67 (count at PaidPromotionAdjustParameter.scala:225) finished in 0.078 s
2021-12-07 13:39:54,020 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 28 finished: count at PaidPromotionAdjustParameter.scala:225, took 2.723087 s
2021-12-07 13:39:54,020 [main] INFO [PaidPromotion$] - 训练集产品包列表数量：132
2021-12-07 13:39:54,026 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at PaidPromotionAdjustParameter.scala:226
2021-12-07 13:39:54,026 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 29 (take at PaidPromotionAdjustParameter.scala:226) with 1 output partitions
2021-12-07 13:39:54,026 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 69 (take at PaidPromotionAdjustParameter.scala:226)
2021-12-07 13:39:54,026 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 68)
2021-12-07 13:39:54,026 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:54,026 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 69 (MapPartitionsRDD[79] at map at PaidPromotionAdjustParameter.scala:219), which has no missing parents
2021-12-07 13:39:54,028 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_50 stored as values in memory (estimated size 183.9 KB, free 1989.8 MB)
2021-12-07 13:39:54,029 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_50_piece0 stored as bytes in memory (estimated size 65.9 KB, free 1989.7 MB)
2021-12-07 13:39:54,030 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_50_piece0 in memory on qb:59396 (size: 65.9 KB, free: 1990.6 MB)
2021-12-07 13:39:54,030 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 50 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:54,030 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[79] at map at PaidPromotionAdjustParameter.scala:219) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:39:54,030 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 69.0 with 1 tasks
2021-12-07 13:39:54,030 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 69.0 (TID 119, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 13:39:54,030 [Executor task launch worker for task 119] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 69.0 (TID 119)
2021-12-07 13:39:54,033 [Executor task launch worker for task 119] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:39:54,033 [Executor task launch worker for task 119] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:39:54,044 [Executor task launch worker for task 119] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 69.0 (TID 119). 17584 bytes result sent to driver
2021-12-07 13:39:54,044 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 69.0 (TID 119) in 14 ms on localhost (executor driver) (1/1)
2021-12-07 13:39:54,044 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 69.0, whose tasks have all completed, from pool 
2021-12-07 13:39:54,045 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 69 (take at PaidPromotionAdjustParameter.scala:226) finished in 0.018 s
2021-12-07 13:39:54,045 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 29 finished: take at PaidPromotionAdjustParameter.scala:226, took 0.018636 s
2021-12-07 13:39:54,049 [main] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:39:54,185 [main] INFO [org.apache.spark.SparkContext] - Starting job: runJob at SparkHadoopWriter.scala:78
2021-12-07 13:39:54,185 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 30 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2021-12-07 13:39:54,185 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 71 (runJob at SparkHadoopWriter.scala:78)
2021-12-07 13:39:54,185 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 70)
2021-12-07 13:39:54,185 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:54,185 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 71 (MapPartitionsRDD[81] at saveAsTextFile at PaidPromotionAdjustParameter.scala:228), which has no missing parents
2021-12-07 13:39:54,193 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_51 stored as values in memory (estimated size 259.5 KB, free 1989.5 MB)
2021-12-07 13:39:54,195 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_51_piece0 stored as bytes in memory (estimated size 94.3 KB, free 1989.4 MB)
2021-12-07 13:39:54,196 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_51_piece0 in memory on qb:59396 (size: 94.3 KB, free: 1990.5 MB)
2021-12-07 13:39:54,196 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 51 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:54,196 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[81] at saveAsTextFile at PaidPromotionAdjustParameter.scala:228) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:39:54,196 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 71.0 with 1 tasks
2021-12-07 13:39:54,196 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 71.0 (TID 120, localhost, executor driver, partition 0, ANY, 7943 bytes)
2021-12-07 13:39:54,197 [Executor task launch worker for task 120] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 71.0 (TID 120)
2021-12-07 13:39:54,202 [Executor task launch worker for task 120] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:39:54,312 [Executor task launch worker for task 120] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:54,312 [Executor task launch worker for task 120] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:54,328 [Executor task launch worker for task 120] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:54,328 [Executor task launch worker for task 120] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:56,794 [Executor task launch worker for task 120] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - Saved output of task 'attempt_20211207133954_0081_m_000000_0' to hdfs://hdp1:8020/sk/chongqing/list/validation-device/_temporary/0/task_20211207133954_0081_m_000000
2021-12-07 13:39:56,794 [Executor task launch worker for task 120] INFO [org.apache.spark.mapred.SparkHadoopMapRedUtil] - attempt_20211207133954_0081_m_000000_0: Committed
2021-12-07 13:39:56,795 [Executor task launch worker for task 120] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 71.0 (TID 120). 1299 bytes result sent to driver
2021-12-07 13:39:56,796 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 71.0 (TID 120) in 2600 ms on localhost (executor driver) (1/1)
2021-12-07 13:39:56,796 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 71.0, whose tasks have all completed, from pool 
2021-12-07 13:39:56,796 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 71 (runJob at SparkHadoopWriter.scala:78) finished in 2.611 s
2021-12-07 13:39:56,796 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 30 finished: runJob at SparkHadoopWriter.scala:78, took 2.611039 s
2021-12-07 13:39:57,927 [main] INFO [org.apache.spark.internal.io.SparkHadoopWriter] - Job job_20211207133954_0081 committed.
2021-12-07 13:39:57,930 [main] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:39:58,014 [main] INFO [org.apache.spark.SparkContext] - Starting job: runJob at SparkHadoopWriter.scala:78
2021-12-07 13:39:58,014 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 31 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2021-12-07 13:39:58,014 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 73 (runJob at SparkHadoopWriter.scala:78)
2021-12-07 13:39:58,014 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 72)
2021-12-07 13:39:58,014 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:39:58,014 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 73 (MapPartitionsRDD[83] at saveAsTextFile at PaidPromotionAdjustParameter.scala:229), which has no missing parents
2021-12-07 13:39:58,025 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_52 stored as values in memory (estimated size 259.7 KB, free 1989.1 MB)
2021-12-07 13:39:58,026 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_52_piece0 stored as bytes in memory (estimated size 94.4 KB, free 1989.0 MB)
2021-12-07 13:39:58,027 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_52_piece0 in memory on qb:59396 (size: 94.4 KB, free: 1990.4 MB)
2021-12-07 13:39:58,027 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 52 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:39:58,027 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[83] at saveAsTextFile at PaidPromotionAdjustParameter.scala:229) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:39:58,027 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 73.0 with 1 tasks
2021-12-07 13:39:58,027 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 73.0 (TID 121, localhost, executor driver, partition 0, ANY, 7943 bytes)
2021-12-07 13:39:58,027 [Executor task launch worker for task 121] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 73.0 (TID 121)
2021-12-07 13:39:58,032 [Executor task launch worker for task 121] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:39:58,111 [Executor task launch worker for task 121] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:58,111 [Executor task launch worker for task 121] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:58,120 [Executor task launch worker for task 121] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:39:58,120 [Executor task launch worker for task 121] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:39:58,136 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1211
2021-12-07 13:39:58,136 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1259
2021-12-07 13:39:58,136 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1216
2021-12-07 13:39:58,136 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1246
2021-12-07 13:39:58,136 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1180
2021-12-07 13:39:58,136 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1219
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1204
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1210
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1184
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1194
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1264
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1234
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1268
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1178
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1187
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1235
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1225
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1232
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1248
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1229
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1199
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1245
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1249
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1203
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1224
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1196
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1193
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1226
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1183
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1206
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1215
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1269
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1209
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1205
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1274
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1230
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1241
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1213
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1185
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1200
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1244
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1177
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1212
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1202
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1228
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1255
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1181
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1242
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1240
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1252
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1176
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1270
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1233
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1262
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1175
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1265
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1191
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1258
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1273
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1263
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1243
2021-12-07 13:39:58,137 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1186
2021-12-07 13:39:58,138 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_50_piece0 on qb:59396 in memory (size: 65.9 KB, free: 1990.5 MB)
2021-12-07 13:39:58,138 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1267
2021-12-07 13:39:58,138 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1218
2021-12-07 13:39:58,139 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_51_piece0 on qb:59396 in memory (size: 94.3 KB, free: 1990.6 MB)
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1198
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1223
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1190
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1201
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1271
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1266
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1197
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1239
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1272
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1257
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1189
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1195
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1256
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1182
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1251
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1207
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1238
2021-12-07 13:39:58,139 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1247
2021-12-07 13:39:58,140 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_48_piece0 on qb:59396 in memory (size: 65.5 KB, free: 1990.6 MB)
2021-12-07 13:39:58,140 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1208
2021-12-07 13:39:58,140 [dispatcher-event-loop-5] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_49_piece0 on qb:59396 in memory (size: 65.8 KB, free: 1990.7 MB)
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1254
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1227
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1231
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1237
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1179
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1253
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1188
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1214
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1236
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1250
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1220
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1261
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1260
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1217
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1221
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1192
2021-12-07 13:39:58,141 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1222
2021-12-07 13:40:00,662 [Executor task launch worker for task 121] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - Saved output of task 'attempt_20211207133957_0083_m_000000_0' to hdfs://hdp1:8020/sk/chongqing/list/validation-production/_temporary/0/task_20211207133957_0083_m_000000
2021-12-07 13:40:00,662 [Executor task launch worker for task 121] INFO [org.apache.spark.mapred.SparkHadoopMapRedUtil] - attempt_20211207133957_0083_m_000000_0: Committed
2021-12-07 13:40:00,662 [Executor task launch worker for task 121] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 73.0 (TID 121). 1342 bytes result sent to driver
2021-12-07 13:40:00,663 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 73.0 (TID 121) in 2636 ms on localhost (executor driver) (1/1)
2021-12-07 13:40:00,663 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 73.0, whose tasks have all completed, from pool 
2021-12-07 13:40:00,664 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 73 (runJob at SparkHadoopWriter.scala:78) finished in 2.649 s
2021-12-07 13:40:00,664 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 31 finished: runJob at SparkHadoopWriter.scala:78, took 2.650253 s
2021-12-07 13:40:01,899 [main] INFO [org.apache.spark.internal.io.SparkHadoopWriter] - Job job_20211207133957_0083 committed.
2021-12-07 13:40:01,901 [main] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:40:01,928 [main] INFO [org.apache.spark.SparkContext] - Starting job: runJob at SparkHadoopWriter.scala:78
2021-12-07 13:40:01,928 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 32 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2021-12-07 13:40:01,928 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 75 (runJob at SparkHadoopWriter.scala:78)
2021-12-07 13:40:01,928 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 74)
2021-12-07 13:40:01,928 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:40:01,928 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 75 (MapPartitionsRDD[85] at saveAsTextFile at PaidPromotionAdjustParameter.scala:230), which has no missing parents
2021-12-07 13:40:01,936 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_53 stored as values in memory (estimated size 260.1 KB, free 1989.9 MB)
2021-12-07 13:40:01,937 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_53_piece0 stored as bytes in memory (estimated size 94.8 KB, free 1989.8 MB)
2021-12-07 13:40:01,938 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_53_piece0 in memory on qb:59396 (size: 94.8 KB, free: 1990.6 MB)
2021-12-07 13:40:01,938 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 53 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:40:01,938 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[85] at saveAsTextFile at PaidPromotionAdjustParameter.scala:230) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:40:01,938 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 75.0 with 1 tasks
2021-12-07 13:40:01,938 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 75.0 (TID 122, localhost, executor driver, partition 0, ANY, 7979 bytes)
2021-12-07 13:40:01,938 [Executor task launch worker for task 122] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 75.0 (TID 122)
2021-12-07 13:40:01,943 [Executor task launch worker for task 122] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:40:01,954 [Executor task launch worker for task 122] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:40:01,954 [Executor task launch worker for task 122] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:40:02,011 [Executor task launch worker for task 122] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:40:02,011 [Executor task launch worker for task 122] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:40:02,067 [Executor task launch worker for task 122] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:40:02,067 [Executor task launch worker for task 122] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:40:02,123 [Executor task launch worker for task 122] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:40:02,123 [Executor task launch worker for task 122] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1277
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1278
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1285
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1289
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1276
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1292
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1297
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1291
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1286
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1284
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1294
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1293
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1281
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1288
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1280
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1296
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1275
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1282
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1279
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1287
2021-12-07 13:40:02,168 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1298
2021-12-07 13:40:02,169 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_52_piece0 on qb:59396 in memory (size: 94.4 KB, free: 1990.7 MB)
2021-12-07 13:40:02,169 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1290
2021-12-07 13:40:02,169 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1295
2021-12-07 13:40:02,169 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1299
2021-12-07 13:40:02,169 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1283
2021-12-07 13:40:18,632 [Executor task launch worker for task 122] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - Saved output of task 'attempt_20211207134001_0085_m_000000_0' to hdfs://hdp1:8020/sk/chongqing/list/train-device/_temporary/0/task_20211207134001_0085_m_000000
2021-12-07 13:40:18,632 [Executor task launch worker for task 122] INFO [org.apache.spark.mapred.SparkHadoopMapRedUtil] - attempt_20211207134001_0085_m_000000_0: Committed
2021-12-07 13:40:18,633 [Executor task launch worker for task 122] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 75.0 (TID 122). 1299 bytes result sent to driver
2021-12-07 13:40:18,634 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 75.0 (TID 122) in 16696 ms on localhost (executor driver) (1/1)
2021-12-07 13:40:18,634 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 75.0, whose tasks have all completed, from pool 
2021-12-07 13:40:18,634 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 75 (runJob at SparkHadoopWriter.scala:78) finished in 16.706 s
2021-12-07 13:40:18,634 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 32 finished: runJob at SparkHadoopWriter.scala:78, took 16.706005 s
2021-12-07 13:40:20,747 [main] INFO [org.apache.spark.internal.io.SparkHadoopWriter] - Job job_20211207134001_0085 committed.
2021-12-07 13:40:20,750 [main] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:40:20,860 [main] INFO [org.apache.spark.SparkContext] - Starting job: runJob at SparkHadoopWriter.scala:78
2021-12-07 13:40:20,860 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 33 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2021-12-07 13:40:20,860 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 77 (runJob at SparkHadoopWriter.scala:78)
2021-12-07 13:40:20,860 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 76)
2021-12-07 13:40:20,860 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:40:20,860 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 77 (MapPartitionsRDD[87] at saveAsTextFile at PaidPromotionAdjustParameter.scala:231), which has no missing parents
2021-12-07 13:40:20,869 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_54 stored as values in memory (estimated size 260.2 KB, free 1989.9 MB)
2021-12-07 13:40:20,871 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_54_piece0 stored as bytes in memory (estimated size 94.8 KB, free 1989.8 MB)
2021-12-07 13:40:20,871 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_54_piece0 in memory on qb:59396 (size: 94.8 KB, free: 1990.6 MB)
2021-12-07 13:40:20,871 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 54 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:40:20,872 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[87] at saveAsTextFile at PaidPromotionAdjustParameter.scala:231) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:40:20,872 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 77.0 with 1 tasks
2021-12-07 13:40:20,872 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 77.0 (TID 123, localhost, executor driver, partition 0, ANY, 7979 bytes)
2021-12-07 13:40:20,872 [Executor task launch worker for task 123] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 77.0 (TID 123)
2021-12-07 13:40:20,876 [Executor task launch worker for task 123] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - File Output Committer Algorithm version is 1
2021-12-07 13:40:21,000 [Executor task launch worker for task 123] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:40:21,000 [Executor task launch worker for task 123] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:40:21,013 [Executor task launch worker for task 123] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:40:21,013 [Executor task launch worker for task 123] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:40:21,044 [Executor task launch worker for task 123] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:40:21,044 [Executor task launch worker for task 123] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:40:21,078 [Executor task launch worker for task 123] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 4 non-empty blocks out of 4 blocks
2021-12-07 13:40:21,078 [Executor task launch worker for task 123] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:40:42,331 [Executor task launch worker for task 123] INFO [org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter] - Saved output of task 'attempt_20211207134020_0087_m_000000_0' to hdfs://hdp1:8020/sk/chongqing/list/train-production/_temporary/0/task_20211207134020_0087_m_000000
2021-12-07 13:40:42,331 [Executor task launch worker for task 123] INFO [org.apache.spark.mapred.SparkHadoopMapRedUtil] - attempt_20211207134020_0087_m_000000_0: Committed
2021-12-07 13:40:42,334 [Executor task launch worker for task 123] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 77.0 (TID 123). 1256 bytes result sent to driver
2021-12-07 13:40:42,335 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 77.0 (TID 123) in 21463 ms on localhost (executor driver) (1/1)
2021-12-07 13:40:42,335 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 77.0, whose tasks have all completed, from pool 
2021-12-07 13:40:42,336 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 77 (runJob at SparkHadoopWriter.scala:78) finished in 21.475 s
2021-12-07 13:40:42,336 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 33 finished: runJob at SparkHadoopWriter.scala:78, took 21.476581 s
2021-12-07 13:40:43,133 [main] INFO [org.apache.spark.internal.io.SparkHadoopWriter] - Job job_20211207134020_0087 committed.
2021-12-07 13:40:43,138 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Stopped Spark@5b800468{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2021-12-07 13:40:43,140 [main] INFO [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://qb:4040
2021-12-07 13:40:43,146 [dispatcher-event-loop-10] INFO [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2021-12-07 13:40:43,232 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2021-12-07 13:40:43,232 [main] INFO [org.apache.spark.storage.BlockManager] - BlockManager stopped
2021-12-07 13:40:43,232 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2021-12-07 13:40:43,234 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2021-12-07 13:40:43,237 [main] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2021-12-07 13:40:43,238 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2021-12-07 13:40:43,239 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\ACER\AppData\Local\Temp\spark-14f4aea6-a6b3-42f6-9eaa-193534eb89e7
2021-12-07 13:43:23,996 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.3.0
2021-12-07 13:43:24,287 [main] INFO [org.apache.spark.SparkContext] - Submitted application: ValidationListSize
2021-12-07 13:43:24,337 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: ACER,root
2021-12-07 13:43:24,337 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: ACER,root
2021-12-07 13:43:24,337 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2021-12-07 13:43:24,338 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2021-12-07 13:43:24,338 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ACER, root); groups with view permissions: Set(); users  with modify permissions: Set(ACER, root); groups with modify permissions: Set()
2021-12-07 13:43:24,900 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 57990.
2021-12-07 13:43:24,915 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2021-12-07 13:43:24,928 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2021-12-07 13:43:24,931 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2021-12-07 13:43:24,931 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2021-12-07 13:43:24,938 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\ACER\AppData\Local\Temp\blockmgr-3809d669-56f3-4943-a910-188ad33cece4
2021-12-07 13:43:24,952 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1990.8 MB
2021-12-07 13:43:24,961 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2021-12-07 13:43:25,011 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @1834ms
2021-12-07 13:43:25,057 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2021-12-07 13:43:25,068 [main] INFO [org.spark_project.jetty.server.Server] - Started @1890ms
2021-12-07 13:43:25,089 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@4d63b624{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2021-12-07 13:43:25,089 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2021-12-07 13:43:25,107 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6b5966e1{/jobs,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,108 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@25bcd0c7{/jobs/json,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,109 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6c6357f9{/jobs/job,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,111 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@593e824f{/jobs/job/json,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,112 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2f7a7219{/stages,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,113 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@361c294e{/stages/json,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,114 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5118388b{/stages/stage,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,115 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5af28b27{/stages/stage/json,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,116 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@332a7fce{/stages/pool,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,117 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5217f3d0{/stages/pool/json,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,118 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@77307458{/storage,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,119 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@33617539{/storage/json,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,120 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f9b7332{/storage/rdd,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,120 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1bdf8190{/storage/rdd/json,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,121 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@ec0c838{/environment,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,123 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@21c64522{/environment/json,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,124 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@76a82f33{/executors,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,125 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@532a02d9{/executors/json,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,126 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@62923ee6{/executors/threadDump,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,127 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@b91d8c4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,132 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@a1217f9{/static,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,133 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1950e8a6{/,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,134 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@724f138e{/api,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,135 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@782a4fff{/jobs/job/kill,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,136 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6063d80a{/stages/stage/kill,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,138 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://qb:4040
2021-12-07 13:43:25,211 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2021-12-07 13:43:25,253 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58032.
2021-12-07 13:43:25,253 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on qb:58032
2021-12-07 13:43:25,254 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2021-12-07 13:43:25,256 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, qb, 58032, None)
2021-12-07 13:43:25,258 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager qb:58032 with 1990.8 MB RAM, BlockManagerId(driver, qb, 58032, None)
2021-12-07 13:43:25,259 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, qb, 58032, None)
2021-12-07 13:43:25,259 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, qb, 58032, None)
2021-12-07 13:43:25,372 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@29a23c3d{/metrics/json,null,AVAILABLE,@Spark}
2021-12-07 13:43:25,790 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 312.7 KB, free 1990.5 MB)
2021-12-07 13:43:25,966 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KB, free 1990.5 MB)
2021-12-07 13:43:25,967 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on qb:58032 (size: 27.3 KB, free: 1990.8 MB)
2021-12-07 13:43:25,970 [main] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from textFile at ValidationListSize.scala:21
2021-12-07 13:43:26,281 [main] WARN [org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
2021-12-07 13:43:26,692 [main] INFO [org.apache.hadoop.mapred.FileInputFormat] - Total input paths to process : 1
2021-12-07 13:43:26,700 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at ValidationListSize.scala:25
2021-12-07 13:43:26,711 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (count at ValidationListSize.scala:25) with 2 output partitions
2021-12-07 13:43:26,712 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (count at ValidationListSize.scala:25)
2021-12-07 13:43:26,712 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:43:26,713 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:43:26,718 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at ValidationListSize.scala:23), which has no missing parents
2021-12-07 13:43:26,746 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 1990.5 MB)
2021-12-07 13:43:26,752 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1973.0 B, free 1990.5 MB)
2021-12-07 13:43:26,752 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on qb:58032 (size: 1973.0 B, free: 1990.8 MB)
2021-12-07 13:43:26,753 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:43:26,762 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at ValidationListSize.scala:23) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:43:26,762 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2021-12-07 13:43:26,792 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7914 bytes)
2021-12-07 13:43:26,793 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7914 bytes)
2021-12-07 13:43:26,798 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2021-12-07 13:43:26,798 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2021-12-07 13:43:26,831 [Executor task launch worker for task 1] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:104158+104159
2021-12-07 13:43:26,831 [Executor task launch worker for task 0] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:0+104158
2021-12-07 13:43:27,187 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 709 bytes result sent to driver
2021-12-07 13:43:27,187 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 752 bytes result sent to driver
2021-12-07 13:43:27,196 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 412 ms on localhost (executor driver) (1/2)
2021-12-07 13:43:27,196 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 403 ms on localhost (executor driver) (2/2)
2021-12-07 13:43:27,197 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2021-12-07 13:43:27,200 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (count at ValidationListSize.scala:25) finished in 0.470 s
2021-12-07 13:43:27,204 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: count at ValidationListSize.scala:25, took 0.503838 s
2021-12-07 13:43:27,230 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at ValidationListSize.scala:30
2021-12-07 13:43:27,237 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 3 (map at ValidationListSize.scala:27)
2021-12-07 13:43:27,238 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (take at ValidationListSize.scala:30) with 1 output partitions
2021-12-07 13:43:27,238 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (take at ValidationListSize.scala:30)
2021-12-07 13:43:27,238 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 1)
2021-12-07 13:43:27,240 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 1)
2021-12-07 13:43:27,240 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at map at ValidationListSize.scala:27), which has no missing parents
2021-12-07 13:43:27,245 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 4.7 KB, free 1990.5 MB)
2021-12-07 13:43:27,249 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 1990.5 MB)
2021-12-07 13:43:27,249 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on qb:58032 (size: 2.7 KB, free: 1990.8 MB)
2021-12-07 13:43:27,250 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:43:27,252 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at map at ValidationListSize.scala:27) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:43:27,252 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2021-12-07 13:43:27,253 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7903 bytes)
2021-12-07 13:43:27,253 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7903 bytes)
2021-12-07 13:43:27,253 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2021-12-07 13:43:27,253 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2021-12-07 13:43:27,257 [Executor task launch worker for task 3] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:104158+104159
2021-12-07 13:43:27,257 [Executor task launch worker for task 2] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:0+104158
2021-12-07 13:43:27,354 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 989 bytes result sent to driver
2021-12-07 13:43:27,369 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 117 ms on localhost (executor driver) (1/2)
2021-12-07 13:43:27,441 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 989 bytes result sent to driver
2021-12-07 13:43:27,443 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 190 ms on localhost (executor driver) (2/2)
2021-12-07 13:43:27,443 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2021-12-07 13:43:27,444 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 1 (map at ValidationListSize.scala:27) finished in 0.202 s
2021-12-07 13:43:27,444 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:43:27,444 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:43:27,445 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 2)
2021-12-07 13:43:27,445 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:43:27,447 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ValidationListSize.scala:29), which has no missing parents
2021-12-07 13:43:27,449 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 3.6 KB, free 1990.5 MB)
2021-12-07 13:43:27,453 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.5 MB)
2021-12-07 13:43:27,453 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on qb:58032 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:43:27,453 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:43:27,454 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ValidationListSize.scala:29) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:43:27,454 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 1 tasks
2021-12-07 13:43:27,455 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7649 bytes)
2021-12-07 13:43:27,456 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2021-12-07 13:43:27,466 [Executor task launch worker for task 4] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 0 non-empty blocks out of 2 blocks
2021-12-07 13:43:27,468 [Executor task launch worker for task 4] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 3 ms
2021-12-07 13:43:27,484 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 1054 bytes result sent to driver
2021-12-07 13:43:27,484 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 30 ms on localhost (executor driver) (1/1)
2021-12-07 13:43:27,484 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2021-12-07 13:43:27,485 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (take at ValidationListSize.scala:30) finished in 0.036 s
2021-12-07 13:43:27,485 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: take at ValidationListSize.scala:30, took 0.253914 s
2021-12-07 13:43:27,492 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at ValidationListSize.scala:30
2021-12-07 13:43:27,493 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (take at ValidationListSize.scala:30) with 1 output partitions
2021-12-07 13:43:27,493 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (take at ValidationListSize.scala:30)
2021-12-07 13:43:27,493 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 3)
2021-12-07 13:43:27,494 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:43:27,494 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (MapPartitionsRDD[5] at map at ValidationListSize.scala:29), which has no missing parents
2021-12-07 13:43:27,496 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 3.6 KB, free 1990.4 MB)
2021-12-07 13:43:27,500 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.4 MB)
2021-12-07 13:43:27,501 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on qb:58032 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:43:27,501 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:43:27,502 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[5] at map at ValidationListSize.scala:29) (first 15 tasks are for partitions Vector(1))
2021-12-07 13:43:27,502 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 1 tasks
2021-12-07 13:43:27,503 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 5, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:43:27,503 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 5)
2021-12-07 13:43:27,505 [Executor task launch worker for task 5] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:43:27,505 [Executor task launch worker for task 5] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 13:43:27,525 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 5). 1013 bytes result sent to driver
2021-12-07 13:43:27,526 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 5) in 24 ms on localhost (executor driver) (1/1)
2021-12-07 13:43:27,526 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2021-12-07 13:43:27,526 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (take at ValidationListSize.scala:30) finished in 0.031 s
2021-12-07 13:43:27,527 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: take at ValidationListSize.scala:30, took 0.034945 s
2021-12-07 13:43:27,529 [Thread-1] INFO [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2021-12-07 13:43:27,539 [Thread-1] INFO [org.spark_project.jetty.server.AbstractConnector] - Stopped Spark@4d63b624{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2021-12-07 13:43:27,541 [Thread-1] INFO [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://qb:4040
2021-12-07 13:43:27,541 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 48
2021-12-07 13:43:27,541 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 27
2021-12-07 13:43:27,542 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 56
2021-12-07 13:43:27,542 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 62
2021-12-07 13:43:27,542 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 29
2021-12-07 13:43:27,542 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 65
2021-12-07 13:43:27,542 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 50
2021-12-07 13:43:27,542 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 32
2021-12-07 13:43:27,542 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 22
2021-12-07 13:43:27,553 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on qb:58032 in memory (size: 1973.0 B, free: 1990.8 MB)
2021-12-07 13:43:27,561 [dispatcher-event-loop-2] INFO [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2021-12-07 13:43:27,573 [Thread-1] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2021-12-07 13:43:27,574 [Thread-1] INFO [org.apache.spark.storage.BlockManager] - BlockManager stopped
2021-12-07 13:43:27,575 [Thread-1] INFO [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2021-12-07 13:43:27,576 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2021-12-07 13:43:27,579 [Thread-1] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2021-12-07 13:43:27,579 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2021-12-07 13:43:27,579 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\ACER\AppData\Local\Temp\spark-74d7b316-9d6c-4d6a-8f3a-25caef09fff4
2021-12-07 13:53:14,890 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.3.0
2021-12-07 13:53:15,153 [main] INFO [org.apache.spark.SparkContext] - Submitted application: ValidationListSize
2021-12-07 13:53:15,197 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: ACER,root
2021-12-07 13:53:15,197 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: ACER,root
2021-12-07 13:53:15,198 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2021-12-07 13:53:15,198 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2021-12-07 13:53:15,198 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ACER, root); groups with view permissions: Set(); users  with modify permissions: Set(ACER, root); groups with modify permissions: Set()
2021-12-07 13:53:15,738 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 58031.
2021-12-07 13:53:15,754 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2021-12-07 13:53:15,768 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2021-12-07 13:53:15,770 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2021-12-07 13:53:15,770 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2021-12-07 13:53:15,778 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\ACER\AppData\Local\Temp\blockmgr-958b0f58-aab4-49d8-90fc-217a637638b6
2021-12-07 13:53:15,791 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1990.8 MB
2021-12-07 13:53:15,801 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2021-12-07 13:53:15,851 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @1789ms
2021-12-07 13:53:15,894 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2021-12-07 13:53:15,906 [main] INFO [org.spark_project.jetty.server.Server] - Started @1846ms
2021-12-07 13:53:15,931 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@4d63b624{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2021-12-07 13:53:15,931 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2021-12-07 13:53:15,950 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6b5966e1{/jobs,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,951 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@25bcd0c7{/jobs/json,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,953 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6c6357f9{/jobs/job,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,954 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@593e824f{/jobs/job/json,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,956 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2f7a7219{/stages,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,957 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@361c294e{/stages/json,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,958 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5118388b{/stages/stage,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,960 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5af28b27{/stages/stage/json,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,962 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@332a7fce{/stages/pool,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,964 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5217f3d0{/stages/pool/json,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,965 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@77307458{/storage,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,966 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@33617539{/storage/json,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,968 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f9b7332{/storage/rdd,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,969 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1bdf8190{/storage/rdd/json,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,971 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@ec0c838{/environment,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,971 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@21c64522{/environment/json,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,973 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@76a82f33{/executors,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,974 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@532a02d9{/executors/json,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,975 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@62923ee6{/executors/threadDump,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,976 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@b91d8c4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,984 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@a1217f9{/static,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,986 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1950e8a6{/,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,990 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@724f138e{/api,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,991 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@782a4fff{/jobs/job/kill,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,993 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6063d80a{/stages/stage/kill,null,AVAILABLE,@Spark}
2021-12-07 13:53:15,994 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://qb:4040
2021-12-07 13:53:16,078 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2021-12-07 13:53:16,123 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58072.
2021-12-07 13:53:16,124 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on qb:58072
2021-12-07 13:53:16,125 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2021-12-07 13:53:16,126 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, qb, 58072, None)
2021-12-07 13:53:16,128 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager qb:58072 with 1990.8 MB RAM, BlockManagerId(driver, qb, 58072, None)
2021-12-07 13:53:16,130 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, qb, 58072, None)
2021-12-07 13:53:16,130 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, qb, 58072, None)
2021-12-07 13:53:16,254 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@29a23c3d{/metrics/json,null,AVAILABLE,@Spark}
2021-12-07 13:53:16,729 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 312.7 KB, free 1990.5 MB)
2021-12-07 13:53:16,930 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KB, free 1990.5 MB)
2021-12-07 13:53:16,931 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on qb:58072 (size: 27.3 KB, free: 1990.8 MB)
2021-12-07 13:53:16,934 [main] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from textFile at ValidationListSize.scala:21
2021-12-07 13:53:17,291 [main] WARN [org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
2021-12-07 13:53:17,728 [main] INFO [org.apache.hadoop.mapred.FileInputFormat] - Total input paths to process : 1
2021-12-07 13:53:17,737 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at ValidationListSize.scala:25
2021-12-07 13:53:17,745 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (count at ValidationListSize.scala:25) with 2 output partitions
2021-12-07 13:53:17,746 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (count at ValidationListSize.scala:25)
2021-12-07 13:53:17,746 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:53:17,747 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:53:17,753 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at ValidationListSize.scala:23), which has no missing parents
2021-12-07 13:53:17,782 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 1990.5 MB)
2021-12-07 13:53:17,787 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1973.0 B, free 1990.5 MB)
2021-12-07 13:53:17,787 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on qb:58072 (size: 1973.0 B, free: 1990.8 MB)
2021-12-07 13:53:17,788 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:53:17,797 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at ValidationListSize.scala:23) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:53:17,798 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2021-12-07 13:53:17,830 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7914 bytes)
2021-12-07 13:53:17,831 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7914 bytes)
2021-12-07 13:53:17,837 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2021-12-07 13:53:17,837 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2021-12-07 13:53:17,871 [Executor task launch worker for task 0] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:0+104158
2021-12-07 13:53:17,871 [Executor task launch worker for task 1] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:104158+104159
2021-12-07 13:53:19,573 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 752 bytes result sent to driver
2021-12-07 13:53:19,581 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 1749 ms on localhost (executor driver) (1/2)
2021-12-07 13:53:19,946 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 752 bytes result sent to driver
2021-12-07 13:53:19,949 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 2128 ms on localhost (executor driver) (2/2)
2021-12-07 13:53:19,950 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2021-12-07 13:53:19,950 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (count at ValidationListSize.scala:25) finished in 2.184 s
2021-12-07 13:53:19,954 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: count at ValidationListSize.scala:25, took 2.218179 s
2021-12-07 13:53:19,979 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at ValidationListSize.scala:30
2021-12-07 13:53:19,987 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 3 (map at ValidationListSize.scala:27)
2021-12-07 13:53:19,987 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (take at ValidationListSize.scala:30) with 1 output partitions
2021-12-07 13:53:19,987 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (take at ValidationListSize.scala:30)
2021-12-07 13:53:19,987 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 1)
2021-12-07 13:53:19,988 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 1)
2021-12-07 13:53:19,988 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at map at ValidationListSize.scala:27), which has no missing parents
2021-12-07 13:53:19,994 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 4.7 KB, free 1990.5 MB)
2021-12-07 13:53:19,997 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 1990.5 MB)
2021-12-07 13:53:19,997 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on qb:58072 (size: 2.7 KB, free: 1990.8 MB)
2021-12-07 13:53:19,998 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:53:20,000 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at map at ValidationListSize.scala:27) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:53:20,000 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2021-12-07 13:53:20,001 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7903 bytes)
2021-12-07 13:53:20,002 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7903 bytes)
2021-12-07 13:53:20,002 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2021-12-07 13:53:20,002 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2021-12-07 13:53:20,006 [Executor task launch worker for task 2] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:0+104158
2021-12-07 13:53:20,006 [Executor task launch worker for task 3] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:104158+104159
2021-12-07 13:53:20,232 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 989 bytes result sent to driver
2021-12-07 13:53:20,247 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 245 ms on localhost (executor driver) (1/2)
2021-12-07 13:53:20,723 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 989 bytes result sent to driver
2021-12-07 13:53:20,726 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 725 ms on localhost (executor driver) (2/2)
2021-12-07 13:53:20,726 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2021-12-07 13:53:20,727 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 1 (map at ValidationListSize.scala:27) finished in 0.736 s
2021-12-07 13:53:20,727 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 13:53:20,727 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 13:53:20,728 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 2)
2021-12-07 13:53:20,728 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 13:53:20,730 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[5] at map at ValidationListSize.scala:29), which has no missing parents
2021-12-07 13:53:20,732 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 3.6 KB, free 1990.5 MB)
2021-12-07 13:53:20,735 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.5 MB)
2021-12-07 13:53:20,735 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on qb:58072 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:53:20,736 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:53:20,736 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at ValidationListSize.scala:29) (first 15 tasks are for partitions Vector(0))
2021-12-07 13:53:20,736 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 1 tasks
2021-12-07 13:53:20,737 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 7649 bytes)
2021-12-07 13:53:20,738 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2021-12-07 13:53:20,747 [Executor task launch worker for task 4] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 0 non-empty blocks out of 2 blocks
2021-12-07 13:53:20,748 [Executor task launch worker for task 4] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 3 ms
2021-12-07 13:53:20,762 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 1054 bytes result sent to driver
2021-12-07 13:53:20,762 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 26 ms on localhost (executor driver) (1/1)
2021-12-07 13:53:20,762 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2021-12-07 13:53:20,763 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (take at ValidationListSize.scala:30) finished in 0.032 s
2021-12-07 13:53:20,763 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: take at ValidationListSize.scala:30, took 0.783228 s
2021-12-07 13:53:20,769 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at ValidationListSize.scala:30
2021-12-07 13:53:20,770 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (take at ValidationListSize.scala:30) with 1 output partitions
2021-12-07 13:53:20,770 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (take at ValidationListSize.scala:30)
2021-12-07 13:53:20,770 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 3)
2021-12-07 13:53:20,770 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:53:20,770 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (MapPartitionsRDD[5] at map at ValidationListSize.scala:29), which has no missing parents
2021-12-07 13:53:20,772 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 3.6 KB, free 1990.4 MB)
2021-12-07 13:53:20,775 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.4 MB)
2021-12-07 13:53:20,776 [dispatcher-event-loop-5] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on qb:58072 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:53:20,776 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:53:20,777 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[5] at map at ValidationListSize.scala:29) (first 15 tasks are for partitions Vector(1))
2021-12-07 13:53:20,777 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 1 tasks
2021-12-07 13:53:20,777 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 5, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 13:53:20,778 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 5)
2021-12-07 13:53:20,780 [Executor task launch worker for task 5] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 13:53:20,781 [Executor task launch worker for task 5] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 13:53:20,796 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 5). 1056 bytes result sent to driver
2021-12-07 13:53:20,796 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 5) in 19 ms on localhost (executor driver) (1/1)
2021-12-07 13:53:20,796 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2021-12-07 13:53:20,797 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (take at ValidationListSize.scala:30) finished in 0.026 s
2021-12-07 13:53:20,797 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: take at ValidationListSize.scala:30, took 0.027338 s
2021-12-07 13:53:20,802 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at ValidationListSize.scala:33
2021-12-07 13:53:20,802 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (count at ValidationListSize.scala:33) with 2 output partitions
2021-12-07 13:53:20,802 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 5 (count at ValidationListSize.scala:33)
2021-12-07 13:53:20,802 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 13:53:20,802 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 13:53:20,803 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 5 (MapPartitionsRDD[6] at flatMap at ValidationListSize.scala:33), which has no missing parents
2021-12-07 13:53:20,804 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 3.5 KB, free 1990.4 MB)
2021-12-07 13:53:20,808 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2046.0 B, free 1990.4 MB)
2021-12-07 13:53:20,809 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on qb:58072 (size: 2046.0 B, free: 1990.8 MB)
2021-12-07 13:53:20,809 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1039
2021-12-07 13:53:20,810 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[6] at flatMap at ValidationListSize.scala:33) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 13:53:20,810 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 5.0 with 2 tasks
2021-12-07 13:53:20,810 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 5.0 (TID 6, localhost, executor driver, partition 0, ANY, 7914 bytes)
2021-12-07 13:53:20,810 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 5.0 (TID 7, localhost, executor driver, partition 1, ANY, 7914 bytes)
2021-12-07 13:53:20,810 [Executor task launch worker for task 7] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 5.0 (TID 7)
2021-12-07 13:53:20,810 [Executor task launch worker for task 6] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 5.0 (TID 6)
2021-12-07 13:53:20,813 [Executor task launch worker for task 6] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:0+104158
2021-12-07 13:53:20,813 [Executor task launch worker for task 7] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:104158+104159
2021-12-07 13:53:20,832 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 22
2021-12-07 13:53:20,833 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 63
2021-12-07 13:53:20,833 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 27
2021-12-07 13:53:20,833 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 13
2021-12-07 13:53:20,833 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 14
2021-12-07 13:53:20,833 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 93
2021-12-07 13:53:20,833 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 41
2021-12-07 13:53:20,833 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 61
2021-12-07 13:53:20,833 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 84
2021-12-07 13:53:20,834 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 83
2021-12-07 13:53:20,834 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 86
2021-12-07 13:53:20,834 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 92
2021-12-07 13:53:20,834 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 72
2021-12-07 13:53:20,834 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 71
2021-12-07 13:53:20,835 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 90
2021-12-07 13:53:20,835 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 64
2021-12-07 13:53:20,847 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2_piece0 on qb:58072 in memory (size: 2.7 KB, free: 1990.8 MB)
2021-12-07 13:53:20,851 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned shuffle 0
2021-12-07 13:53:20,852 [dispatcher-event-loop-8] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on qb:58072 in memory (size: 1973.0 B, free: 1990.8 MB)
2021-12-07 13:53:20,852 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 88
2021-12-07 13:53:20,852 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 65
2021-12-07 13:53:20,853 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_3_piece0 on qb:58072 in memory (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:53:20,853 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 16
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 54
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 36
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 95
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 99
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 9
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 50
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 67
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 35
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 60
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 66
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 3
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 51
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 68
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 29
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 12
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 10
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 32
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 45
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 5
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 19
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 82
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 4
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 26
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 91
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 43
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 53
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 94
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 55
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 97
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 40
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 8
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 2
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 49
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 47
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 28
2021-12-07 13:53:20,854 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 58
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 62
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 73
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 69
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 96
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 33
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 57
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 48
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 7
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 37
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 77
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 56
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 79
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 34
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 15
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 38
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 6
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 98
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 30
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 17
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 39
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 31
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 81
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 25
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 89
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 23
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 44
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 20
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 21
2021-12-07 13:53:20,855 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 59
2021-12-07 13:53:20,856 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_4_piece0 on qb:58072 in memory (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 13:53:20,856 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 70
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 0
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 52
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 85
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 11
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 80
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 74
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 78
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 76
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 46
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 42
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 87
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 18
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 75
2021-12-07 13:53:20,857 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 24
2021-12-07 13:53:21,447 [Executor task launch worker for task 6] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 5.0 (TID 6). 710 bytes result sent to driver
2021-12-07 13:53:21,448 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 5.0 (TID 6) in 638 ms on localhost (executor driver) (1/2)
2021-12-07 13:53:21,542 [Executor task launch worker for task 7] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 5.0 (TID 7). 710 bytes result sent to driver
2021-12-07 13:53:21,542 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 5.0 (TID 7) in 732 ms on localhost (executor driver) (2/2)
2021-12-07 13:53:21,542 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2021-12-07 13:53:21,543 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 5 (count at ValidationListSize.scala:33) finished in 0.740 s
2021-12-07 13:53:21,543 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: count at ValidationListSize.scala:33, took 0.741386 s
2021-12-07 13:53:21,546 [Thread-1] INFO [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2021-12-07 13:53:21,550 [Thread-1] INFO [org.spark_project.jetty.server.AbstractConnector] - Stopped Spark@4d63b624{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2021-12-07 13:53:21,552 [Thread-1] INFO [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://qb:4040
2021-12-07 13:53:21,560 [dispatcher-event-loop-6] INFO [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2021-12-07 13:53:21,573 [Thread-1] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2021-12-07 13:53:21,574 [Thread-1] INFO [org.apache.spark.storage.BlockManager] - BlockManager stopped
2021-12-07 13:53:21,575 [Thread-1] INFO [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2021-12-07 13:53:21,576 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2021-12-07 13:53:21,579 [Thread-1] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2021-12-07 13:53:21,579 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2021-12-07 13:53:21,580 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\ACER\AppData\Local\Temp\spark-76bcf876-c6d7-48b7-99a0-83020412b03b
2021-12-07 14:29:35,093 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.3.0
2021-12-07 14:29:35,370 [main] INFO [org.apache.spark.SparkContext] - Submitted application: PaidPromotionAdjustParameter
2021-12-07 14:29:35,420 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: ACER,root
2021-12-07 14:29:35,420 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: ACER,root
2021-12-07 14:29:35,421 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2021-12-07 14:29:35,421 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2021-12-07 14:29:35,422 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ACER, root); groups with view permissions: Set(); users  with modify permissions: Set(ACER, root); groups with modify permissions: Set()
2021-12-07 14:29:35,999 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 59961.
2021-12-07 14:29:36,017 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2021-12-07 14:29:36,031 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2021-12-07 14:29:36,034 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2021-12-07 14:29:36,034 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2021-12-07 14:29:36,041 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\ACER\AppData\Local\Temp\blockmgr-0947d438-69ed-4c38-bbff-ae77902c93e0
2021-12-07 14:29:36,056 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1990.8 MB
2021-12-07 14:29:36,065 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2021-12-07 14:29:36,126 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @1872ms
2021-12-07 14:29:36,183 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2021-12-07 14:29:36,194 [main] INFO [org.spark_project.jetty.server.Server] - Started @1940ms
2021-12-07 14:29:36,220 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@5b800468{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2021-12-07 14:29:36,220 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2021-12-07 14:29:36,239 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1568159{/jobs,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,240 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@63cd604c{/jobs/json,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,241 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3954d008{/jobs/job,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,242 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6d8792db{/jobs/job/json,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,243 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@3a1d593e{/stages,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,244 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@285d851a{/stages/json,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,244 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7876d598{/stages/stage,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,246 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@4985cbcb{/stages/stage/json,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,247 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@54361a9{/stages/pool,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,248 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@293bb8a5{/stages/pool/json,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,249 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@290b1b2e{/storage,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,250 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5db4c359{/storage/json,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,250 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6fefce9e{/storage/rdd,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,251 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@8a589a2{/storage/rdd/json,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,253 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5cc69cfe{/environment,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,254 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@11dee337{/environment/json,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,256 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@74bdc168{/executors,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,257 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@7bb3a9fe{/executors/json,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,259 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f19c9d2{/executors/threadDump,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,260 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@a77614d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,268 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@523424b5{/static,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,269 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@12cd9150{/,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,272 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@32fe9d0a{/api,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,274 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@59fc684e{/jobs/job/kill,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,275 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@355e34c7{/stages/stage/kill,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,277 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://qb:4040
2021-12-07 14:29:36,351 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2021-12-07 14:29:36,398 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60003.
2021-12-07 14:29:36,399 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on qb:60003
2021-12-07 14:29:36,400 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2021-12-07 14:29:36,401 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, qb, 60003, None)
2021-12-07 14:29:36,403 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager qb:60003 with 1990.8 MB RAM, BlockManagerId(driver, qb, 60003, None)
2021-12-07 14:29:36,405 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, qb, 60003, None)
2021-12-07 14:29:36,406 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, qb, 60003, None)
2021-12-07 14:29:36,526 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6fe46b62{/metrics/json,null,AVAILABLE,@Spark}
2021-12-07 14:29:36,963 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 312.7 KB, free 1990.5 MB)
2021-12-07 14:29:37,147 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KB, free 1990.5 MB)
2021-12-07 14:29:37,149 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on qb:60003 (size: 27.3 KB, free: 1990.8 MB)
2021-12-07 14:29:37,152 [main] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from textFile at PaidPromotionAdjustParameter.scala:50
2021-12-07 14:29:37,462 [main] WARN [org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
2021-12-07 14:29:37,603 [main] INFO [org.apache.hadoop.mapred.FileInputFormat] - Total input paths to process : 1
2021-12-07 14:29:37,661 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:52
2021-12-07 14:29:37,671 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (count at PaidPromotionAdjustParameter.scala:52) with 20 output partitions
2021-12-07 14:29:37,671 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (count at PaidPromotionAdjustParameter.scala:52)
2021-12-07 14:29:37,672 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 14:29:37,673 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 14:29:37,679 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (/sk/chongqing/data/behavior_data.bcp MapPartitionsRDD[1] at textFile at PaidPromotionAdjustParameter.scala:50), which has no missing parents
2021-12-07 14:29:37,711 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 1990.5 MB)
2021-12-07 14:29:37,716 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1903.0 B, free 1990.5 MB)
2021-12-07 14:29:37,717 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on qb:60003 (size: 1903.0 B, free: 1990.8 MB)
2021-12-07 14:29:37,717 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2021-12-07 14:29:37,726 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 20 missing tasks from ResultStage 0 (/sk/chongqing/data/behavior_data.bcp MapPartitionsRDD[1] at textFile at PaidPromotionAdjustParameter.scala:50) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2021-12-07 14:29:37,727 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 20 tasks
2021-12-07 14:29:37,757 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7899 bytes)
2021-12-07 14:29:37,759 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7899 bytes)
2021-12-07 14:29:37,759 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 7899 bytes)
2021-12-07 14:29:37,759 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 7899 bytes)
2021-12-07 14:29:37,759 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 7899 bytes)
2021-12-07 14:29:37,760 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 7899 bytes)
2021-12-07 14:29:37,760 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 7899 bytes)
2021-12-07 14:29:37,760 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 7899 bytes)
2021-12-07 14:29:37,761 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 7899 bytes)
2021-12-07 14:29:37,761 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 7899 bytes)
2021-12-07 14:29:37,761 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 7899 bytes)
2021-12-07 14:29:37,761 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 7899 bytes)
2021-12-07 14:29:37,767 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Running task 5.0 in stage 0.0 (TID 5)
2021-12-07 14:29:37,767 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2021-12-07 14:29:37,767 [Executor task launch worker for task 9] INFO [org.apache.spark.executor.Executor] - Running task 9.0 in stage 0.0 (TID 9)
2021-12-07 14:29:37,767 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2021-12-07 14:29:37,767 [Executor task launch worker for task 10] INFO [org.apache.spark.executor.Executor] - Running task 10.0 in stage 0.0 (TID 10)
2021-12-07 14:29:37,767 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 0.0 (TID 3)
2021-12-07 14:29:37,767 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Running task 4.0 in stage 0.0 (TID 4)
2021-12-07 14:29:37,767 [Executor task launch worker for task 8] INFO [org.apache.spark.executor.Executor] - Running task 8.0 in stage 0.0 (TID 8)
2021-12-07 14:29:37,767 [Executor task launch worker for task 7] INFO [org.apache.spark.executor.Executor] - Running task 7.0 in stage 0.0 (TID 7)
2021-12-07 14:29:37,767 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 0.0 (TID 2)
2021-12-07 14:29:37,767 [Executor task launch worker for task 11] INFO [org.apache.spark.executor.Executor] - Running task 11.0 in stage 0.0 (TID 11)
2021-12-07 14:29:37,767 [Executor task launch worker for task 6] INFO [org.apache.spark.executor.Executor] - Running task 6.0 in stage 0.0 (TID 6)
2021-12-07 14:29:37,806 [Executor task launch worker for task 2] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:268435456+134217728
2021-12-07 14:29:37,806 [Executor task launch worker for task 4] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:536870912+134217728
2021-12-07 14:29:37,806 [Executor task launch worker for task 11] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1476395008+134217728
2021-12-07 14:29:37,806 [Executor task launch worker for task 10] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1342177280+134217728
2021-12-07 14:29:37,806 [Executor task launch worker for task 7] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:939524096+134217728
2021-12-07 14:29:37,806 [Executor task launch worker for task 8] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1073741824+134217728
2021-12-07 14:29:37,806 [Executor task launch worker for task 6] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:805306368+134217728
2021-12-07 14:29:37,806 [Executor task launch worker for task 0] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:0+134217728
2021-12-07 14:29:37,806 [Executor task launch worker for task 5] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:671088640+134217728
2021-12-07 14:29:37,806 [Executor task launch worker for task 3] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:402653184+134217728
2021-12-07 14:29:37,806 [Executor task launch worker for task 1] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:134217728+134217728
2021-12-07 14:29:37,806 [Executor task launch worker for task 9] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1207959552+134217728
2021-12-07 14:30:26,482 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 755 bytes result sent to driver
2021-12-07 14:30:26,484 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 7899 bytes)
2021-12-07 14:30:26,484 [Executor task launch worker for task 12] INFO [org.apache.spark.executor.Executor] - Running task 12.0 in stage 0.0 (TID 12)
2021-12-07 14:30:26,486 [Executor task launch worker for task 12] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1610612736+134217728
2021-12-07 14:30:26,491 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 48731 ms on localhost (executor driver) (1/20)
2021-12-07 14:31:30,163 [Executor task launch worker for task 10] INFO [org.apache.spark.executor.Executor] - Finished task 10.0 in stage 0.0 (TID 10). 755 bytes result sent to driver
2021-12-07 14:31:30,164 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 7899 bytes)
2021-12-07 14:31:30,164 [Executor task launch worker for task 13] INFO [org.apache.spark.executor.Executor] - Running task 13.0 in stage 0.0 (TID 13)
2021-12-07 14:31:30,165 [Executor task launch worker for task 13] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1744830464+134217728
2021-12-07 14:31:30,168 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 10.0 in stage 0.0 (TID 10) in 112407 ms on localhost (executor driver) (2/20)
2021-12-07 14:31:56,073 [Executor task launch worker for task 9] INFO [org.apache.spark.executor.Executor] - Finished task 9.0 in stage 0.0 (TID 9). 755 bytes result sent to driver
2021-12-07 14:31:56,074 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 7899 bytes)
2021-12-07 14:31:56,074 [Executor task launch worker for task 14] INFO [org.apache.spark.executor.Executor] - Running task 14.0 in stage 0.0 (TID 14)
2021-12-07 14:31:56,075 [Executor task launch worker for task 14] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1879048192+134217728
2021-12-07 14:31:56,076 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 9.0 in stage 0.0 (TID 9) in 138315 ms on localhost (executor driver) (3/20)
2021-12-07 14:31:57,046 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 0.0 (TID 3). 755 bytes result sent to driver
2021-12-07 14:31:57,047 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 15.0 in stage 0.0 (TID 15, localhost, executor driver, partition 15, ANY, 7899 bytes)
2021-12-07 14:31:57,047 [Executor task launch worker for task 15] INFO [org.apache.spark.executor.Executor] - Running task 15.0 in stage 0.0 (TID 15)
2021-12-07 14:31:57,049 [Executor task launch worker for task 15] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:2013265920+134217728
2021-12-07 14:31:57,049 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 0.0 (TID 3) in 139290 ms on localhost (executor driver) (4/20)
2021-12-07 14:32:32,936 [Executor task launch worker for task 12] INFO [org.apache.spark.executor.Executor] - Finished task 12.0 in stage 0.0 (TID 12). 755 bytes result sent to driver
2021-12-07 14:32:32,937 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 16.0 in stage 0.0 (TID 16, localhost, executor driver, partition 16, ANY, 7899 bytes)
2021-12-07 14:32:32,937 [Executor task launch worker for task 16] INFO [org.apache.spark.executor.Executor] - Running task 16.0 in stage 0.0 (TID 16)
2021-12-07 14:32:32,937 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 12.0 in stage 0.0 (TID 12) in 126453 ms on localhost (executor driver) (5/20)
2021-12-07 14:32:32,938 [Executor task launch worker for task 16] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:2147483648+134217728
2021-12-07 14:32:37,922 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Finished task 4.0 in stage 0.0 (TID 4). 755 bytes result sent to driver
2021-12-07 14:32:37,923 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 17.0 in stage 0.0 (TID 17, localhost, executor driver, partition 17, ANY, 7899 bytes)
2021-12-07 14:32:37,923 [Executor task launch worker for task 17] INFO [org.apache.spark.executor.Executor] - Running task 17.0 in stage 0.0 (TID 17)
2021-12-07 14:32:37,923 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 4.0 in stage 0.0 (TID 4) in 180164 ms on localhost (executor driver) (6/20)
2021-12-07 14:32:37,924 [Executor task launch worker for task 17] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:2281701376+134217728
2021-12-07 14:32:39,197 [Executor task launch worker for task 11] INFO [org.apache.spark.executor.Executor] - Finished task 11.0 in stage 0.0 (TID 11). 755 bytes result sent to driver
2021-12-07 14:32:39,198 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 18.0 in stage 0.0 (TID 18, localhost, executor driver, partition 18, ANY, 7899 bytes)
2021-12-07 14:32:39,198 [Executor task launch worker for task 18] INFO [org.apache.spark.executor.Executor] - Running task 18.0 in stage 0.0 (TID 18)
2021-12-07 14:32:39,198 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 11.0 in stage 0.0 (TID 11) in 181437 ms on localhost (executor driver) (7/20)
2021-12-07 14:32:39,199 [Executor task launch worker for task 18] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:2415919104+134217728
2021-12-07 14:32:46,586 [Executor task launch worker for task 7] INFO [org.apache.spark.executor.Executor] - Finished task 7.0 in stage 0.0 (TID 7). 755 bytes result sent to driver
2021-12-07 14:32:46,586 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 19.0 in stage 0.0 (TID 19, localhost, executor driver, partition 19, ANY, 7899 bytes)
2021-12-07 14:32:46,587 [Executor task launch worker for task 19] INFO [org.apache.spark.executor.Executor] - Running task 19.0 in stage 0.0 (TID 19)
2021-12-07 14:32:46,587 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 7.0 in stage 0.0 (TID 7) in 188827 ms on localhost (executor driver) (8/20)
2021-12-07 14:32:46,588 [Executor task launch worker for task 19] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:2550136832+85027535
2021-12-07 14:32:49,410 [Executor task launch worker for task 13] INFO [org.apache.spark.executor.Executor] - Finished task 13.0 in stage 0.0 (TID 13). 755 bytes result sent to driver
2021-12-07 14:32:49,411 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 13.0 in stage 0.0 (TID 13) in 79248 ms on localhost (executor driver) (9/20)
2021-12-07 14:32:49,990 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 0.0 (TID 2). 755 bytes result sent to driver
2021-12-07 14:32:49,991 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 0.0 (TID 2) in 192232 ms on localhost (executor driver) (10/20)
2021-12-07 14:32:58,204 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Finished task 5.0 in stage 0.0 (TID 5). 755 bytes result sent to driver
2021-12-07 14:32:58,204 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 5.0 in stage 0.0 (TID 5) in 200444 ms on localhost (executor driver) (11/20)
2021-12-07 14:33:01,808 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 755 bytes result sent to driver
2021-12-07 14:33:01,808 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 204059 ms on localhost (executor driver) (12/20)
2021-12-07 14:33:02,245 [Executor task launch worker for task 6] INFO [org.apache.spark.executor.Executor] - Finished task 6.0 in stage 0.0 (TID 6). 755 bytes result sent to driver
2021-12-07 14:33:02,246 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 6.0 in stage 0.0 (TID 6) in 204486 ms on localhost (executor driver) (13/20)
2021-12-07 14:33:08,704 [Executor task launch worker for task 8] INFO [org.apache.spark.executor.Executor] - Finished task 8.0 in stage 0.0 (TID 8). 755 bytes result sent to driver
2021-12-07 14:33:08,704 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 8.0 in stage 0.0 (TID 8) in 210944 ms on localhost (executor driver) (14/20)
2021-12-07 14:33:30,869 [Executor task launch worker for task 18] INFO [org.apache.spark.executor.Executor] - Finished task 18.0 in stage 0.0 (TID 18). 712 bytes result sent to driver
2021-12-07 14:33:30,870 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 18.0 in stage 0.0 (TID 18) in 51672 ms on localhost (executor driver) (15/20)
2021-12-07 14:33:37,542 [Executor task launch worker for task 19] INFO [org.apache.spark.executor.Executor] - Finished task 19.0 in stage 0.0 (TID 19). 755 bytes result sent to driver
2021-12-07 14:33:37,543 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 19.0 in stage 0.0 (TID 19) in 50957 ms on localhost (executor driver) (16/20)
2021-12-07 14:33:59,237 [Executor task launch worker for task 14] INFO [org.apache.spark.executor.Executor] - Finished task 14.0 in stage 0.0 (TID 14). 712 bytes result sent to driver
2021-12-07 14:33:59,238 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 14.0 in stage 0.0 (TID 14) in 123164 ms on localhost (executor driver) (17/20)
2021-12-07 14:34:11,466 [Executor task launch worker for task 15] INFO [org.apache.spark.executor.Executor] - Finished task 15.0 in stage 0.0 (TID 15). 755 bytes result sent to driver
2021-12-07 14:34:11,466 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 15.0 in stage 0.0 (TID 15) in 134419 ms on localhost (executor driver) (18/20)
2021-12-07 14:34:21,983 [Executor task launch worker for task 17] INFO [org.apache.spark.executor.Executor] - Finished task 17.0 in stage 0.0 (TID 17). 712 bytes result sent to driver
2021-12-07 14:34:21,984 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 17.0 in stage 0.0 (TID 17) in 104061 ms on localhost (executor driver) (19/20)
2021-12-07 14:34:24,401 [Executor task launch worker for task 16] INFO [org.apache.spark.executor.Executor] - Finished task 16.0 in stage 0.0 (TID 16). 755 bytes result sent to driver
2021-12-07 14:34:24,401 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 16.0 in stage 0.0 (TID 16) in 111465 ms on localhost (executor driver) (20/20)
2021-12-07 14:34:24,402 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2021-12-07 14:34:24,403 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (count at PaidPromotionAdjustParameter.scala:52) finished in 286.709 s
2021-12-07 14:34:24,406 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: count at PaidPromotionAdjustParameter.scala:52, took 286.744565 s
2021-12-07 14:34:24,407 [main] INFO [PaidPromotionAdjustParameter$] - 收视总数60870678
2021-12-07 14:34:24,424 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:61
2021-12-07 14:34:24,431 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 3 (map at PaidPromotionAdjustParameter.scala:60)
2021-12-07 14:34:24,432 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (count at PaidPromotionAdjustParameter.scala:61) with 20 output partitions
2021-12-07 14:34:24,432 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (count at PaidPromotionAdjustParameter.scala:61)
2021-12-07 14:34:24,432 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 1)
2021-12-07 14:34:24,432 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 1)
2021-12-07 14:34:24,433 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at map at PaidPromotionAdjustParameter.scala:60), which has no missing parents
2021-12-07 14:34:24,442 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 4.7 KB, free 1990.5 MB)
2021-12-07 14:34:24,445 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 1990.5 MB)
2021-12-07 14:34:24,446 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on qb:60003 (size: 2.7 KB, free: 1990.8 MB)
2021-12-07 14:34:24,446 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2021-12-07 14:34:24,448 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 20 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at map at PaidPromotionAdjustParameter.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2021-12-07 14:34:24,448 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 20 tasks
2021-12-07 14:34:24,449 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 20, localhost, executor driver, partition 0, ANY, 7888 bytes)
2021-12-07 14:34:24,449 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 21, localhost, executor driver, partition 1, ANY, 7888 bytes)
2021-12-07 14:34:24,449 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 1.0 (TID 22, localhost, executor driver, partition 2, ANY, 7888 bytes)
2021-12-07 14:34:24,449 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 1.0 (TID 23, localhost, executor driver, partition 3, ANY, 7888 bytes)
2021-12-07 14:34:24,450 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 4.0 in stage 1.0 (TID 24, localhost, executor driver, partition 4, ANY, 7888 bytes)
2021-12-07 14:34:24,450 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 5.0 in stage 1.0 (TID 25, localhost, executor driver, partition 5, ANY, 7888 bytes)
2021-12-07 14:34:24,450 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 6.0 in stage 1.0 (TID 26, localhost, executor driver, partition 6, ANY, 7888 bytes)
2021-12-07 14:34:24,450 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 7.0 in stage 1.0 (TID 27, localhost, executor driver, partition 7, ANY, 7888 bytes)
2021-12-07 14:34:24,450 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 8.0 in stage 1.0 (TID 28, localhost, executor driver, partition 8, ANY, 7888 bytes)
2021-12-07 14:34:24,450 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 9.0 in stage 1.0 (TID 29, localhost, executor driver, partition 9, ANY, 7888 bytes)
2021-12-07 14:34:24,453 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 10.0 in stage 1.0 (TID 30, localhost, executor driver, partition 10, ANY, 7888 bytes)
2021-12-07 14:34:24,454 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 11.0 in stage 1.0 (TID 31, localhost, executor driver, partition 11, ANY, 7888 bytes)
2021-12-07 14:34:24,454 [Executor task launch worker for task 20] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 20)
2021-12-07 14:34:24,454 [Executor task launch worker for task 23] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 1.0 (TID 23)
2021-12-07 14:34:24,454 [Executor task launch worker for task 25] INFO [org.apache.spark.executor.Executor] - Running task 5.0 in stage 1.0 (TID 25)
2021-12-07 14:34:24,454 [Executor task launch worker for task 22] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 1.0 (TID 22)
2021-12-07 14:34:24,454 [Executor task launch worker for task 24] INFO [org.apache.spark.executor.Executor] - Running task 4.0 in stage 1.0 (TID 24)
2021-12-07 14:34:24,454 [Executor task launch worker for task 21] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 21)
2021-12-07 14:34:24,455 [Executor task launch worker for task 26] INFO [org.apache.spark.executor.Executor] - Running task 6.0 in stage 1.0 (TID 26)
2021-12-07 14:34:24,455 [Executor task launch worker for task 27] INFO [org.apache.spark.executor.Executor] - Running task 7.0 in stage 1.0 (TID 27)
2021-12-07 14:34:24,455 [Executor task launch worker for task 29] INFO [org.apache.spark.executor.Executor] - Running task 9.0 in stage 1.0 (TID 29)
2021-12-07 14:34:24,456 [Executor task launch worker for task 30] INFO [org.apache.spark.executor.Executor] - Running task 10.0 in stage 1.0 (TID 30)
2021-12-07 14:34:24,456 [Executor task launch worker for task 28] INFO [org.apache.spark.executor.Executor] - Running task 8.0 in stage 1.0 (TID 28)
2021-12-07 14:34:24,456 [Executor task launch worker for task 31] INFO [org.apache.spark.executor.Executor] - Running task 11.0 in stage 1.0 (TID 31)
2021-12-07 14:34:24,459 [Executor task launch worker for task 29] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1207959552+134217728
2021-12-07 14:34:24,459 [Executor task launch worker for task 25] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:671088640+134217728
2021-12-07 14:34:24,459 [Executor task launch worker for task 28] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1073741824+134217728
2021-12-07 14:34:24,459 [Executor task launch worker for task 24] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:536870912+134217728
2021-12-07 14:34:24,459 [Executor task launch worker for task 26] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:805306368+134217728
2021-12-07 14:34:24,459 [Executor task launch worker for task 20] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:0+134217728
2021-12-07 14:34:24,459 [Executor task launch worker for task 21] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:134217728+134217728
2021-12-07 14:34:24,459 [Executor task launch worker for task 23] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:402653184+134217728
2021-12-07 14:34:24,459 [Executor task launch worker for task 27] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:939524096+134217728
2021-12-07 14:34:24,459 [Executor task launch worker for task 31] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1476395008+134217728
2021-12-07 14:34:24,459 [Executor task launch worker for task 30] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1342177280+134217728
2021-12-07 14:34:24,459 [Executor task launch worker for task 22] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:268435456+134217728
2021-12-07 14:36:09,854 [Executor task launch worker for task 26] INFO [org.apache.spark.executor.Executor] - Finished task 6.0 in stage 1.0 (TID 26). 1050 bytes result sent to driver
2021-12-07 14:36:09,855 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 12.0 in stage 1.0 (TID 32, localhost, executor driver, partition 12, ANY, 7888 bytes)
2021-12-07 14:36:09,855 [Executor task launch worker for task 32] INFO [org.apache.spark.executor.Executor] - Running task 12.0 in stage 1.0 (TID 32)
2021-12-07 14:36:09,856 [Executor task launch worker for task 32] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1610612736+134217728
2021-12-07 14:36:09,869 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 6.0 in stage 1.0 (TID 26) in 105419 ms on localhost (executor driver) (1/20)
2021-12-07 14:36:17,991 [Executor task launch worker for task 24] INFO [org.apache.spark.executor.Executor] - Finished task 4.0 in stage 1.0 (TID 24). 1050 bytes result sent to driver
2021-12-07 14:36:17,992 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 13.0 in stage 1.0 (TID 33, localhost, executor driver, partition 13, ANY, 7888 bytes)
2021-12-07 14:36:17,992 [Executor task launch worker for task 33] INFO [org.apache.spark.executor.Executor] - Running task 13.0 in stage 1.0 (TID 33)
2021-12-07 14:36:17,992 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 4.0 in stage 1.0 (TID 24) in 113542 ms on localhost (executor driver) (2/20)
2021-12-07 14:36:17,993 [Executor task launch worker for task 33] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1744830464+134217728
2021-12-07 14:36:36,947 [Executor task launch worker for task 22] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 1.0 (TID 22). 1050 bytes result sent to driver
2021-12-07 14:36:36,948 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 14.0 in stage 1.0 (TID 34, localhost, executor driver, partition 14, ANY, 7888 bytes)
2021-12-07 14:36:36,948 [Executor task launch worker for task 34] INFO [org.apache.spark.executor.Executor] - Running task 14.0 in stage 1.0 (TID 34)
2021-12-07 14:36:36,949 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 1.0 (TID 22) in 132500 ms on localhost (executor driver) (3/20)
2021-12-07 14:36:36,949 [Executor task launch worker for task 34] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1879048192+134217728
2021-12-07 14:36:57,581 [Executor task launch worker for task 20] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 20). 1050 bytes result sent to driver
2021-12-07 14:36:57,582 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 15.0 in stage 1.0 (TID 35, localhost, executor driver, partition 15, ANY, 7888 bytes)
2021-12-07 14:36:57,582 [Executor task launch worker for task 35] INFO [org.apache.spark.executor.Executor] - Running task 15.0 in stage 1.0 (TID 35)
2021-12-07 14:36:57,582 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 20) in 153134 ms on localhost (executor driver) (4/20)
2021-12-07 14:36:57,583 [Executor task launch worker for task 35] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:2013265920+134217728
2021-12-07 14:36:59,618 [Executor task launch worker for task 21] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 21). 1050 bytes result sent to driver
2021-12-07 14:36:59,618 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 16.0 in stage 1.0 (TID 36, localhost, executor driver, partition 16, ANY, 7888 bytes)
2021-12-07 14:36:59,618 [Executor task launch worker for task 36] INFO [org.apache.spark.executor.Executor] - Running task 16.0 in stage 1.0 (TID 36)
2021-12-07 14:36:59,618 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 21) in 155169 ms on localhost (executor driver) (5/20)
2021-12-07 14:36:59,619 [Executor task launch worker for task 36] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:2147483648+134217728
2021-12-07 14:37:05,687 [Executor task launch worker for task 30] INFO [org.apache.spark.executor.Executor] - Finished task 10.0 in stage 1.0 (TID 30). 1050 bytes result sent to driver
2021-12-07 14:37:05,688 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 17.0 in stage 1.0 (TID 37, localhost, executor driver, partition 17, ANY, 7888 bytes)
2021-12-07 14:37:05,688 [Executor task launch worker for task 37] INFO [org.apache.spark.executor.Executor] - Running task 17.0 in stage 1.0 (TID 37)
2021-12-07 14:37:05,688 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 10.0 in stage 1.0 (TID 30) in 161238 ms on localhost (executor driver) (6/20)
2021-12-07 14:37:05,688 [Executor task launch worker for task 37] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:2281701376+134217728
2021-12-07 14:37:21,871 [Executor task launch worker for task 23] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 1.0 (TID 23). 1050 bytes result sent to driver
2021-12-07 14:37:21,871 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 18.0 in stage 1.0 (TID 38, localhost, executor driver, partition 18, ANY, 7888 bytes)
2021-12-07 14:37:21,871 [Executor task launch worker for task 38] INFO [org.apache.spark.executor.Executor] - Running task 18.0 in stage 1.0 (TID 38)
2021-12-07 14:37:21,871 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 1.0 (TID 23) in 177422 ms on localhost (executor driver) (7/20)
2021-12-07 14:37:21,872 [Executor task launch worker for task 38] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:2415919104+134217728
2021-12-07 14:37:36,406 [Executor task launch worker for task 29] INFO [org.apache.spark.executor.Executor] - Finished task 9.0 in stage 1.0 (TID 29). 1050 bytes result sent to driver
2021-12-07 14:37:36,407 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 19.0 in stage 1.0 (TID 39, localhost, executor driver, partition 19, ANY, 7888 bytes)
2021-12-07 14:37:36,407 [Executor task launch worker for task 39] INFO [org.apache.spark.executor.Executor] - Running task 19.0 in stage 1.0 (TID 39)
2021-12-07 14:37:36,407 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 9.0 in stage 1.0 (TID 29) in 191957 ms on localhost (executor driver) (8/20)
2021-12-07 14:37:36,408 [Executor task launch worker for task 39] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:2550136832+85027535
2021-12-07 14:37:40,064 [Executor task launch worker for task 28] INFO [org.apache.spark.executor.Executor] - Finished task 8.0 in stage 1.0 (TID 28). 1050 bytes result sent to driver
2021-12-07 14:37:40,064 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 8.0 in stage 1.0 (TID 28) in 195614 ms on localhost (executor driver) (9/20)
2021-12-07 14:37:50,326 [Executor task launch worker for task 27] INFO [org.apache.spark.executor.Executor] - Finished task 7.0 in stage 1.0 (TID 27). 1050 bytes result sent to driver
2021-12-07 14:37:50,327 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 7.0 in stage 1.0 (TID 27) in 205877 ms on localhost (executor driver) (10/20)
2021-12-07 14:37:53,315 [Executor task launch worker for task 25] INFO [org.apache.spark.executor.Executor] - Finished task 5.0 in stage 1.0 (TID 25). 1050 bytes result sent to driver
2021-12-07 14:37:53,315 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 5.0 in stage 1.0 (TID 25) in 208865 ms on localhost (executor driver) (11/20)
2021-12-07 14:37:56,384 [Executor task launch worker for task 31] INFO [org.apache.spark.executor.Executor] - Finished task 11.0 in stage 1.0 (TID 31). 1050 bytes result sent to driver
2021-12-07 14:37:56,385 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 11.0 in stage 1.0 (TID 31) in 211931 ms on localhost (executor driver) (12/20)
2021-12-07 14:38:12,031 [Executor task launch worker for task 34] INFO [org.apache.spark.executor.Executor] - Finished task 14.0 in stage 1.0 (TID 34). 1050 bytes result sent to driver
2021-12-07 14:38:12,031 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 14.0 in stage 1.0 (TID 34) in 95083 ms on localhost (executor driver) (13/20)
2021-12-07 14:38:25,208 [Executor task launch worker for task 37] INFO [org.apache.spark.executor.Executor] - Finished task 17.0 in stage 1.0 (TID 37). 1007 bytes result sent to driver
2021-12-07 14:38:25,208 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 17.0 in stage 1.0 (TID 37) in 79521 ms on localhost (executor driver) (14/20)
2021-12-07 14:38:41,251 [Executor task launch worker for task 36] INFO [org.apache.spark.executor.Executor] - Finished task 16.0 in stage 1.0 (TID 36). 1050 bytes result sent to driver
2021-12-07 14:38:41,252 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 16.0 in stage 1.0 (TID 36) in 101634 ms on localhost (executor driver) (15/20)
2021-12-07 14:38:45,799 [Executor task launch worker for task 32] INFO [org.apache.spark.executor.Executor] - Finished task 12.0 in stage 1.0 (TID 32). 1007 bytes result sent to driver
2021-12-07 14:38:45,799 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 12.0 in stage 1.0 (TID 32) in 155944 ms on localhost (executor driver) (16/20)
2021-12-07 14:38:47,026 [Executor task launch worker for task 33] INFO [org.apache.spark.executor.Executor] - Finished task 13.0 in stage 1.0 (TID 33). 1050 bytes result sent to driver
2021-12-07 14:38:47,026 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 13.0 in stage 1.0 (TID 33) in 149035 ms on localhost (executor driver) (17/20)
2021-12-07 14:38:47,604 [Executor task launch worker for task 39] INFO [org.apache.spark.executor.Executor] - Finished task 19.0 in stage 1.0 (TID 39). 1007 bytes result sent to driver
2021-12-07 14:38:47,605 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 19.0 in stage 1.0 (TID 39) in 71198 ms on localhost (executor driver) (18/20)
2021-12-07 14:38:53,926 [Executor task launch worker for task 35] INFO [org.apache.spark.executor.Executor] - Finished task 15.0 in stage 1.0 (TID 35). 1050 bytes result sent to driver
2021-12-07 14:38:53,927 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 15.0 in stage 1.0 (TID 35) in 116345 ms on localhost (executor driver) (19/20)
2021-12-07 14:38:55,514 [Executor task launch worker for task 38] INFO [org.apache.spark.executor.Executor] - Finished task 18.0 in stage 1.0 (TID 38). 1007 bytes result sent to driver
2021-12-07 14:38:55,515 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 18.0 in stage 1.0 (TID 38) in 93644 ms on localhost (executor driver) (20/20)
2021-12-07 14:38:55,515 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2021-12-07 14:38:55,515 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 1 (map at PaidPromotionAdjustParameter.scala:60) finished in 271.079 s
2021-12-07 14:38:55,516 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 14:38:55,516 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 14:38:55,516 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 2)
2021-12-07 14:38:55,516 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 14:38:55,519 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (ShuffledRDD[4] at reduceByKey at PaidPromotionAdjustParameter.scala:60), which has no missing parents
2021-12-07 14:38:55,524 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 3.0 KB, free 1990.5 MB)
2021-12-07 14:38:55,526 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1907.0 B, free 1990.5 MB)
2021-12-07 14:38:55,526 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on qb:60003 (size: 1907.0 B, free: 1990.8 MB)
2021-12-07 14:38:55,526 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2021-12-07 14:38:55,527 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 20 missing tasks from ResultStage 2 (ShuffledRDD[4] at reduceByKey at PaidPromotionAdjustParameter.scala:60) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2021-12-07 14:38:55,527 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 20 tasks
2021-12-07 14:38:55,528 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 40, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 14:38:55,528 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 41, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 14:38:55,528 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 2.0 (TID 42, localhost, executor driver, partition 2, ANY, 7649 bytes)
2021-12-07 14:38:55,528 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 2.0 (TID 43, localhost, executor driver, partition 3, ANY, 7649 bytes)
2021-12-07 14:38:55,528 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 4.0 in stage 2.0 (TID 44, localhost, executor driver, partition 4, ANY, 7649 bytes)
2021-12-07 14:38:55,528 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 5.0 in stage 2.0 (TID 45, localhost, executor driver, partition 5, ANY, 7649 bytes)
2021-12-07 14:38:55,528 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 6.0 in stage 2.0 (TID 46, localhost, executor driver, partition 6, ANY, 7649 bytes)
2021-12-07 14:38:55,529 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 7.0 in stage 2.0 (TID 47, localhost, executor driver, partition 7, ANY, 7649 bytes)
2021-12-07 14:38:55,529 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 8.0 in stage 2.0 (TID 48, localhost, executor driver, partition 8, ANY, 7649 bytes)
2021-12-07 14:38:55,529 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 9.0 in stage 2.0 (TID 49, localhost, executor driver, partition 9, ANY, 7649 bytes)
2021-12-07 14:38:55,529 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 10.0 in stage 2.0 (TID 50, localhost, executor driver, partition 10, ANY, 7649 bytes)
2021-12-07 14:38:55,529 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 11.0 in stage 2.0 (TID 51, localhost, executor driver, partition 11, ANY, 7649 bytes)
2021-12-07 14:38:55,529 [Executor task launch worker for task 40] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 40)
2021-12-07 14:38:55,529 [Executor task launch worker for task 45] INFO [org.apache.spark.executor.Executor] - Running task 5.0 in stage 2.0 (TID 45)
2021-12-07 14:38:55,529 [Executor task launch worker for task 43] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 2.0 (TID 43)
2021-12-07 14:38:55,529 [Executor task launch worker for task 46] INFO [org.apache.spark.executor.Executor] - Running task 6.0 in stage 2.0 (TID 46)
2021-12-07 14:38:55,529 [Executor task launch worker for task 47] INFO [org.apache.spark.executor.Executor] - Running task 7.0 in stage 2.0 (TID 47)
2021-12-07 14:38:55,529 [Executor task launch worker for task 42] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 2.0 (TID 42)
2021-12-07 14:38:55,529 [Executor task launch worker for task 41] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 41)
2021-12-07 14:38:55,529 [Executor task launch worker for task 44] INFO [org.apache.spark.executor.Executor] - Running task 4.0 in stage 2.0 (TID 44)
2021-12-07 14:38:55,530 [Executor task launch worker for task 50] INFO [org.apache.spark.executor.Executor] - Running task 10.0 in stage 2.0 (TID 50)
2021-12-07 14:38:55,530 [Executor task launch worker for task 49] INFO [org.apache.spark.executor.Executor] - Running task 9.0 in stage 2.0 (TID 49)
2021-12-07 14:38:55,530 [Executor task launch worker for task 51] INFO [org.apache.spark.executor.Executor] - Running task 11.0 in stage 2.0 (TID 51)
2021-12-07 14:38:55,529 [Executor task launch worker for task 48] INFO [org.apache.spark.executor.Executor] - Running task 8.0 in stage 2.0 (TID 48)
2021-12-07 14:38:55,542 [Executor task launch worker for task 43] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,542 [Executor task launch worker for task 51] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,542 [Executor task launch worker for task 49] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,542 [Executor task launch worker for task 50] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,542 [Executor task launch worker for task 44] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,542 [Executor task launch worker for task 40] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,542 [Executor task launch worker for task 42] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,542 [Executor task launch worker for task 41] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,542 [Executor task launch worker for task 48] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,542 [Executor task launch worker for task 47] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,542 [Executor task launch worker for task 46] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,542 [Executor task launch worker for task 45] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,544 [Executor task launch worker for task 41] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 5 ms
2021-12-07 14:38:55,544 [Executor task launch worker for task 49] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 5 ms
2021-12-07 14:38:55,544 [Executor task launch worker for task 43] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 6 ms
2021-12-07 14:38:55,544 [Executor task launch worker for task 47] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 6 ms
2021-12-07 14:38:55,544 [Executor task launch worker for task 50] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 6 ms
2021-12-07 14:38:55,544 [Executor task launch worker for task 45] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 5 ms
2021-12-07 14:38:55,544 [Executor task launch worker for task 40] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 6 ms
2021-12-07 14:38:55,544 [Executor task launch worker for task 44] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 6 ms
2021-12-07 14:38:55,544 [Executor task launch worker for task 48] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 6 ms
2021-12-07 14:38:55,544 [Executor task launch worker for task 46] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 6 ms
2021-12-07 14:38:55,544 [Executor task launch worker for task 42] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 5 ms
2021-12-07 14:38:55,544 [Executor task launch worker for task 51] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 6 ms
2021-12-07 14:38:55,672 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 23
2021-12-07 14:38:55,672 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 16
2021-12-07 14:38:55,672 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 22
2021-12-07 14:38:55,672 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 10
2021-12-07 14:38:55,672 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 3
2021-12-07 14:38:55,672 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 14
2021-12-07 14:38:55,672 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 21
2021-12-07 14:38:55,672 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 4
2021-12-07 14:38:55,672 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 15
2021-12-07 14:38:55,672 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 11
2021-12-07 14:38:55,673 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 8
2021-12-07 14:38:55,673 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 13
2021-12-07 14:38:55,673 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 20
2021-12-07 14:38:55,673 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 18
2021-12-07 14:38:55,673 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 9
2021-12-07 14:38:55,673 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 5
2021-12-07 14:38:55,702 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on qb:60003 in memory (size: 1903.0 B, free: 1990.8 MB)
2021-12-07 14:38:55,716 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 7
2021-12-07 14:38:55,719 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2_piece0 on qb:60003 in memory (size: 2.7 KB, free: 1990.8 MB)
2021-12-07 14:38:55,728 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 19
2021-12-07 14:38:55,728 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1
2021-12-07 14:38:55,728 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 24
2021-12-07 14:38:55,728 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 2
2021-12-07 14:38:55,728 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 0
2021-12-07 14:38:55,728 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 6
2021-12-07 14:38:55,728 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 12
2021-12-07 14:38:55,728 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 17
2021-12-07 14:38:55,952 [Executor task launch worker for task 46] INFO [org.apache.spark.executor.Executor] - Finished task 6.0 in stage 2.0 (TID 46). 1098 bytes result sent to driver
2021-12-07 14:38:55,954 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 12.0 in stage 2.0 (TID 52, localhost, executor driver, partition 12, ANY, 7649 bytes)
2021-12-07 14:38:55,955 [Executor task launch worker for task 48] INFO [org.apache.spark.executor.Executor] - Finished task 8.0 in stage 2.0 (TID 48). 1098 bytes result sent to driver
2021-12-07 14:38:55,956 [Executor task launch worker for task 52] INFO [org.apache.spark.executor.Executor] - Running task 12.0 in stage 2.0 (TID 52)
2021-12-07 14:38:55,956 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 13.0 in stage 2.0 (TID 53, localhost, executor driver, partition 13, ANY, 7649 bytes)
2021-12-07 14:38:55,956 [Executor task launch worker for task 51] INFO [org.apache.spark.executor.Executor] - Finished task 11.0 in stage 2.0 (TID 51). 1098 bytes result sent to driver
2021-12-07 14:38:55,956 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 6.0 in stage 2.0 (TID 46) in 428 ms on localhost (executor driver) (1/20)
2021-12-07 14:38:55,956 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 8.0 in stage 2.0 (TID 48) in 427 ms on localhost (executor driver) (2/20)
2021-12-07 14:38:55,957 [Executor task launch worker for task 53] INFO [org.apache.spark.executor.Executor] - Running task 13.0 in stage 2.0 (TID 53)
2021-12-07 14:38:55,957 [Executor task launch worker for task 52] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,957 [Executor task launch worker for task 52] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:55,958 [Executor task launch worker for task 53] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,958 [Executor task launch worker for task 53] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:55,958 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 14.0 in stage 2.0 (TID 54, localhost, executor driver, partition 14, ANY, 7649 bytes)
2021-12-07 14:38:55,959 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 11.0 in stage 2.0 (TID 51) in 430 ms on localhost (executor driver) (3/20)
2021-12-07 14:38:55,963 [Executor task launch worker for task 40] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 40). 1098 bytes result sent to driver
2021-12-07 14:38:55,963 [Executor task launch worker for task 50] INFO [org.apache.spark.executor.Executor] - Finished task 10.0 in stage 2.0 (TID 50). 1098 bytes result sent to driver
2021-12-07 14:38:55,964 [Executor task launch worker for task 54] INFO [org.apache.spark.executor.Executor] - Running task 14.0 in stage 2.0 (TID 54)
2021-12-07 14:38:55,965 [Executor task launch worker for task 54] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,965 [Executor task launch worker for task 54] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:55,965 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 15.0 in stage 2.0 (TID 55, localhost, executor driver, partition 15, ANY, 7649 bytes)
2021-12-07 14:38:55,966 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 16.0 in stage 2.0 (TID 56, localhost, executor driver, partition 16, ANY, 7649 bytes)
2021-12-07 14:38:55,966 [Executor task launch worker for task 55] INFO [org.apache.spark.executor.Executor] - Running task 15.0 in stage 2.0 (TID 55)
2021-12-07 14:38:55,966 [Executor task launch worker for task 56] INFO [org.apache.spark.executor.Executor] - Running task 16.0 in stage 2.0 (TID 56)
2021-12-07 14:38:55,966 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 40) in 439 ms on localhost (executor driver) (4/20)
2021-12-07 14:38:55,968 [Executor task launch worker for task 41] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2.0 (TID 41). 1098 bytes result sent to driver
2021-12-07 14:38:55,968 [Executor task launch worker for task 43] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 2.0 (TID 43). 1098 bytes result sent to driver
2021-12-07 14:38:55,969 [Executor task launch worker for task 56] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,969 [Executor task launch worker for task 56] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:55,969 [Executor task launch worker for task 55] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,969 [Executor task launch worker for task 55] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 2 ms
2021-12-07 14:38:55,968 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 10.0 in stage 2.0 (TID 50) in 439 ms on localhost (executor driver) (5/20)
2021-12-07 14:38:55,970 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 17.0 in stage 2.0 (TID 57, localhost, executor driver, partition 17, ANY, 7649 bytes)
2021-12-07 14:38:55,970 [Executor task launch worker for task 44] INFO [org.apache.spark.executor.Executor] - Finished task 4.0 in stage 2.0 (TID 44). 1098 bytes result sent to driver
2021-12-07 14:38:55,970 [Executor task launch worker for task 45] INFO [org.apache.spark.executor.Executor] - Finished task 5.0 in stage 2.0 (TID 45). 1098 bytes result sent to driver
2021-12-07 14:38:55,970 [Executor task launch worker for task 49] INFO [org.apache.spark.executor.Executor] - Finished task 9.0 in stage 2.0 (TID 49). 1098 bytes result sent to driver
2021-12-07 14:38:55,970 [Executor task launch worker for task 42] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 2.0 (TID 42). 1098 bytes result sent to driver
2021-12-07 14:38:55,970 [Executor task launch worker for task 47] INFO [org.apache.spark.executor.Executor] - Finished task 7.0 in stage 2.0 (TID 47). 1098 bytes result sent to driver
2021-12-07 14:38:55,970 [Executor task launch worker for task 57] INFO [org.apache.spark.executor.Executor] - Running task 17.0 in stage 2.0 (TID 57)
2021-12-07 14:38:55,971 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 18.0 in stage 2.0 (TID 58, localhost, executor driver, partition 18, ANY, 7649 bytes)
2021-12-07 14:38:55,971 [Executor task launch worker for task 58] INFO [org.apache.spark.executor.Executor] - Running task 18.0 in stage 2.0 (TID 58)
2021-12-07 14:38:55,971 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 2.0 (TID 43) in 443 ms on localhost (executor driver) (6/20)
2021-12-07 14:38:55,971 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 4.0 in stage 2.0 (TID 44) in 443 ms on localhost (executor driver) (7/20)
2021-12-07 14:38:55,972 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 19.0 in stage 2.0 (TID 59, localhost, executor driver, partition 19, ANY, 7649 bytes)
2021-12-07 14:38:55,972 [Executor task launch worker for task 57] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,972 [Executor task launch worker for task 57] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:55,972 [Executor task launch worker for task 59] INFO [org.apache.spark.executor.Executor] - Running task 19.0 in stage 2.0 (TID 59)
2021-12-07 14:38:55,972 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2.0 (TID 41) in 444 ms on localhost (executor driver) (8/20)
2021-12-07 14:38:55,972 [Executor task launch worker for task 58] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,972 [Executor task launch worker for task 58] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:55,972 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 5.0 in stage 2.0 (TID 45) in 444 ms on localhost (executor driver) (9/20)
2021-12-07 14:38:55,973 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 2.0 (TID 42) in 445 ms on localhost (executor driver) (10/20)
2021-12-07 14:38:55,973 [Executor task launch worker for task 59] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:55,973 [Executor task launch worker for task 59] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:55,973 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 9.0 in stage 2.0 (TID 49) in 444 ms on localhost (executor driver) (11/20)
2021-12-07 14:38:55,973 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 7.0 in stage 2.0 (TID 47) in 444 ms on localhost (executor driver) (12/20)
2021-12-07 14:38:56,054 [Executor task launch worker for task 52] INFO [org.apache.spark.executor.Executor] - Finished task 12.0 in stage 2.0 (TID 52). 1055 bytes result sent to driver
2021-12-07 14:38:56,056 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 12.0 in stage 2.0 (TID 52) in 101 ms on localhost (executor driver) (13/20)
2021-12-07 14:38:56,057 [Executor task launch worker for task 56] INFO [org.apache.spark.executor.Executor] - Finished task 16.0 in stage 2.0 (TID 56). 1098 bytes result sent to driver
2021-12-07 14:38:56,058 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 16.0 in stage 2.0 (TID 56) in 92 ms on localhost (executor driver) (14/20)
2021-12-07 14:38:56,062 [Executor task launch worker for task 57] INFO [org.apache.spark.executor.Executor] - Finished task 17.0 in stage 2.0 (TID 57). 1098 bytes result sent to driver
2021-12-07 14:38:56,062 [Executor task launch worker for task 55] INFO [org.apache.spark.executor.Executor] - Finished task 15.0 in stage 2.0 (TID 55). 1098 bytes result sent to driver
2021-12-07 14:38:56,063 [Executor task launch worker for task 58] INFO [org.apache.spark.executor.Executor] - Finished task 18.0 in stage 2.0 (TID 58). 1098 bytes result sent to driver
2021-12-07 14:38:56,063 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 17.0 in stage 2.0 (TID 57) in 94 ms on localhost (executor driver) (15/20)
2021-12-07 14:38:56,063 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 15.0 in stage 2.0 (TID 55) in 98 ms on localhost (executor driver) (16/20)
2021-12-07 14:38:56,063 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 18.0 in stage 2.0 (TID 58) in 93 ms on localhost (executor driver) (17/20)
2021-12-07 14:38:56,063 [Executor task launch worker for task 53] INFO [org.apache.spark.executor.Executor] - Finished task 13.0 in stage 2.0 (TID 53). 1098 bytes result sent to driver
2021-12-07 14:38:56,063 [Executor task launch worker for task 54] INFO [org.apache.spark.executor.Executor] - Finished task 14.0 in stage 2.0 (TID 54). 1098 bytes result sent to driver
2021-12-07 14:38:56,063 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 13.0 in stage 2.0 (TID 53) in 107 ms on localhost (executor driver) (18/20)
2021-12-07 14:38:56,064 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 14.0 in stage 2.0 (TID 54) in 106 ms on localhost (executor driver) (19/20)
2021-12-07 14:38:56,067 [Executor task launch worker for task 59] INFO [org.apache.spark.executor.Executor] - Finished task 19.0 in stage 2.0 (TID 59). 1055 bytes result sent to driver
2021-12-07 14:38:56,067 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 19.0 in stage 2.0 (TID 59) in 96 ms on localhost (executor driver) (20/20)
2021-12-07 14:38:56,067 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2021-12-07 14:38:56,067 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (count at PaidPromotionAdjustParameter.scala:61) finished in 0.545 s
2021-12-07 14:38:56,067 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: count at PaidPromotionAdjustParameter.scala:61, took 271.643491 s
2021-12-07 14:38:56,068 [main] INFO [PaidPromotionAdjustParameter$] - 用户总数627740
2021-12-07 14:38:56,088 [main] INFO [org.apache.spark.SparkContext] - Starting job: sortByKey at PaidPromotionAdjustParameter.scala:66
2021-12-07 14:38:56,088 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (sortByKey at PaidPromotionAdjustParameter.scala:66) with 20 output partitions
2021-12-07 14:38:56,088 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (sortByKey at PaidPromotionAdjustParameter.scala:66)
2021-12-07 14:38:56,088 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 3)
2021-12-07 14:38:56,088 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 14:38:56,089 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (MapPartitionsRDD[7] at sortByKey at PaidPromotionAdjustParameter.scala:66), which has no missing parents
2021-12-07 14:38:56,092 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 4.1 KB, free 1990.5 MB)
2021-12-07 14:38:56,093 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1990.5 MB)
2021-12-07 14:38:56,094 [dispatcher-event-loop-5] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on qb:60003 (size: 2.4 KB, free: 1990.8 MB)
2021-12-07 14:38:56,094 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1039
2021-12-07 14:38:56,094 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 20 missing tasks from ResultStage 4 (MapPartitionsRDD[7] at sortByKey at PaidPromotionAdjustParameter.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2021-12-07 14:38:56,094 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 20 tasks
2021-12-07 14:38:56,095 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 60, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 14:38:56,095 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 4.0 (TID 61, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 14:38:56,095 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 4.0 (TID 62, localhost, executor driver, partition 2, ANY, 7649 bytes)
2021-12-07 14:38:56,095 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 4.0 (TID 63, localhost, executor driver, partition 3, ANY, 7649 bytes)
2021-12-07 14:38:56,095 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 4.0 in stage 4.0 (TID 64, localhost, executor driver, partition 4, ANY, 7649 bytes)
2021-12-07 14:38:56,096 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 5.0 in stage 4.0 (TID 65, localhost, executor driver, partition 5, ANY, 7649 bytes)
2021-12-07 14:38:56,096 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 6.0 in stage 4.0 (TID 66, localhost, executor driver, partition 6, ANY, 7649 bytes)
2021-12-07 14:38:56,096 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 7.0 in stage 4.0 (TID 67, localhost, executor driver, partition 7, ANY, 7649 bytes)
2021-12-07 14:38:56,096 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 8.0 in stage 4.0 (TID 68, localhost, executor driver, partition 8, ANY, 7649 bytes)
2021-12-07 14:38:56,096 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 9.0 in stage 4.0 (TID 69, localhost, executor driver, partition 9, ANY, 7649 bytes)
2021-12-07 14:38:56,096 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 10.0 in stage 4.0 (TID 70, localhost, executor driver, partition 10, ANY, 7649 bytes)
2021-12-07 14:38:56,096 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 11.0 in stage 4.0 (TID 71, localhost, executor driver, partition 11, ANY, 7649 bytes)
2021-12-07 14:38:56,096 [Executor task launch worker for task 64] INFO [org.apache.spark.executor.Executor] - Running task 4.0 in stage 4.0 (TID 64)
2021-12-07 14:38:56,096 [Executor task launch worker for task 60] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 60)
2021-12-07 14:38:56,096 [Executor task launch worker for task 70] INFO [org.apache.spark.executor.Executor] - Running task 10.0 in stage 4.0 (TID 70)
2021-12-07 14:38:56,096 [Executor task launch worker for task 67] INFO [org.apache.spark.executor.Executor] - Running task 7.0 in stage 4.0 (TID 67)
2021-12-07 14:38:56,096 [Executor task launch worker for task 66] INFO [org.apache.spark.executor.Executor] - Running task 6.0 in stage 4.0 (TID 66)
2021-12-07 14:38:56,096 [Executor task launch worker for task 65] INFO [org.apache.spark.executor.Executor] - Running task 5.0 in stage 4.0 (TID 65)
2021-12-07 14:38:56,096 [Executor task launch worker for task 61] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 4.0 (TID 61)
2021-12-07 14:38:56,096 [Executor task launch worker for task 62] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 4.0 (TID 62)
2021-12-07 14:38:56,096 [Executor task launch worker for task 63] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 4.0 (TID 63)
2021-12-07 14:38:56,096 [Executor task launch worker for task 71] INFO [org.apache.spark.executor.Executor] - Running task 11.0 in stage 4.0 (TID 71)
2021-12-07 14:38:56,096 [Executor task launch worker for task 69] INFO [org.apache.spark.executor.Executor] - Running task 9.0 in stage 4.0 (TID 69)
2021-12-07 14:38:56,096 [Executor task launch worker for task 68] INFO [org.apache.spark.executor.Executor] - Running task 8.0 in stage 4.0 (TID 68)
2021-12-07 14:38:56,098 [Executor task launch worker for task 61] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,098 [Executor task launch worker for task 61] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,098 [Executor task launch worker for task 65] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,098 [Executor task launch worker for task 71] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,098 [Executor task launch worker for task 65] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,098 [Executor task launch worker for task 71] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,098 [Executor task launch worker for task 64] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,098 [Executor task launch worker for task 64] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,098 [Executor task launch worker for task 67] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,098 [Executor task launch worker for task 60] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,098 [Executor task launch worker for task 67] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,098 [Executor task launch worker for task 60] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,098 [Executor task launch worker for task 66] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,099 [Executor task launch worker for task 66] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:56,099 [Executor task launch worker for task 69] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,098 [Executor task launch worker for task 68] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,099 [Executor task launch worker for task 63] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,099 [Executor task launch worker for task 69] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:56,099 [Executor task launch worker for task 63] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:56,099 [Executor task launch worker for task 62] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,099 [Executor task launch worker for task 70] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,099 [Executor task launch worker for task 68] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:56,099 [Executor task launch worker for task 70] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,099 [Executor task launch worker for task 62] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,206 [Executor task launch worker for task 66] INFO [org.apache.spark.executor.Executor] - Finished task 6.0 in stage 4.0 (TID 66). 1201 bytes result sent to driver
2021-12-07 14:38:56,207 [Executor task launch worker for task 67] INFO [org.apache.spark.executor.Executor] - Finished task 7.0 in stage 4.0 (TID 67). 1196 bytes result sent to driver
2021-12-07 14:38:56,207 [Executor task launch worker for task 65] INFO [org.apache.spark.executor.Executor] - Finished task 5.0 in stage 4.0 (TID 65). 1204 bytes result sent to driver
2021-12-07 14:38:56,207 [Executor task launch worker for task 64] INFO [org.apache.spark.executor.Executor] - Finished task 4.0 in stage 4.0 (TID 64). 1201 bytes result sent to driver
2021-12-07 14:38:56,207 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 12.0 in stage 4.0 (TID 72, localhost, executor driver, partition 12, ANY, 7649 bytes)
2021-12-07 14:38:56,207 [Executor task launch worker for task 72] INFO [org.apache.spark.executor.Executor] - Running task 12.0 in stage 4.0 (TID 72)
2021-12-07 14:38:56,207 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 13.0 in stage 4.0 (TID 73, localhost, executor driver, partition 13, ANY, 7649 bytes)
2021-12-07 14:38:56,207 [Executor task launch worker for task 73] INFO [org.apache.spark.executor.Executor] - Running task 13.0 in stage 4.0 (TID 73)
2021-12-07 14:38:56,207 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 6.0 in stage 4.0 (TID 66) in 111 ms on localhost (executor driver) (1/20)
2021-12-07 14:38:56,208 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 14.0 in stage 4.0 (TID 74, localhost, executor driver, partition 14, ANY, 7649 bytes)
2021-12-07 14:38:56,208 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 5.0 in stage 4.0 (TID 65) in 113 ms on localhost (executor driver) (2/20)
2021-12-07 14:38:56,208 [Executor task launch worker for task 74] INFO [org.apache.spark.executor.Executor] - Running task 14.0 in stage 4.0 (TID 74)
2021-12-07 14:38:56,208 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 4.0 in stage 4.0 (TID 64) in 113 ms on localhost (executor driver) (3/20)
2021-12-07 14:38:56,208 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 7.0 in stage 4.0 (TID 67) in 112 ms on localhost (executor driver) (4/20)
2021-12-07 14:38:56,208 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 15.0 in stage 4.0 (TID 75, localhost, executor driver, partition 15, ANY, 7649 bytes)
2021-12-07 14:38:56,208 [Executor task launch worker for task 75] INFO [org.apache.spark.executor.Executor] - Running task 15.0 in stage 4.0 (TID 75)
2021-12-07 14:38:56,208 [Executor task launch worker for task 72] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,208 [Executor task launch worker for task 72] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,208 [Executor task launch worker for task 73] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,208 [Executor task launch worker for task 73] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,209 [Executor task launch worker for task 74] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,209 [Executor task launch worker for task 74] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,209 [Executor task launch worker for task 75] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,209 [Executor task launch worker for task 68] INFO [org.apache.spark.executor.Executor] - Finished task 8.0 in stage 4.0 (TID 68). 1201 bytes result sent to driver
2021-12-07 14:38:56,209 [Executor task launch worker for task 75] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,210 [dispatcher-event-loop-7] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 16.0 in stage 4.0 (TID 76, localhost, executor driver, partition 16, ANY, 7649 bytes)
2021-12-07 14:38:56,210 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 8.0 in stage 4.0 (TID 68) in 114 ms on localhost (executor driver) (5/20)
2021-12-07 14:38:56,210 [Executor task launch worker for task 76] INFO [org.apache.spark.executor.Executor] - Running task 16.0 in stage 4.0 (TID 76)
2021-12-07 14:38:56,210 [Executor task launch worker for task 70] INFO [org.apache.spark.executor.Executor] - Finished task 10.0 in stage 4.0 (TID 70). 1193 bytes result sent to driver
2021-12-07 14:38:56,210 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 17.0 in stage 4.0 (TID 77, localhost, executor driver, partition 17, ANY, 7649 bytes)
2021-12-07 14:38:56,210 [Executor task launch worker for task 77] INFO [org.apache.spark.executor.Executor] - Running task 17.0 in stage 4.0 (TID 77)
2021-12-07 14:38:56,210 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 10.0 in stage 4.0 (TID 70) in 114 ms on localhost (executor driver) (6/20)
2021-12-07 14:38:56,211 [Executor task launch worker for task 76] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,211 [Executor task launch worker for task 76] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,212 [Executor task launch worker for task 77] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,212 [Executor task launch worker for task 77] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,213 [Executor task launch worker for task 71] INFO [org.apache.spark.executor.Executor] - Finished task 11.0 in stage 4.0 (TID 71). 1203 bytes result sent to driver
2021-12-07 14:38:56,214 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 18.0 in stage 4.0 (TID 78, localhost, executor driver, partition 18, ANY, 7649 bytes)
2021-12-07 14:38:56,214 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 11.0 in stage 4.0 (TID 71) in 118 ms on localhost (executor driver) (7/20)
2021-12-07 14:38:56,214 [Executor task launch worker for task 78] INFO [org.apache.spark.executor.Executor] - Running task 18.0 in stage 4.0 (TID 78)
2021-12-07 14:38:56,215 [Executor task launch worker for task 62] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 4.0 (TID 62). 1196 bytes result sent to driver
2021-12-07 14:38:56,216 [Executor task launch worker for task 78] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,216 [Executor task launch worker for task 78] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,216 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 19.0 in stage 4.0 (TID 79, localhost, executor driver, partition 19, ANY, 7649 bytes)
2021-12-07 14:38:56,216 [Executor task launch worker for task 79] INFO [org.apache.spark.executor.Executor] - Running task 19.0 in stage 4.0 (TID 79)
2021-12-07 14:38:56,216 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 4.0 (TID 62) in 121 ms on localhost (executor driver) (8/20)
2021-12-07 14:38:56,217 [Executor task launch worker for task 79] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,218 [Executor task launch worker for task 79] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:56,218 [Executor task launch worker for task 69] INFO [org.apache.spark.executor.Executor] - Finished task 9.0 in stage 4.0 (TID 69). 1203 bytes result sent to driver
2021-12-07 14:38:56,218 [Executor task launch worker for task 60] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 60). 1201 bytes result sent to driver
2021-12-07 14:38:56,218 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 9.0 in stage 4.0 (TID 69) in 122 ms on localhost (executor driver) (9/20)
2021-12-07 14:38:56,219 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 60) in 124 ms on localhost (executor driver) (10/20)
2021-12-07 14:38:56,228 [Executor task launch worker for task 63] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 4.0 (TID 63). 1207 bytes result sent to driver
2021-12-07 14:38:56,228 [Executor task launch worker for task 61] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 4.0 (TID 61). 1205 bytes result sent to driver
2021-12-07 14:38:56,241 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 4.0 (TID 63) in 146 ms on localhost (executor driver) (11/20)
2021-12-07 14:38:56,246 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 4.0 (TID 61) in 151 ms on localhost (executor driver) (12/20)
2021-12-07 14:38:56,294 [Executor task launch worker for task 72] INFO [org.apache.spark.executor.Executor] - Finished task 12.0 in stage 4.0 (TID 72). 1157 bytes result sent to driver
2021-12-07 14:38:56,295 [Executor task launch worker for task 73] INFO [org.apache.spark.executor.Executor] - Finished task 13.0 in stage 4.0 (TID 73). 1243 bytes result sent to driver
2021-12-07 14:38:56,295 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 12.0 in stage 4.0 (TID 72) in 88 ms on localhost (executor driver) (13/20)
2021-12-07 14:38:56,295 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 13.0 in stage 4.0 (TID 73) in 88 ms on localhost (executor driver) (14/20)
2021-12-07 14:38:56,296 [Executor task launch worker for task 74] INFO [org.apache.spark.executor.Executor] - Finished task 14.0 in stage 4.0 (TID 74). 1162 bytes result sent to driver
2021-12-07 14:38:56,296 [Executor task launch worker for task 79] INFO [org.apache.spark.executor.Executor] - Finished task 19.0 in stage 4.0 (TID 79). 1200 bytes result sent to driver
2021-12-07 14:38:56,296 [Executor task launch worker for task 78] INFO [org.apache.spark.executor.Executor] - Finished task 18.0 in stage 4.0 (TID 78). 1160 bytes result sent to driver
2021-12-07 14:38:56,296 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 14.0 in stage 4.0 (TID 74) in 88 ms on localhost (executor driver) (15/20)
2021-12-07 14:38:56,296 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 19.0 in stage 4.0 (TID 79) in 81 ms on localhost (executor driver) (16/20)
2021-12-07 14:38:56,296 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 18.0 in stage 4.0 (TID 78) in 82 ms on localhost (executor driver) (17/20)
2021-12-07 14:38:56,298 [Executor task launch worker for task 76] INFO [org.apache.spark.executor.Executor] - Finished task 16.0 in stage 4.0 (TID 76). 1155 bytes result sent to driver
2021-12-07 14:38:56,298 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 16.0 in stage 4.0 (TID 76) in 89 ms on localhost (executor driver) (18/20)
2021-12-07 14:38:56,298 [Executor task launch worker for task 77] INFO [org.apache.spark.executor.Executor] - Finished task 17.0 in stage 4.0 (TID 77). 1205 bytes result sent to driver
2021-12-07 14:38:56,298 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 17.0 in stage 4.0 (TID 77) in 88 ms on localhost (executor driver) (19/20)
2021-12-07 14:38:56,298 [Executor task launch worker for task 75] INFO [org.apache.spark.executor.Executor] - Finished task 15.0 in stage 4.0 (TID 75). 1159 bytes result sent to driver
2021-12-07 14:38:56,299 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 15.0 in stage 4.0 (TID 75) in 91 ms on localhost (executor driver) (20/20)
2021-12-07 14:38:56,299 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2021-12-07 14:38:56,299 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (sortByKey at PaidPromotionAdjustParameter.scala:66) finished in 0.208 s
2021-12-07 14:38:56,299 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: sortByKey at PaidPromotionAdjustParameter.scala:66, took 0.211560 s
2021-12-07 14:38:56,313 [main] INFO [org.apache.spark.SparkContext] - Starting job: zipWithIndex at PaidPromotionAdjustParameter.scala:67
2021-12-07 14:38:56,314 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 5 (map at PaidPromotionAdjustParameter.scala:65)
2021-12-07 14:38:56,314 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (zipWithIndex at PaidPromotionAdjustParameter.scala:67) with 19 output partitions
2021-12-07 14:38:56,314 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 7 (zipWithIndex at PaidPromotionAdjustParameter.scala:67)
2021-12-07 14:38:56,314 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 6)
2021-12-07 14:38:56,314 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 6)
2021-12-07 14:38:56,314 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 6 (MapPartitionsRDD[5] at map at PaidPromotionAdjustParameter.scala:65), which has no missing parents
2021-12-07 14:38:56,321 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 4.1 KB, free 1990.5 MB)
2021-12-07 14:38:56,323 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1990.5 MB)
2021-12-07 14:38:56,323 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on qb:60003 (size: 2.4 KB, free: 1990.8 MB)
2021-12-07 14:38:56,324 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1039
2021-12-07 14:38:56,324 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 20 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[5] at map at PaidPromotionAdjustParameter.scala:65) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2021-12-07 14:38:56,324 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 6.0 with 20 tasks
2021-12-07 14:38:56,324 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 6.0 (TID 80, localhost, executor driver, partition 0, ANY, 7638 bytes)
2021-12-07 14:38:56,324 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 6.0 (TID 81, localhost, executor driver, partition 1, ANY, 7638 bytes)
2021-12-07 14:38:56,325 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 6.0 (TID 82, localhost, executor driver, partition 2, ANY, 7638 bytes)
2021-12-07 14:38:56,325 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 6.0 (TID 83, localhost, executor driver, partition 3, ANY, 7638 bytes)
2021-12-07 14:38:56,325 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 4.0 in stage 6.0 (TID 84, localhost, executor driver, partition 4, ANY, 7638 bytes)
2021-12-07 14:38:56,325 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 5.0 in stage 6.0 (TID 85, localhost, executor driver, partition 5, ANY, 7638 bytes)
2021-12-07 14:38:56,325 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 6.0 in stage 6.0 (TID 86, localhost, executor driver, partition 6, ANY, 7638 bytes)
2021-12-07 14:38:56,325 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 7.0 in stage 6.0 (TID 87, localhost, executor driver, partition 7, ANY, 7638 bytes)
2021-12-07 14:38:56,325 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 8.0 in stage 6.0 (TID 88, localhost, executor driver, partition 8, ANY, 7638 bytes)
2021-12-07 14:38:56,325 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 9.0 in stage 6.0 (TID 89, localhost, executor driver, partition 9, ANY, 7638 bytes)
2021-12-07 14:38:56,325 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 10.0 in stage 6.0 (TID 90, localhost, executor driver, partition 10, ANY, 7638 bytes)
2021-12-07 14:38:56,325 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 11.0 in stage 6.0 (TID 91, localhost, executor driver, partition 11, ANY, 7638 bytes)
2021-12-07 14:38:56,325 [Executor task launch worker for task 80] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 6.0 (TID 80)
2021-12-07 14:38:56,325 [Executor task launch worker for task 82] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 6.0 (TID 82)
2021-12-07 14:38:56,325 [Executor task launch worker for task 87] INFO [org.apache.spark.executor.Executor] - Running task 7.0 in stage 6.0 (TID 87)
2021-12-07 14:38:56,325 [Executor task launch worker for task 83] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 6.0 (TID 83)
2021-12-07 14:38:56,325 [Executor task launch worker for task 85] INFO [org.apache.spark.executor.Executor] - Running task 5.0 in stage 6.0 (TID 85)
2021-12-07 14:38:56,325 [Executor task launch worker for task 86] INFO [org.apache.spark.executor.Executor] - Running task 6.0 in stage 6.0 (TID 86)
2021-12-07 14:38:56,325 [Executor task launch worker for task 84] INFO [org.apache.spark.executor.Executor] - Running task 4.0 in stage 6.0 (TID 84)
2021-12-07 14:38:56,325 [Executor task launch worker for task 81] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 6.0 (TID 81)
2021-12-07 14:38:56,325 [Executor task launch worker for task 90] INFO [org.apache.spark.executor.Executor] - Running task 10.0 in stage 6.0 (TID 90)
2021-12-07 14:38:56,325 [Executor task launch worker for task 88] INFO [org.apache.spark.executor.Executor] - Running task 8.0 in stage 6.0 (TID 88)
2021-12-07 14:38:56,325 [Executor task launch worker for task 89] INFO [org.apache.spark.executor.Executor] - Running task 9.0 in stage 6.0 (TID 89)
2021-12-07 14:38:56,325 [Executor task launch worker for task 91] INFO [org.apache.spark.executor.Executor] - Running task 11.0 in stage 6.0 (TID 91)
2021-12-07 14:38:56,334 [Executor task launch worker for task 85] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,334 [Executor task launch worker for task 80] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,334 [Executor task launch worker for task 87] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,334 [Executor task launch worker for task 80] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,334 [Executor task launch worker for task 87] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,334 [Executor task launch worker for task 85] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:56,334 [Executor task launch worker for task 84] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,334 [Executor task launch worker for task 84] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,334 [Executor task launch worker for task 86] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,334 [Executor task launch worker for task 90] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,334 [Executor task launch worker for task 83] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,334 [Executor task launch worker for task 90] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,334 [Executor task launch worker for task 86] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,334 [Executor task launch worker for task 81] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,334 [Executor task launch worker for task 88] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,334 [Executor task launch worker for task 88] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,334 [Executor task launch worker for task 83] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,334 [Executor task launch worker for task 91] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,334 [Executor task launch worker for task 82] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,334 [Executor task launch worker for task 81] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,334 [Executor task launch worker for task 82] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,334 [Executor task launch worker for task 91] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,334 [Executor task launch worker for task 89] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,334 [Executor task launch worker for task 89] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,811 [Executor task launch worker for task 84] INFO [org.apache.spark.executor.Executor] - Finished task 4.0 in stage 6.0 (TID 84). 1265 bytes result sent to driver
2021-12-07 14:38:56,811 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 12.0 in stage 6.0 (TID 92, localhost, executor driver, partition 12, ANY, 7638 bytes)
2021-12-07 14:38:56,812 [Executor task launch worker for task 92] INFO [org.apache.spark.executor.Executor] - Running task 12.0 in stage 6.0 (TID 92)
2021-12-07 14:38:56,812 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 4.0 in stage 6.0 (TID 84) in 487 ms on localhost (executor driver) (1/20)
2021-12-07 14:38:56,815 [Executor task launch worker for task 92] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,815 [Executor task launch worker for task 92] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,826 [Executor task launch worker for task 83] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 6.0 (TID 83). 1265 bytes result sent to driver
2021-12-07 14:38:56,826 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 13.0 in stage 6.0 (TID 93, localhost, executor driver, partition 13, ANY, 7638 bytes)
2021-12-07 14:38:56,828 [Executor task launch worker for task 93] INFO [org.apache.spark.executor.Executor] - Running task 13.0 in stage 6.0 (TID 93)
2021-12-07 14:38:56,828 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 6.0 (TID 83) in 503 ms on localhost (executor driver) (2/20)
2021-12-07 14:38:56,832 [Executor task launch worker for task 93] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,832 [Executor task launch worker for task 93] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,835 [Executor task launch worker for task 88] INFO [org.apache.spark.executor.Executor] - Finished task 8.0 in stage 6.0 (TID 88). 1265 bytes result sent to driver
2021-12-07 14:38:56,836 [dispatcher-event-loop-4] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 14.0 in stage 6.0 (TID 94, localhost, executor driver, partition 14, ANY, 7638 bytes)
2021-12-07 14:38:56,837 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 8.0 in stage 6.0 (TID 88) in 512 ms on localhost (executor driver) (3/20)
2021-12-07 14:38:56,837 [Executor task launch worker for task 89] INFO [org.apache.spark.executor.Executor] - Finished task 9.0 in stage 6.0 (TID 89). 1265 bytes result sent to driver
2021-12-07 14:38:56,838 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 15.0 in stage 6.0 (TID 95, localhost, executor driver, partition 15, ANY, 7638 bytes)
2021-12-07 14:38:56,838 [Executor task launch worker for task 95] INFO [org.apache.spark.executor.Executor] - Running task 15.0 in stage 6.0 (TID 95)
2021-12-07 14:38:56,838 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 9.0 in stage 6.0 (TID 89) in 513 ms on localhost (executor driver) (4/20)
2021-12-07 14:38:56,840 [Executor task launch worker for task 85] INFO [org.apache.spark.executor.Executor] - Finished task 5.0 in stage 6.0 (TID 85). 1265 bytes result sent to driver
2021-12-07 14:38:56,841 [dispatcher-event-loop-10] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 16.0 in stage 6.0 (TID 96, localhost, executor driver, partition 16, ANY, 7638 bytes)
2021-12-07 14:38:56,841 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 5.0 in stage 6.0 (TID 85) in 516 ms on localhost (executor driver) (5/20)
2021-12-07 14:38:56,841 [Executor task launch worker for task 96] INFO [org.apache.spark.executor.Executor] - Running task 16.0 in stage 6.0 (TID 96)
2021-12-07 14:38:56,842 [Executor task launch worker for task 95] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,842 [Executor task launch worker for task 95] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,842 [Executor task launch worker for task 94] INFO [org.apache.spark.executor.Executor] - Running task 14.0 in stage 6.0 (TID 94)
2021-12-07 14:38:56,845 [Executor task launch worker for task 87] INFO [org.apache.spark.executor.Executor] - Finished task 7.0 in stage 6.0 (TID 87). 1308 bytes result sent to driver
2021-12-07 14:38:56,845 [Executor task launch worker for task 90] INFO [org.apache.spark.executor.Executor] - Finished task 10.0 in stage 6.0 (TID 90). 1265 bytes result sent to driver
2021-12-07 14:38:56,845 [Executor task launch worker for task 96] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,846 [Executor task launch worker for task 96] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:56,846 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 17.0 in stage 6.0 (TID 97, localhost, executor driver, partition 17, ANY, 7638 bytes)
2021-12-07 14:38:56,846 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 18.0 in stage 6.0 (TID 98, localhost, executor driver, partition 18, ANY, 7638 bytes)
2021-12-07 14:38:56,846 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 7.0 in stage 6.0 (TID 87) in 521 ms on localhost (executor driver) (6/20)
2021-12-07 14:38:56,846 [Executor task launch worker for task 97] INFO [org.apache.spark.executor.Executor] - Running task 17.0 in stage 6.0 (TID 97)
2021-12-07 14:38:56,847 [Executor task launch worker for task 94] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,847 [Executor task launch worker for task 94] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:56,846 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 10.0 in stage 6.0 (TID 90) in 521 ms on localhost (executor driver) (7/20)
2021-12-07 14:38:56,847 [Executor task launch worker for task 98] INFO [org.apache.spark.executor.Executor] - Running task 18.0 in stage 6.0 (TID 98)
2021-12-07 14:38:56,847 [Executor task launch worker for task 91] INFO [org.apache.spark.executor.Executor] - Finished task 11.0 in stage 6.0 (TID 91). 1265 bytes result sent to driver
2021-12-07 14:38:56,848 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 19.0 in stage 6.0 (TID 99, localhost, executor driver, partition 19, ANY, 7638 bytes)
2021-12-07 14:38:56,848 [Executor task launch worker for task 99] INFO [org.apache.spark.executor.Executor] - Running task 19.0 in stage 6.0 (TID 99)
2021-12-07 14:38:56,849 [Executor task launch worker for task 80] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 6.0 (TID 80). 1265 bytes result sent to driver
2021-12-07 14:38:56,850 [Executor task launch worker for task 97] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,850 [Executor task launch worker for task 97] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,851 [Executor task launch worker for task 99] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,851 [Executor task launch worker for task 99] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,852 [Executor task launch worker for task 98] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:56,852 [Executor task launch worker for task 98] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:56,854 [Executor task launch worker for task 86] INFO [org.apache.spark.executor.Executor] - Finished task 6.0 in stage 6.0 (TID 86). 1265 bytes result sent to driver
2021-12-07 14:38:56,857 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 11.0 in stage 6.0 (TID 91) in 532 ms on localhost (executor driver) (8/20)
2021-12-07 14:38:56,857 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 6.0 in stage 6.0 (TID 86) in 532 ms on localhost (executor driver) (9/20)
2021-12-07 14:38:56,857 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 6.0 (TID 80) in 533 ms on localhost (executor driver) (10/20)
2021-12-07 14:38:56,865 [Executor task launch worker for task 81] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 6.0 (TID 81). 1265 bytes result sent to driver
2021-12-07 14:38:56,865 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 6.0 (TID 81) in 541 ms on localhost (executor driver) (11/20)
2021-12-07 14:38:56,865 [Executor task launch worker for task 82] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 6.0 (TID 82). 1265 bytes result sent to driver
2021-12-07 14:38:56,866 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 6.0 (TID 82) in 542 ms on localhost (executor driver) (12/20)
2021-12-07 14:38:57,148 [Executor task launch worker for task 93] INFO [org.apache.spark.executor.Executor] - Finished task 13.0 in stage 6.0 (TID 93). 1265 bytes result sent to driver
2021-12-07 14:38:57,149 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 13.0 in stage 6.0 (TID 93) in 323 ms on localhost (executor driver) (13/20)
2021-12-07 14:38:57,159 [Executor task launch worker for task 92] INFO [org.apache.spark.executor.Executor] - Finished task 12.0 in stage 6.0 (TID 92). 1265 bytes result sent to driver
2021-12-07 14:38:57,159 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 12.0 in stage 6.0 (TID 92) in 348 ms on localhost (executor driver) (14/20)
2021-12-07 14:38:57,161 [Executor task launch worker for task 94] INFO [org.apache.spark.executor.Executor] - Finished task 14.0 in stage 6.0 (TID 94). 1265 bytes result sent to driver
2021-12-07 14:38:57,162 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 14.0 in stage 6.0 (TID 94) in 326 ms on localhost (executor driver) (15/20)
2021-12-07 14:38:57,162 [Executor task launch worker for task 95] INFO [org.apache.spark.executor.Executor] - Finished task 15.0 in stage 6.0 (TID 95). 1308 bytes result sent to driver
2021-12-07 14:38:57,162 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 15.0 in stage 6.0 (TID 95) in 324 ms on localhost (executor driver) (16/20)
2021-12-07 14:38:57,170 [Executor task launch worker for task 97] INFO [org.apache.spark.executor.Executor] - Finished task 17.0 in stage 6.0 (TID 97). 1265 bytes result sent to driver
2021-12-07 14:38:57,170 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 17.0 in stage 6.0 (TID 97) in 325 ms on localhost (executor driver) (17/20)
2021-12-07 14:38:57,171 [Executor task launch worker for task 98] INFO [org.apache.spark.executor.Executor] - Finished task 18.0 in stage 6.0 (TID 98). 1265 bytes result sent to driver
2021-12-07 14:38:57,172 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 18.0 in stage 6.0 (TID 98) in 326 ms on localhost (executor driver) (18/20)
2021-12-07 14:38:57,174 [Executor task launch worker for task 96] INFO [org.apache.spark.executor.Executor] - Finished task 16.0 in stage 6.0 (TID 96). 1265 bytes result sent to driver
2021-12-07 14:38:57,175 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 16.0 in stage 6.0 (TID 96) in 334 ms on localhost (executor driver) (19/20)
2021-12-07 14:38:57,182 [Executor task launch worker for task 99] INFO [org.apache.spark.executor.Executor] - Finished task 19.0 in stage 6.0 (TID 99). 1265 bytes result sent to driver
2021-12-07 14:38:57,182 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 19.0 in stage 6.0 (TID 99) in 334 ms on localhost (executor driver) (20/20)
2021-12-07 14:38:57,182 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2021-12-07 14:38:57,183 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 6 (map at PaidPromotionAdjustParameter.scala:65) finished in 0.868 s
2021-12-07 14:38:57,183 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 14:38:57,183 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 14:38:57,183 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 7)
2021-12-07 14:38:57,183 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 14:38:57,183 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 7 (ShuffledRDD[8] at sortByKey at PaidPromotionAdjustParameter.scala:66), which has no missing parents
2021-12-07 14:38:57,185 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6 stored as values in memory (estimated size 3.4 KB, free 1990.4 MB)
2021-12-07 14:38:57,186 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.0 KB, free 1990.4 MB)
2021-12-07 14:38:57,187 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_6_piece0 in memory on qb:60003 (size: 2.0 KB, free: 1990.8 MB)
2021-12-07 14:38:57,187 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 6 from broadcast at DAGScheduler.scala:1039
2021-12-07 14:38:57,187 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 19 missing tasks from ResultStage 7 (ShuffledRDD[8] at sortByKey at PaidPromotionAdjustParameter.scala:66) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2021-12-07 14:38:57,187 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 7.0 with 19 tasks
2021-12-07 14:38:57,188 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 7.0 (TID 100, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 14:38:57,188 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 7.0 (TID 101, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 14:38:57,188 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 7.0 (TID 102, localhost, executor driver, partition 2, ANY, 7649 bytes)
2021-12-07 14:38:57,188 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 7.0 (TID 103, localhost, executor driver, partition 3, ANY, 7649 bytes)
2021-12-07 14:38:57,188 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 4.0 in stage 7.0 (TID 104, localhost, executor driver, partition 4, ANY, 7649 bytes)
2021-12-07 14:38:57,188 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 5.0 in stage 7.0 (TID 105, localhost, executor driver, partition 5, ANY, 7649 bytes)
2021-12-07 14:38:57,188 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 6.0 in stage 7.0 (TID 106, localhost, executor driver, partition 6, ANY, 7649 bytes)
2021-12-07 14:38:57,188 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 7.0 in stage 7.0 (TID 107, localhost, executor driver, partition 7, ANY, 7649 bytes)
2021-12-07 14:38:57,188 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 8.0 in stage 7.0 (TID 108, localhost, executor driver, partition 8, ANY, 7649 bytes)
2021-12-07 14:38:57,188 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 9.0 in stage 7.0 (TID 109, localhost, executor driver, partition 9, ANY, 7649 bytes)
2021-12-07 14:38:57,189 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 10.0 in stage 7.0 (TID 110, localhost, executor driver, partition 10, ANY, 7649 bytes)
2021-12-07 14:38:57,189 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 11.0 in stage 7.0 (TID 111, localhost, executor driver, partition 11, ANY, 7649 bytes)
2021-12-07 14:38:57,189 [Executor task launch worker for task 100] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 7.0 (TID 100)
2021-12-07 14:38:57,189 [Executor task launch worker for task 105] INFO [org.apache.spark.executor.Executor] - Running task 5.0 in stage 7.0 (TID 105)
2021-12-07 14:38:57,189 [Executor task launch worker for task 108] INFO [org.apache.spark.executor.Executor] - Running task 8.0 in stage 7.0 (TID 108)
2021-12-07 14:38:57,189 [Executor task launch worker for task 106] INFO [org.apache.spark.executor.Executor] - Running task 6.0 in stage 7.0 (TID 106)
2021-12-07 14:38:57,189 [Executor task launch worker for task 107] INFO [org.apache.spark.executor.Executor] - Running task 7.0 in stage 7.0 (TID 107)
2021-12-07 14:38:57,189 [Executor task launch worker for task 102] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 7.0 (TID 102)
2021-12-07 14:38:57,189 [Executor task launch worker for task 103] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 7.0 (TID 103)
2021-12-07 14:38:57,189 [Executor task launch worker for task 101] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 7.0 (TID 101)
2021-12-07 14:38:57,189 [Executor task launch worker for task 104] INFO [org.apache.spark.executor.Executor] - Running task 4.0 in stage 7.0 (TID 104)
2021-12-07 14:38:57,189 [Executor task launch worker for task 110] INFO [org.apache.spark.executor.Executor] - Running task 10.0 in stage 7.0 (TID 110)
2021-12-07 14:38:57,189 [Executor task launch worker for task 109] INFO [org.apache.spark.executor.Executor] - Running task 9.0 in stage 7.0 (TID 109)
2021-12-07 14:38:57,189 [Executor task launch worker for task 111] INFO [org.apache.spark.executor.Executor] - Running task 11.0 in stage 7.0 (TID 111)
2021-12-07 14:38:57,194 [Executor task launch worker for task 104] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,194 [Executor task launch worker for task 104] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,194 [Executor task launch worker for task 107] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,194 [Executor task launch worker for task 100] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,194 [Executor task launch worker for task 107] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,194 [Executor task launch worker for task 103] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,194 [Executor task launch worker for task 103] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,194 [Executor task launch worker for task 110] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,194 [Executor task launch worker for task 106] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,194 [Executor task launch worker for task 110] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,194 [Executor task launch worker for task 108] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,194 [Executor task launch worker for task 111] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,194 [Executor task launch worker for task 100] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,195 [Executor task launch worker for task 111] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:57,194 [Executor task launch worker for task 108] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,194 [Executor task launch worker for task 109] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,195 [Executor task launch worker for task 109] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:57,194 [Executor task launch worker for task 102] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,194 [Executor task launch worker for task 105] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,195 [Executor task launch worker for task 105] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:57,194 [Executor task launch worker for task 106] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,195 [Executor task launch worker for task 102] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:57,196 [Executor task launch worker for task 101] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,196 [Executor task launch worker for task 101] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,471 [Executor task launch worker for task 110] INFO [org.apache.spark.executor.Executor] - Finished task 10.0 in stage 7.0 (TID 110). 1098 bytes result sent to driver
2021-12-07 14:38:57,480 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 12.0 in stage 7.0 (TID 112, localhost, executor driver, partition 12, ANY, 7649 bytes)
2021-12-07 14:38:57,480 [Executor task launch worker for task 112] INFO [org.apache.spark.executor.Executor] - Running task 12.0 in stage 7.0 (TID 112)
2021-12-07 14:38:57,482 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 10.0 in stage 7.0 (TID 110) in 293 ms on localhost (executor driver) (1/19)
2021-12-07 14:38:57,483 [Executor task launch worker for task 112] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,483 [Executor task launch worker for task 112] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,491 [Executor task launch worker for task 108] INFO [org.apache.spark.executor.Executor] - Finished task 8.0 in stage 7.0 (TID 108). 1098 bytes result sent to driver
2021-12-07 14:38:57,491 [Executor task launch worker for task 104] INFO [org.apache.spark.executor.Executor] - Finished task 4.0 in stage 7.0 (TID 104). 1098 bytes result sent to driver
2021-12-07 14:38:57,491 [Executor task launch worker for task 109] INFO [org.apache.spark.executor.Executor] - Finished task 9.0 in stage 7.0 (TID 109). 1098 bytes result sent to driver
2021-12-07 14:38:57,492 [Executor task launch worker for task 107] INFO [org.apache.spark.executor.Executor] - Finished task 7.0 in stage 7.0 (TID 107). 1098 bytes result sent to driver
2021-12-07 14:38:57,492 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 13.0 in stage 7.0 (TID 113, localhost, executor driver, partition 13, ANY, 7649 bytes)
2021-12-07 14:38:57,492 [Executor task launch worker for task 111] INFO [org.apache.spark.executor.Executor] - Finished task 11.0 in stage 7.0 (TID 111). 1098 bytes result sent to driver
2021-12-07 14:38:57,492 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 8.0 in stage 7.0 (TID 108) in 304 ms on localhost (executor driver) (2/19)
2021-12-07 14:38:57,492 [Executor task launch worker for task 113] INFO [org.apache.spark.executor.Executor] - Running task 13.0 in stage 7.0 (TID 113)
2021-12-07 14:38:57,492 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 14.0 in stage 7.0 (TID 114, localhost, executor driver, partition 14, ANY, 7649 bytes)
2021-12-07 14:38:57,493 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 15.0 in stage 7.0 (TID 115, localhost, executor driver, partition 15, ANY, 7649 bytes)
2021-12-07 14:38:57,493 [Executor task launch worker for task 114] INFO [org.apache.spark.executor.Executor] - Running task 14.0 in stage 7.0 (TID 114)
2021-12-07 14:38:57,493 [Executor task launch worker for task 101] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 7.0 (TID 101). 1098 bytes result sent to driver
2021-12-07 14:38:57,493 [Executor task launch worker for task 115] INFO [org.apache.spark.executor.Executor] - Running task 15.0 in stage 7.0 (TID 115)
2021-12-07 14:38:57,493 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 16.0 in stage 7.0 (TID 116, localhost, executor driver, partition 16, ANY, 7649 bytes)
2021-12-07 14:38:57,493 [Executor task launch worker for task 103] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 7.0 (TID 103). 1098 bytes result sent to driver
2021-12-07 14:38:57,493 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 17.0 in stage 7.0 (TID 117, localhost, executor driver, partition 17, ANY, 7649 bytes)
2021-12-07 14:38:57,493 [Executor task launch worker for task 102] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 7.0 (TID 102). 1098 bytes result sent to driver
2021-12-07 14:38:57,493 [Executor task launch worker for task 117] INFO [org.apache.spark.executor.Executor] - Running task 17.0 in stage 7.0 (TID 117)
2021-12-07 14:38:57,493 [Executor task launch worker for task 116] INFO [org.apache.spark.executor.Executor] - Running task 16.0 in stage 7.0 (TID 116)
2021-12-07 14:38:57,494 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 18.0 in stage 7.0 (TID 118, localhost, executor driver, partition 18, ANY, 7649 bytes)
2021-12-07 14:38:57,494 [Executor task launch worker for task 106] INFO [org.apache.spark.executor.Executor] - Finished task 6.0 in stage 7.0 (TID 106). 1098 bytes result sent to driver
2021-12-07 14:38:57,494 [Executor task launch worker for task 118] INFO [org.apache.spark.executor.Executor] - Running task 18.0 in stage 7.0 (TID 118)
2021-12-07 14:38:57,494 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 11.0 in stage 7.0 (TID 111) in 305 ms on localhost (executor driver) (3/19)
2021-12-07 14:38:57,494 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 7.0 (TID 101) in 306 ms on localhost (executor driver) (4/19)
2021-12-07 14:38:57,494 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 7.0 (TID 103) in 306 ms on localhost (executor driver) (5/19)
2021-12-07 14:38:57,494 [Executor task launch worker for task 105] INFO [org.apache.spark.executor.Executor] - Finished task 5.0 in stage 7.0 (TID 105). 1098 bytes result sent to driver
2021-12-07 14:38:57,494 [Executor task launch worker for task 100] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 7.0 (TID 100). 1098 bytes result sent to driver
2021-12-07 14:38:57,495 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 9.0 in stage 7.0 (TID 109) in 307 ms on localhost (executor driver) (6/19)
2021-12-07 14:38:57,495 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 4.0 in stage 7.0 (TID 104) in 307 ms on localhost (executor driver) (7/19)
2021-12-07 14:38:57,495 [Executor task launch worker for task 113] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,495 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 7.0 (TID 102) in 307 ms on localhost (executor driver) (8/19)
2021-12-07 14:38:57,495 [Executor task launch worker for task 113] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,495 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 7.0 in stage 7.0 (TID 107) in 307 ms on localhost (executor driver) (9/19)
2021-12-07 14:38:57,496 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 6.0 in stage 7.0 (TID 106) in 308 ms on localhost (executor driver) (10/19)
2021-12-07 14:38:57,496 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 5.0 in stage 7.0 (TID 105) in 308 ms on localhost (executor driver) (11/19)
2021-12-07 14:38:57,496 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 7.0 (TID 100) in 308 ms on localhost (executor driver) (12/19)
2021-12-07 14:38:57,496 [Executor task launch worker for task 114] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,496 [Executor task launch worker for task 116] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,497 [Executor task launch worker for task 114] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:57,497 [Executor task launch worker for task 116] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:57,497 [Executor task launch worker for task 118] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,497 [Executor task launch worker for task 118] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,497 [Executor task launch worker for task 117] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,497 [Executor task launch worker for task 117] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,498 [Executor task launch worker for task 115] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,498 [Executor task launch worker for task 115] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,540 [Executor task launch worker for task 112] INFO [org.apache.spark.executor.Executor] - Finished task 12.0 in stage 7.0 (TID 112). 1141 bytes result sent to driver
2021-12-07 14:38:57,541 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 12.0 in stage 7.0 (TID 112) in 61 ms on localhost (executor driver) (13/19)
2021-12-07 14:38:57,549 [Executor task launch worker for task 114] INFO [org.apache.spark.executor.Executor] - Finished task 14.0 in stage 7.0 (TID 114). 1098 bytes result sent to driver
2021-12-07 14:38:57,549 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 14.0 in stage 7.0 (TID 114) in 57 ms on localhost (executor driver) (14/19)
2021-12-07 14:38:57,554 [Executor task launch worker for task 118] INFO [org.apache.spark.executor.Executor] - Finished task 18.0 in stage 7.0 (TID 118). 1141 bytes result sent to driver
2021-12-07 14:38:57,554 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 18.0 in stage 7.0 (TID 118) in 61 ms on localhost (executor driver) (15/19)
2021-12-07 14:38:57,558 [Executor task launch worker for task 116] INFO [org.apache.spark.executor.Executor] - Finished task 16.0 in stage 7.0 (TID 116). 1141 bytes result sent to driver
2021-12-07 14:38:57,554 [Executor task launch worker for task 117] INFO [org.apache.spark.executor.Executor] - Finished task 17.0 in stage 7.0 (TID 117). 1141 bytes result sent to driver
2021-12-07 14:38:57,561 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 16.0 in stage 7.0 (TID 116) in 68 ms on localhost (executor driver) (16/19)
2021-12-07 14:38:57,561 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 17.0 in stage 7.0 (TID 117) in 68 ms on localhost (executor driver) (17/19)
2021-12-07 14:38:57,561 [Executor task launch worker for task 115] INFO [org.apache.spark.executor.Executor] - Finished task 15.0 in stage 7.0 (TID 115). 1098 bytes result sent to driver
2021-12-07 14:38:57,561 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 15.0 in stage 7.0 (TID 115) in 68 ms on localhost (executor driver) (18/19)
2021-12-07 14:38:57,570 [Executor task launch worker for task 113] INFO [org.apache.spark.executor.Executor] - Finished task 13.0 in stage 7.0 (TID 113). 1098 bytes result sent to driver
2021-12-07 14:38:57,570 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 13.0 in stage 7.0 (TID 113) in 78 ms on localhost (executor driver) (19/19)
2021-12-07 14:38:57,570 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2021-12-07 14:38:57,570 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 7 (zipWithIndex at PaidPromotionAdjustParameter.scala:67) finished in 0.386 s
2021-12-07 14:38:57,570 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: zipWithIndex at PaidPromotionAdjustParameter.scala:67, took 1.256810 s
2021-12-07 14:38:57,583 [main] INFO [org.apache.spark.SparkContext] - Starting job: takeSample at PaidPromotionAdjustParameter.scala:70
2021-12-07 14:38:57,584 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 4 (takeSample at PaidPromotionAdjustParameter.scala:70) with 20 output partitions
2021-12-07 14:38:57,584 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 10 (takeSample at PaidPromotionAdjustParameter.scala:70)
2021-12-07 14:38:57,584 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 9)
2021-12-07 14:38:57,584 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 14:38:57,584 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 10 (MapPartitionsRDD[11] at map at PaidPromotionAdjustParameter.scala:69), which has no missing parents
2021-12-07 14:38:57,588 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7 stored as values in memory (estimated size 4.2 KB, free 1990.4 MB)
2021-12-07 14:38:57,590 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1990.4 MB)
2021-12-07 14:38:57,590 [dispatcher-event-loop-2] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_7_piece0 in memory on qb:60003 (size: 2.4 KB, free: 1990.8 MB)
2021-12-07 14:38:57,590 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 7 from broadcast at DAGScheduler.scala:1039
2021-12-07 14:38:57,591 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 20 missing tasks from ResultStage 10 (MapPartitionsRDD[11] at map at PaidPromotionAdjustParameter.scala:69) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2021-12-07 14:38:57,591 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 10.0 with 20 tasks
2021-12-07 14:38:57,591 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 10.0 (TID 119, localhost, executor driver, partition 0, ANY, 7759 bytes)
2021-12-07 14:38:57,592 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 10.0 (TID 120, localhost, executor driver, partition 1, ANY, 7759 bytes)
2021-12-07 14:38:57,592 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 10.0 (TID 121, localhost, executor driver, partition 2, ANY, 7759 bytes)
2021-12-07 14:38:57,592 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 10.0 (TID 122, localhost, executor driver, partition 3, ANY, 7759 bytes)
2021-12-07 14:38:57,592 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 4.0 in stage 10.0 (TID 123, localhost, executor driver, partition 4, ANY, 7759 bytes)
2021-12-07 14:38:57,592 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 5.0 in stage 10.0 (TID 124, localhost, executor driver, partition 5, ANY, 7759 bytes)
2021-12-07 14:38:57,592 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 6.0 in stage 10.0 (TID 125, localhost, executor driver, partition 6, ANY, 7759 bytes)
2021-12-07 14:38:57,592 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 7.0 in stage 10.0 (TID 126, localhost, executor driver, partition 7, ANY, 7759 bytes)
2021-12-07 14:38:57,592 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 8.0 in stage 10.0 (TID 127, localhost, executor driver, partition 8, ANY, 7759 bytes)
2021-12-07 14:38:57,592 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 9.0 in stage 10.0 (TID 128, localhost, executor driver, partition 9, ANY, 7759 bytes)
2021-12-07 14:38:57,592 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 10.0 in stage 10.0 (TID 129, localhost, executor driver, partition 10, ANY, 7759 bytes)
2021-12-07 14:38:57,592 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 11.0 in stage 10.0 (TID 130, localhost, executor driver, partition 11, ANY, 7759 bytes)
2021-12-07 14:38:57,592 [Executor task launch worker for task 119] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 10.0 (TID 119)
2021-12-07 14:38:57,592 [Executor task launch worker for task 124] INFO [org.apache.spark.executor.Executor] - Running task 5.0 in stage 10.0 (TID 124)
2021-12-07 14:38:57,592 [Executor task launch worker for task 123] INFO [org.apache.spark.executor.Executor] - Running task 4.0 in stage 10.0 (TID 123)
2021-12-07 14:38:57,592 [Executor task launch worker for task 128] INFO [org.apache.spark.executor.Executor] - Running task 9.0 in stage 10.0 (TID 128)
2021-12-07 14:38:57,592 [Executor task launch worker for task 120] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 10.0 (TID 120)
2021-12-07 14:38:57,592 [Executor task launch worker for task 122] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 10.0 (TID 122)
2021-12-07 14:38:57,592 [Executor task launch worker for task 121] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 10.0 (TID 121)
2021-12-07 14:38:57,592 [Executor task launch worker for task 130] INFO [org.apache.spark.executor.Executor] - Running task 11.0 in stage 10.0 (TID 130)
2021-12-07 14:38:57,592 [Executor task launch worker for task 129] INFO [org.apache.spark.executor.Executor] - Running task 10.0 in stage 10.0 (TID 129)
2021-12-07 14:38:57,592 [Executor task launch worker for task 127] INFO [org.apache.spark.executor.Executor] - Running task 8.0 in stage 10.0 (TID 127)
2021-12-07 14:38:57,592 [Executor task launch worker for task 126] INFO [org.apache.spark.executor.Executor] - Running task 7.0 in stage 10.0 (TID 126)
2021-12-07 14:38:57,592 [Executor task launch worker for task 125] INFO [org.apache.spark.executor.Executor] - Running task 6.0 in stage 10.0 (TID 125)
2021-12-07 14:38:57,595 [Executor task launch worker for task 120] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,595 [Executor task launch worker for task 120] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,595 [Executor task launch worker for task 123] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,595 [Executor task launch worker for task 123] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,596 [Executor task launch worker for task 122] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,596 [Executor task launch worker for task 122] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:57,596 [Executor task launch worker for task 121] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,596 [Executor task launch worker for task 121] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,596 [Executor task launch worker for task 127] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,596 [Executor task launch worker for task 127] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,597 [Executor task launch worker for task 129] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,597 [Executor task launch worker for task 129] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:57,597 [Executor task launch worker for task 124] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,597 [Executor task launch worker for task 124] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,597 [Executor task launch worker for task 125] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,597 [Executor task launch worker for task 125] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,597 [Executor task launch worker for task 126] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,598 [Executor task launch worker for task 126] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:57,598 [Executor task launch worker for task 119] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,598 [Executor task launch worker for task 119] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,598 [Executor task launch worker for task 128] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,598 [Executor task launch worker for task 128] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,598 [Executor task launch worker for task 130] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,598 [Executor task launch worker for task 130] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,701 [Executor task launch worker for task 120] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 10.0 (TID 120). 1097 bytes result sent to driver
2021-12-07 14:38:57,702 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 12.0 in stage 10.0 (TID 131, localhost, executor driver, partition 12, ANY, 7759 bytes)
2021-12-07 14:38:57,702 [Executor task launch worker for task 131] INFO [org.apache.spark.executor.Executor] - Running task 12.0 in stage 10.0 (TID 131)
2021-12-07 14:38:57,702 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 10.0 (TID 120) in 110 ms on localhost (executor driver) (1/20)
2021-12-07 14:38:57,705 [Executor task launch worker for task 131] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,706 [Executor task launch worker for task 131] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:57,706 [Executor task launch worker for task 126] INFO [org.apache.spark.executor.Executor] - Finished task 7.0 in stage 10.0 (TID 126). 1098 bytes result sent to driver
2021-12-07 14:38:57,707 [Executor task launch worker for task 129] INFO [org.apache.spark.executor.Executor] - Finished task 10.0 in stage 10.0 (TID 129). 1098 bytes result sent to driver
2021-12-07 14:38:57,708 [Executor task launch worker for task 123] INFO [org.apache.spark.executor.Executor] - Finished task 4.0 in stage 10.0 (TID 123). 1098 bytes result sent to driver
2021-12-07 14:38:57,706 [Executor task launch worker for task 121] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 10.0 (TID 121). 1098 bytes result sent to driver
2021-12-07 14:38:57,708 [Executor task launch worker for task 122] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 10.0 (TID 122). 1098 bytes result sent to driver
2021-12-07 14:38:57,708 [Executor task launch worker for task 127] INFO [org.apache.spark.executor.Executor] - Finished task 8.0 in stage 10.0 (TID 127). 1098 bytes result sent to driver
2021-12-07 14:38:57,709 [Executor task launch worker for task 130] INFO [org.apache.spark.executor.Executor] - Finished task 11.0 in stage 10.0 (TID 130). 1098 bytes result sent to driver
2021-12-07 14:38:57,710 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 13.0 in stage 10.0 (TID 132, localhost, executor driver, partition 13, ANY, 7759 bytes)
2021-12-07 14:38:57,710 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 7.0 in stage 10.0 (TID 126) in 118 ms on localhost (executor driver) (2/20)
2021-12-07 14:38:57,710 [Executor task launch worker for task 132] INFO [org.apache.spark.executor.Executor] - Running task 13.0 in stage 10.0 (TID 132)
2021-12-07 14:38:57,710 [Executor task launch worker for task 124] INFO [org.apache.spark.executor.Executor] - Finished task 5.0 in stage 10.0 (TID 124). 1098 bytes result sent to driver
2021-12-07 14:38:57,710 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 14.0 in stage 10.0 (TID 133, localhost, executor driver, partition 14, ANY, 7759 bytes)
2021-12-07 14:38:57,710 [Executor task launch worker for task 133] INFO [org.apache.spark.executor.Executor] - Running task 14.0 in stage 10.0 (TID 133)
2021-12-07 14:38:57,710 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 10.0 in stage 10.0 (TID 129) in 118 ms on localhost (executor driver) (3/20)
2021-12-07 14:38:57,711 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 15.0 in stage 10.0 (TID 134, localhost, executor driver, partition 15, ANY, 7759 bytes)
2021-12-07 14:38:57,711 [Executor task launch worker for task 134] INFO [org.apache.spark.executor.Executor] - Running task 15.0 in stage 10.0 (TID 134)
2021-12-07 14:38:57,711 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 16.0 in stage 10.0 (TID 135, localhost, executor driver, partition 16, ANY, 7759 bytes)
2021-12-07 14:38:57,711 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 10.0 (TID 122) in 119 ms on localhost (executor driver) (4/20)
2021-12-07 14:38:57,711 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 4.0 in stage 10.0 (TID 123) in 119 ms on localhost (executor driver) (5/20)
2021-12-07 14:38:57,711 [Executor task launch worker for task 135] INFO [org.apache.spark.executor.Executor] - Running task 16.0 in stage 10.0 (TID 135)
2021-12-07 14:38:57,711 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 17.0 in stage 10.0 (TID 136, localhost, executor driver, partition 17, ANY, 7759 bytes)
2021-12-07 14:38:57,711 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 8.0 in stage 10.0 (TID 127) in 119 ms on localhost (executor driver) (6/20)
2021-12-07 14:38:57,712 [Executor task launch worker for task 136] INFO [org.apache.spark.executor.Executor] - Running task 17.0 in stage 10.0 (TID 136)
2021-12-07 14:38:57,712 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 18.0 in stage 10.0 (TID 137, localhost, executor driver, partition 18, ANY, 7759 bytes)
2021-12-07 14:38:57,712 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 10.0 (TID 121) in 120 ms on localhost (executor driver) (7/20)
2021-12-07 14:38:57,712 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 11.0 in stage 10.0 (TID 130) in 120 ms on localhost (executor driver) (8/20)
2021-12-07 14:38:57,712 [Executor task launch worker for task 137] INFO [org.apache.spark.executor.Executor] - Running task 18.0 in stage 10.0 (TID 137)
2021-12-07 14:38:57,712 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 19.0 in stage 10.0 (TID 138, localhost, executor driver, partition 19, ANY, 7759 bytes)
2021-12-07 14:38:57,712 [Executor task launch worker for task 138] INFO [org.apache.spark.executor.Executor] - Running task 19.0 in stage 10.0 (TID 138)
2021-12-07 14:38:57,713 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 5.0 in stage 10.0 (TID 124) in 121 ms on localhost (executor driver) (9/20)
2021-12-07 14:38:57,713 [Executor task launch worker for task 132] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,713 [Executor task launch worker for task 132] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,713 [Executor task launch worker for task 125] INFO [org.apache.spark.executor.Executor] - Finished task 6.0 in stage 10.0 (TID 125). 1098 bytes result sent to driver
2021-12-07 14:38:57,713 [Executor task launch worker for task 135] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,713 [Executor task launch worker for task 135] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,714 [Executor task launch worker for task 133] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,714 [Executor task launch worker for task 133] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,714 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 6.0 in stage 10.0 (TID 125) in 122 ms on localhost (executor driver) (10/20)
2021-12-07 14:38:57,714 [Executor task launch worker for task 136] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,714 [Executor task launch worker for task 136] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,716 [Executor task launch worker for task 134] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,716 [Executor task launch worker for task 134] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,716 [Executor task launch worker for task 137] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,716 [Executor task launch worker for task 137] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,716 [Executor task launch worker for task 138] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,716 [Executor task launch worker for task 138] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,718 [Executor task launch worker for task 128] INFO [org.apache.spark.executor.Executor] - Finished task 9.0 in stage 10.0 (TID 128). 1098 bytes result sent to driver
2021-12-07 14:38:57,718 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 9.0 in stage 10.0 (TID 128) in 126 ms on localhost (executor driver) (11/20)
2021-12-07 14:38:57,722 [Executor task launch worker for task 119] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 10.0 (TID 119). 1096 bytes result sent to driver
2021-12-07 14:38:57,722 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 10.0 (TID 119) in 131 ms on localhost (executor driver) (12/20)
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 61
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 73
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 53
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 107
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 66
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 48
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 59
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 97
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 68
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 51
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 81
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 125
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 67
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 98
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 56
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 44
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 39
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 69
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 29
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 149
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 140
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 135
2021-12-07 14:38:57,797 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 36
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 108
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 129
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 121
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 32
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 122
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 71
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 74
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 132
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 112
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 41
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 134
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 119
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 146
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 136
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 104
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 57
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 58
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 85
2021-12-07 14:38:57,798 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 133
2021-12-07 14:38:57,799 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_4_piece0 on qb:60003 in memory (size: 2.4 KB, free: 1990.8 MB)
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 93
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 55
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 47
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 27
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 114
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 141
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 52
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 38
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 37
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 147
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 91
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 123
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 31
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 30
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 75
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 35
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 78
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 26
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 102
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 124
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 139
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 28
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 50
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 105
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 130
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 25
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 65
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 83
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 120
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 86
2021-12-07 14:38:57,801 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 42
2021-12-07 14:38:57,802 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_5_piece0 on qb:60003 in memory (size: 2.4 KB, free: 1990.8 MB)
2021-12-07 14:38:57,802 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 100
2021-12-07 14:38:57,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 80
2021-12-07 14:38:57,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 90
2021-12-07 14:38:57,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 118
2021-12-07 14:38:57,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 63
2021-12-07 14:38:57,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 60
2021-12-07 14:38:57,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 94
2021-12-07 14:38:57,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 148
2021-12-07 14:38:57,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 144
2021-12-07 14:38:57,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 84
2021-12-07 14:38:57,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 45
2021-12-07 14:38:57,803 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 46
2021-12-07 14:38:57,804 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_3_piece0 on qb:60003 in memory (size: 1907.0 B, free: 1990.8 MB)
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 143
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 34
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 127
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 89
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 79
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 82
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 110
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 111
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 109
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 131
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 142
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 126
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 43
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 87
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 137
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 64
2021-12-07 14:38:57,804 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 72
2021-12-07 14:38:57,805 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_6_piece0 on qb:60003 in memory (size: 2.0 KB, free: 1990.8 MB)
2021-12-07 14:38:57,805 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 76
2021-12-07 14:38:57,805 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 62
2021-12-07 14:38:57,805 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 128
2021-12-07 14:38:57,805 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 40
2021-12-07 14:38:57,805 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 103
2021-12-07 14:38:57,805 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 88
2021-12-07 14:38:57,805 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 33
2021-12-07 14:38:57,805 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 138
2021-12-07 14:38:57,805 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 116
2021-12-07 14:38:57,805 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 70
2021-12-07 14:38:57,805 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 117
2021-12-07 14:38:57,805 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 106
2021-12-07 14:38:57,806 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 96
2021-12-07 14:38:57,806 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 92
2021-12-07 14:38:57,806 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 49
2021-12-07 14:38:57,806 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 101
2021-12-07 14:38:57,806 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 77
2021-12-07 14:38:57,806 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 54
2021-12-07 14:38:57,806 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 95
2021-12-07 14:38:57,806 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 115
2021-12-07 14:38:57,806 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 113
2021-12-07 14:38:57,806 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 99
2021-12-07 14:38:57,806 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 145
2021-12-07 14:38:57,812 [Executor task launch worker for task 133] INFO [org.apache.spark.executor.Executor] - Finished task 14.0 in stage 10.0 (TID 133). 1098 bytes result sent to driver
2021-12-07 14:38:57,813 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 14.0 in stage 10.0 (TID 133) in 103 ms on localhost (executor driver) (13/20)
2021-12-07 14:38:57,813 [Executor task launch worker for task 131] INFO [org.apache.spark.executor.Executor] - Finished task 12.0 in stage 10.0 (TID 131). 1098 bytes result sent to driver
2021-12-07 14:38:57,814 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 12.0 in stage 10.0 (TID 131) in 113 ms on localhost (executor driver) (14/20)
2021-12-07 14:38:57,822 [Executor task launch worker for task 136] INFO [org.apache.spark.executor.Executor] - Finished task 17.0 in stage 10.0 (TID 136). 1096 bytes result sent to driver
2021-12-07 14:38:57,822 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 17.0 in stage 10.0 (TID 136) in 111 ms on localhost (executor driver) (15/20)
2021-12-07 14:38:57,822 [Executor task launch worker for task 137] INFO [org.apache.spark.executor.Executor] - Finished task 18.0 in stage 10.0 (TID 137). 1096 bytes result sent to driver
2021-12-07 14:38:57,823 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 18.0 in stage 10.0 (TID 137) in 111 ms on localhost (executor driver) (16/20)
2021-12-07 14:38:57,824 [Executor task launch worker for task 134] INFO [org.apache.spark.executor.Executor] - Finished task 15.0 in stage 10.0 (TID 134). 1098 bytes result sent to driver
2021-12-07 14:38:57,825 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 15.0 in stage 10.0 (TID 134) in 115 ms on localhost (executor driver) (17/20)
2021-12-07 14:38:57,825 [Executor task launch worker for task 135] INFO [org.apache.spark.executor.Executor] - Finished task 16.0 in stage 10.0 (TID 135). 1098 bytes result sent to driver
2021-12-07 14:38:57,825 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 16.0 in stage 10.0 (TID 135) in 114 ms on localhost (executor driver) (18/20)
2021-12-07 14:38:57,825 [Executor task launch worker for task 132] INFO [org.apache.spark.executor.Executor] - Finished task 13.0 in stage 10.0 (TID 132). 1098 bytes result sent to driver
2021-12-07 14:38:57,825 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 13.0 in stage 10.0 (TID 132) in 116 ms on localhost (executor driver) (19/20)
2021-12-07 14:38:57,826 [Executor task launch worker for task 138] INFO [org.apache.spark.executor.Executor] - Finished task 19.0 in stage 10.0 (TID 138). 1139 bytes result sent to driver
2021-12-07 14:38:57,826 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 19.0 in stage 10.0 (TID 138) in 114 ms on localhost (executor driver) (20/20)
2021-12-07 14:38:57,827 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2021-12-07 14:38:57,827 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 10 (takeSample at PaidPromotionAdjustParameter.scala:70) finished in 0.241 s
2021-12-07 14:38:57,827 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 4 finished: takeSample at PaidPromotionAdjustParameter.scala:70, took 0.243963 s
2021-12-07 14:38:57,840 [main] INFO [org.apache.spark.SparkContext] - Starting job: takeSample at PaidPromotionAdjustParameter.scala:70
2021-12-07 14:38:57,841 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 5 (takeSample at PaidPromotionAdjustParameter.scala:70) with 20 output partitions
2021-12-07 14:38:57,841 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 13 (takeSample at PaidPromotionAdjustParameter.scala:70)
2021-12-07 14:38:57,841 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 12)
2021-12-07 14:38:57,841 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 14:38:57,841 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 13 (PartitionwiseSampledRDD[12] at takeSample at PaidPromotionAdjustParameter.scala:70), which has no missing parents
2021-12-07 14:38:57,843 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8 stored as values in memory (estimated size 4.9 KB, free 1990.5 MB)
2021-12-07 14:38:57,844 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1990.5 MB)
2021-12-07 14:38:57,845 [dispatcher-event-loop-11] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_8_piece0 in memory on qb:60003 (size: 2.8 KB, free: 1990.8 MB)
2021-12-07 14:38:57,845 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 8 from broadcast at DAGScheduler.scala:1039
2021-12-07 14:38:57,845 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 20 missing tasks from ResultStage 13 (PartitionwiseSampledRDD[12] at takeSample at PaidPromotionAdjustParameter.scala:70) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2021-12-07 14:38:57,845 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 13.0 with 20 tasks
2021-12-07 14:38:57,846 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 13.0 (TID 139, localhost, executor driver, partition 0, ANY, 7868 bytes)
2021-12-07 14:38:57,846 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 13.0 (TID 140, localhost, executor driver, partition 1, ANY, 7868 bytes)
2021-12-07 14:38:57,846 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 13.0 (TID 141, localhost, executor driver, partition 2, ANY, 7868 bytes)
2021-12-07 14:38:57,847 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 13.0 (TID 142, localhost, executor driver, partition 3, ANY, 7868 bytes)
2021-12-07 14:38:57,847 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 4.0 in stage 13.0 (TID 143, localhost, executor driver, partition 4, ANY, 7868 bytes)
2021-12-07 14:38:57,847 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 5.0 in stage 13.0 (TID 144, localhost, executor driver, partition 5, ANY, 7868 bytes)
2021-12-07 14:38:57,847 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 6.0 in stage 13.0 (TID 145, localhost, executor driver, partition 6, ANY, 7868 bytes)
2021-12-07 14:38:57,847 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 7.0 in stage 13.0 (TID 146, localhost, executor driver, partition 7, ANY, 7868 bytes)
2021-12-07 14:38:57,847 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 8.0 in stage 13.0 (TID 147, localhost, executor driver, partition 8, ANY, 7868 bytes)
2021-12-07 14:38:57,847 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 9.0 in stage 13.0 (TID 148, localhost, executor driver, partition 9, ANY, 7868 bytes)
2021-12-07 14:38:57,847 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 10.0 in stage 13.0 (TID 149, localhost, executor driver, partition 10, ANY, 7868 bytes)
2021-12-07 14:38:57,847 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 11.0 in stage 13.0 (TID 150, localhost, executor driver, partition 11, ANY, 7868 bytes)
2021-12-07 14:38:57,847 [Executor task launch worker for task 140] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 13.0 (TID 140)
2021-12-07 14:38:57,847 [Executor task launch worker for task 146] INFO [org.apache.spark.executor.Executor] - Running task 7.0 in stage 13.0 (TID 146)
2021-12-07 14:38:57,847 [Executor task launch worker for task 149] INFO [org.apache.spark.executor.Executor] - Running task 10.0 in stage 13.0 (TID 149)
2021-12-07 14:38:57,847 [Executor task launch worker for task 148] INFO [org.apache.spark.executor.Executor] - Running task 9.0 in stage 13.0 (TID 148)
2021-12-07 14:38:57,847 [Executor task launch worker for task 139] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 13.0 (TID 139)
2021-12-07 14:38:57,847 [Executor task launch worker for task 143] INFO [org.apache.spark.executor.Executor] - Running task 4.0 in stage 13.0 (TID 143)
2021-12-07 14:38:57,847 [Executor task launch worker for task 145] INFO [org.apache.spark.executor.Executor] - Running task 6.0 in stage 13.0 (TID 145)
2021-12-07 14:38:57,847 [Executor task launch worker for task 144] INFO [org.apache.spark.executor.Executor] - Running task 5.0 in stage 13.0 (TID 144)
2021-12-07 14:38:57,847 [Executor task launch worker for task 142] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 13.0 (TID 142)
2021-12-07 14:38:57,847 [Executor task launch worker for task 141] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 13.0 (TID 141)
2021-12-07 14:38:57,847 [Executor task launch worker for task 150] INFO [org.apache.spark.executor.Executor] - Running task 11.0 in stage 13.0 (TID 150)
2021-12-07 14:38:57,847 [Executor task launch worker for task 147] INFO [org.apache.spark.executor.Executor] - Running task 8.0 in stage 13.0 (TID 147)
2021-12-07 14:38:57,850 [Executor task launch worker for task 147] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,850 [Executor task launch worker for task 141] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,851 [Executor task launch worker for task 147] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:57,851 [Executor task launch worker for task 141] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 1 ms
2021-12-07 14:38:57,851 [Executor task launch worker for task 140] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,851 [Executor task launch worker for task 140] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,851 [Executor task launch worker for task 142] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,851 [Executor task launch worker for task 142] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,851 [Executor task launch worker for task 149] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,851 [Executor task launch worker for task 149] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,852 [Executor task launch worker for task 145] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,852 [Executor task launch worker for task 145] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,852 [Executor task launch worker for task 150] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,852 [Executor task launch worker for task 150] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,852 [Executor task launch worker for task 144] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,852 [Executor task launch worker for task 144] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,853 [Executor task launch worker for task 148] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,853 [Executor task launch worker for task 148] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,853 [Executor task launch worker for task 143] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,853 [Executor task launch worker for task 143] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,853 [Executor task launch worker for task 139] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,853 [Executor task launch worker for task 139] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,854 [Executor task launch worker for task 146] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,854 [Executor task launch worker for task 146] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,979 [Executor task launch worker for task 140] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 13.0 (TID 140). 3958 bytes result sent to driver
2021-12-07 14:38:57,979 [Executor task launch worker for task 144] INFO [org.apache.spark.executor.Executor] - Finished task 5.0 in stage 13.0 (TID 144). 183361 bytes result sent to driver
2021-12-07 14:38:57,979 [dispatcher-event-loop-5] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 12.0 in stage 13.0 (TID 151, localhost, executor driver, partition 12, ANY, 7868 bytes)
2021-12-07 14:38:57,979 [Executor task launch worker for task 151] INFO [org.apache.spark.executor.Executor] - Running task 12.0 in stage 13.0 (TID 151)
2021-12-07 14:38:57,980 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 13.0 (TID 140) in 134 ms on localhost (executor driver) (1/20)
2021-12-07 14:38:57,980 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 13.0 in stage 13.0 (TID 152, localhost, executor driver, partition 13, ANY, 7868 bytes)
2021-12-07 14:38:57,980 [Executor task launch worker for task 152] INFO [org.apache.spark.executor.Executor] - Running task 13.0 in stage 13.0 (TID 152)
2021-12-07 14:38:57,981 [Executor task launch worker for task 141] INFO [org.apache.spark.executor.Executor] - Finished task 2.0 in stage 13.0 (TID 141). 160778 bytes result sent to driver
2021-12-07 14:38:57,981 [dispatcher-event-loop-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 14.0 in stage 13.0 (TID 153, localhost, executor driver, partition 14, ANY, 7868 bytes)
2021-12-07 14:38:57,981 [Executor task launch worker for task 142] INFO [org.apache.spark.executor.Executor] - Finished task 3.0 in stage 13.0 (TID 142). 181869 bytes result sent to driver
2021-12-07 14:38:57,981 [Executor task launch worker for task 150] INFO [org.apache.spark.executor.Executor] - Finished task 11.0 in stage 13.0 (TID 150). 178320 bytes result sent to driver
2021-12-07 14:38:57,981 [Executor task launch worker for task 153] INFO [org.apache.spark.executor.Executor] - Running task 14.0 in stage 13.0 (TID 153)
2021-12-07 14:38:57,983 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 15.0 in stage 13.0 (TID 154, localhost, executor driver, partition 15, ANY, 7868 bytes)
2021-12-07 14:38:57,983 [Executor task launch worker for task 154] INFO [org.apache.spark.executor.Executor] - Running task 15.0 in stage 13.0 (TID 154)
2021-12-07 14:38:57,983 [Executor task launch worker for task 151] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,983 [Executor task launch worker for task 151] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,983 [dispatcher-event-loop-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 16.0 in stage 13.0 (TID 155, localhost, executor driver, partition 16, ANY, 7868 bytes)
2021-12-07 14:38:57,984 [Executor task launch worker for task 155] INFO [org.apache.spark.executor.Executor] - Running task 16.0 in stage 13.0 (TID 155)
2021-12-07 14:38:57,984 [Executor task launch worker for task 147] INFO [org.apache.spark.executor.Executor] - Finished task 8.0 in stage 13.0 (TID 147). 201764 bytes result sent to driver
2021-12-07 14:38:57,984 [Executor task launch worker for task 153] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,984 [Executor task launch worker for task 153] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,984 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 5.0 in stage 13.0 (TID 144) in 137 ms on localhost (executor driver) (2/20)
2021-12-07 14:38:57,983 [Executor task launch worker for task 146] INFO [org.apache.spark.executor.Executor] - Finished task 7.0 in stage 13.0 (TID 146). 198482 bytes result sent to driver
2021-12-07 14:38:57,986 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 2.0 in stage 13.0 (TID 141) in 140 ms on localhost (executor driver) (3/20)
2021-12-07 14:38:57,986 [Executor task launch worker for task 154] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,986 [Executor task launch worker for task 154] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,986 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 17.0 in stage 13.0 (TID 156, localhost, executor driver, partition 17, ANY, 7868 bytes)
2021-12-07 14:38:57,986 [Executor task launch worker for task 156] INFO [org.apache.spark.executor.Executor] - Running task 17.0 in stage 13.0 (TID 156)
2021-12-07 14:38:57,986 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 18.0 in stage 13.0 (TID 157, localhost, executor driver, partition 18, ANY, 7868 bytes)
2021-12-07 14:38:57,987 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 3.0 in stage 13.0 (TID 142) in 141 ms on localhost (executor driver) (4/20)
2021-12-07 14:38:57,987 [Executor task launch worker for task 157] INFO [org.apache.spark.executor.Executor] - Running task 18.0 in stage 13.0 (TID 157)
2021-12-07 14:38:57,987 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 11.0 in stage 13.0 (TID 150) in 140 ms on localhost (executor driver) (5/20)
2021-12-07 14:38:57,988 [Executor task launch worker for task 149] INFO [org.apache.spark.executor.Executor] - Finished task 10.0 in stage 13.0 (TID 149). 162337 bytes result sent to driver
2021-12-07 14:38:57,988 [dispatcher-event-loop-11] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 19.0 in stage 13.0 (TID 158, localhost, executor driver, partition 19, ANY, 7868 bytes)
2021-12-07 14:38:57,989 [Executor task launch worker for task 158] INFO [org.apache.spark.executor.Executor] - Running task 19.0 in stage 13.0 (TID 158)
2021-12-07 14:38:57,989 [Executor task launch worker for task 156] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,989 [Executor task launch worker for task 156] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,989 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 7.0 in stage 13.0 (TID 146) in 142 ms on localhost (executor driver) (6/20)
2021-12-07 14:38:57,990 [Executor task launch worker for task 152] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,990 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 8.0 in stage 13.0 (TID 147) in 143 ms on localhost (executor driver) (7/20)
2021-12-07 14:38:57,990 [Executor task launch worker for task 152] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,990 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 10.0 in stage 13.0 (TID 149) in 143 ms on localhost (executor driver) (8/20)
2021-12-07 14:38:57,990 [Executor task launch worker for task 145] INFO [org.apache.spark.executor.Executor] - Finished task 6.0 in stage 13.0 (TID 145). 200903 bytes result sent to driver
2021-12-07 14:38:57,992 [Executor task launch worker for task 155] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,992 [Executor task launch worker for task 155] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 6 ms
2021-12-07 14:38:57,992 [Executor task launch worker for task 148] INFO [org.apache.spark.executor.Executor] - Finished task 9.0 in stage 13.0 (TID 148). 173843 bytes result sent to driver
2021-12-07 14:38:57,993 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 6.0 in stage 13.0 (TID 145) in 146 ms on localhost (executor driver) (9/20)
2021-12-07 14:38:57,995 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 9.0 in stage 13.0 (TID 148) in 148 ms on localhost (executor driver) (10/20)
2021-12-07 14:38:57,995 [Executor task launch worker for task 158] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:57,995 [Executor task launch worker for task 158] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:57,995 [Executor task launch worker for task 143] INFO [org.apache.spark.executor.Executor] - Finished task 4.0 in stage 13.0 (TID 143). 185316 bytes result sent to driver
2021-12-07 14:38:57,997 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 4.0 in stage 13.0 (TID 143) in 150 ms on localhost (executor driver) (11/20)
2021-12-07 14:38:58,002 [Executor task launch worker for task 157] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 20 non-empty blocks out of 20 blocks
2021-12-07 14:38:58,002 [Executor task launch worker for task 157] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 14:38:58,030 [Executor task launch worker for task 139] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 13.0 (TID 139). 1097 bytes result sent to driver
2021-12-07 14:38:58,030 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 13.0 (TID 139) in 185 ms on localhost (executor driver) (12/20)
2021-12-07 14:38:58,055 [Executor task launch worker for task 156] INFO [org.apache.spark.executor.Executor] - Finished task 17.0 in stage 13.0 (TID 156). 1097 bytes result sent to driver
2021-12-07 14:38:58,056 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 17.0 in stage 13.0 (TID 156) in 69 ms on localhost (executor driver) (13/20)
2021-12-07 14:38:58,057 [Executor task launch worker for task 151] INFO [org.apache.spark.executor.Executor] - Finished task 12.0 in stage 13.0 (TID 151). 161583 bytes result sent to driver
2021-12-07 14:38:58,058 [Executor task launch worker for task 153] INFO [org.apache.spark.executor.Executor] - Finished task 14.0 in stage 13.0 (TID 153). 140018 bytes result sent to driver
2021-12-07 14:38:58,059 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 12.0 in stage 13.0 (TID 151) in 80 ms on localhost (executor driver) (14/20)
2021-12-07 14:38:58,060 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 14.0 in stage 13.0 (TID 153) in 79 ms on localhost (executor driver) (15/20)
2021-12-07 14:38:58,063 [Executor task launch worker for task 158] INFO [org.apache.spark.executor.Executor] - Finished task 19.0 in stage 13.0 (TID 158). 1097 bytes result sent to driver
2021-12-07 14:38:58,064 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 19.0 in stage 13.0 (TID 158) in 76 ms on localhost (executor driver) (16/20)
2021-12-07 14:38:58,067 [Executor task launch worker for task 157] INFO [org.apache.spark.executor.Executor] - Finished task 18.0 in stage 13.0 (TID 157). 1097 bytes result sent to driver
2021-12-07 14:38:58,068 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 18.0 in stage 13.0 (TID 157) in 82 ms on localhost (executor driver) (17/20)
2021-12-07 14:38:58,069 [Executor task launch worker for task 154] INFO [org.apache.spark.executor.Executor] - Finished task 15.0 in stage 13.0 (TID 154). 195664 bytes result sent to driver
2021-12-07 14:38:58,070 [Executor task launch worker for task 155] INFO [org.apache.spark.executor.Executor] - Finished task 16.0 in stage 13.0 (TID 155). 180409 bytes result sent to driver
2021-12-07 14:38:58,070 [Executor task launch worker for task 152] INFO [org.apache.spark.executor.Executor] - Finished task 13.0 in stage 13.0 (TID 152). 210198 bytes result sent to driver
2021-12-07 14:38:58,070 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 15.0 in stage 13.0 (TID 154) in 87 ms on localhost (executor driver) (18/20)
2021-12-07 14:38:58,071 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 16.0 in stage 13.0 (TID 155) in 88 ms on localhost (executor driver) (19/20)
2021-12-07 14:38:58,071 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 13.0 in stage 13.0 (TID 152) in 91 ms on localhost (executor driver) (20/20)
2021-12-07 14:38:58,071 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 13.0, whose tasks have all completed, from pool 
2021-12-07 14:38:58,071 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 13 (takeSample at PaidPromotionAdjustParameter.scala:70) finished in 0.229 s
2021-12-07 14:38:58,071 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 5 finished: takeSample at PaidPromotionAdjustParameter.scala:70, took 0.231554 s
2021-12-07 14:38:58,087 [main] INFO [PaidPromotionAdjustParameter$] - 小数据集个数：80000
2021-12-07 14:38:58,130 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at PaidPromotionAdjustParameter.scala:78
2021-12-07 14:38:58,131 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 6 (count at PaidPromotionAdjustParameter.scala:78) with 20 output partitions
2021-12-07 14:38:58,131 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 14 (count at PaidPromotionAdjustParameter.scala:78)
2021-12-07 14:38:58,131 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 14:38:58,131 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 14:38:58,131 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 14 (MapPartitionsRDD[14] at map at PaidPromotionAdjustParameter.scala:76), which has no missing parents
2021-12-07 14:38:58,158 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9 stored as values in memory (estimated size 2.7 MB, free 1987.8 MB)
2021-12-07 14:38:58,165 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 938.8 KB, free 1986.9 MB)
2021-12-07 14:38:58,165 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_9_piece0 in memory on qb:60003 (size: 938.8 KB, free: 1989.9 MB)
2021-12-07 14:38:58,165 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 9 from broadcast at DAGScheduler.scala:1039
2021-12-07 14:38:58,166 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 20 missing tasks from ResultStage 14 (MapPartitionsRDD[14] at map at PaidPromotionAdjustParameter.scala:76) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2021-12-07 14:38:58,166 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 14.0 with 20 tasks
2021-12-07 14:38:58,166 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 14.0 (TID 159, localhost, executor driver, partition 0, ANY, 7899 bytes)
2021-12-07 14:38:58,166 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 14.0 (TID 160, localhost, executor driver, partition 1, ANY, 7899 bytes)
2021-12-07 14:38:58,166 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 2.0 in stage 14.0 (TID 161, localhost, executor driver, partition 2, ANY, 7899 bytes)
2021-12-07 14:38:58,166 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 3.0 in stage 14.0 (TID 162, localhost, executor driver, partition 3, ANY, 7899 bytes)
2021-12-07 14:38:58,167 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 4.0 in stage 14.0 (TID 163, localhost, executor driver, partition 4, ANY, 7899 bytes)
2021-12-07 14:38:58,167 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 5.0 in stage 14.0 (TID 164, localhost, executor driver, partition 5, ANY, 7899 bytes)
2021-12-07 14:38:58,167 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 6.0 in stage 14.0 (TID 165, localhost, executor driver, partition 6, ANY, 7899 bytes)
2021-12-07 14:38:58,167 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 7.0 in stage 14.0 (TID 166, localhost, executor driver, partition 7, ANY, 7899 bytes)
2021-12-07 14:38:58,167 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 8.0 in stage 14.0 (TID 167, localhost, executor driver, partition 8, ANY, 7899 bytes)
2021-12-07 14:38:58,167 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 9.0 in stage 14.0 (TID 168, localhost, executor driver, partition 9, ANY, 7899 bytes)
2021-12-07 14:38:58,167 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 10.0 in stage 14.0 (TID 169, localhost, executor driver, partition 10, ANY, 7899 bytes)
2021-12-07 14:38:58,167 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 11.0 in stage 14.0 (TID 170, localhost, executor driver, partition 11, ANY, 7899 bytes)
2021-12-07 14:38:58,168 [Executor task launch worker for task 163] INFO [org.apache.spark.executor.Executor] - Running task 4.0 in stage 14.0 (TID 163)
2021-12-07 14:38:58,168 [Executor task launch worker for task 165] INFO [org.apache.spark.executor.Executor] - Running task 6.0 in stage 14.0 (TID 165)
2021-12-07 14:38:58,168 [Executor task launch worker for task 164] INFO [org.apache.spark.executor.Executor] - Running task 5.0 in stage 14.0 (TID 164)
2021-12-07 14:38:58,168 [Executor task launch worker for task 168] INFO [org.apache.spark.executor.Executor] - Running task 9.0 in stage 14.0 (TID 168)
2021-12-07 14:38:58,168 [Executor task launch worker for task 170] INFO [org.apache.spark.executor.Executor] - Running task 11.0 in stage 14.0 (TID 170)
2021-12-07 14:38:58,168 [Executor task launch worker for task 159] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 14.0 (TID 159)
2021-12-07 14:38:58,168 [Executor task launch worker for task 160] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 14.0 (TID 160)
2021-12-07 14:38:58,168 [Executor task launch worker for task 161] INFO [org.apache.spark.executor.Executor] - Running task 2.0 in stage 14.0 (TID 161)
2021-12-07 14:38:58,168 [Executor task launch worker for task 169] INFO [org.apache.spark.executor.Executor] - Running task 10.0 in stage 14.0 (TID 169)
2021-12-07 14:38:58,168 [Executor task launch worker for task 167] INFO [org.apache.spark.executor.Executor] - Running task 8.0 in stage 14.0 (TID 167)
2021-12-07 14:38:58,168 [Executor task launch worker for task 162] INFO [org.apache.spark.executor.Executor] - Running task 3.0 in stage 14.0 (TID 162)
2021-12-07 14:38:58,168 [Executor task launch worker for task 166] INFO [org.apache.spark.executor.Executor] - Running task 7.0 in stage 14.0 (TID 166)
2021-12-07 14:38:58,236 [Executor task launch worker for task 162] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:402653184+134217728
2021-12-07 14:38:58,236 [Executor task launch worker for task 160] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:134217728+134217728
2021-12-07 14:38:58,236 [Executor task launch worker for task 169] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1342177280+134217728
2021-12-07 14:38:58,238 [Executor task launch worker for task 161] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:268435456+134217728
2021-12-07 14:38:58,241 [Executor task launch worker for task 165] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:805306368+134217728
2021-12-07 14:38:58,244 [Executor task launch worker for task 167] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1073741824+134217728
2021-12-07 14:38:58,245 [Executor task launch worker for task 159] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:0+134217728
2021-12-07 14:38:58,245 [Executor task launch worker for task 166] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:939524096+134217728
2021-12-07 14:38:58,254 [Executor task launch worker for task 170] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1476395008+134217728
2021-12-07 14:38:58,260 [Executor task launch worker for task 168] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:1207959552+134217728
2021-12-07 14:38:58,260 [Executor task launch worker for task 163] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:536870912+134217728
2021-12-07 14:38:58,263 [Executor task launch worker for task 164] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/data/behavior_data.bcp:671088640+134217728
2021-12-07 16:03:30,201 [main] INFO [org.apache.spark.SparkContext] - Running Spark version 2.3.0
2021-12-07 16:03:30,460 [main] INFO [org.apache.spark.SparkContext] - Submitted application: ValidationListSize
2021-12-07 16:03:30,505 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls to: ACER,root
2021-12-07 16:03:30,505 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls to: ACER,root
2021-12-07 16:03:30,506 [main] INFO [org.apache.spark.SecurityManager] - Changing view acls groups to: 
2021-12-07 16:03:30,506 [main] INFO [org.apache.spark.SecurityManager] - Changing modify acls groups to: 
2021-12-07 16:03:30,506 [main] INFO [org.apache.spark.SecurityManager] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ACER, root); groups with view permissions: Set(); users  with modify permissions: Set(ACER, root); groups with modify permissions: Set()
2021-12-07 16:03:31,058 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'sparkDriver' on port 62488.
2021-12-07 16:03:31,074 [main] INFO [org.apache.spark.SparkEnv] - Registering MapOutputTracker
2021-12-07 16:03:31,088 [main] INFO [org.apache.spark.SparkEnv] - Registering BlockManagerMaster
2021-12-07 16:03:31,091 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2021-12-07 16:03:31,091 [main] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - BlockManagerMasterEndpoint up
2021-12-07 16:03:31,100 [main] INFO [org.apache.spark.storage.DiskBlockManager] - Created local directory at C:\Users\ACER\AppData\Local\Temp\blockmgr-e4674b4a-65fe-47b9-85c0-4fb1388beb30
2021-12-07 16:03:31,114 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore started with capacity 1990.8 MB
2021-12-07 16:03:31,123 [main] INFO [org.apache.spark.SparkEnv] - Registering OutputCommitCoordinator
2021-12-07 16:03:31,175 [main] INFO [org.spark_project.jetty.util.log] - Logging initialized @1811ms
2021-12-07 16:03:31,223 [main] INFO [org.spark_project.jetty.server.Server] - jetty-9.3.z-SNAPSHOT
2021-12-07 16:03:31,235 [main] INFO [org.spark_project.jetty.server.Server] - Started @1871ms
2021-12-07 16:03:31,260 [main] INFO [org.spark_project.jetty.server.AbstractConnector] - Started ServerConnector@4d63b624{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2021-12-07 16:03:31,260 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'SparkUI' on port 4040.
2021-12-07 16:03:31,279 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6b5966e1{/jobs,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,280 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@25bcd0c7{/jobs/json,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,281 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6c6357f9{/jobs/job,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,282 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@593e824f{/jobs/job/json,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,283 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@2f7a7219{/stages,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,283 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@361c294e{/stages/json,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,284 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5118388b{/stages/stage,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,286 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5af28b27{/stages/stage/json,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,287 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@332a7fce{/stages/pool,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,289 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@5217f3d0{/stages/pool/json,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,290 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@77307458{/storage,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,291 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@33617539{/storage/json,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,292 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@f9b7332{/storage/rdd,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,293 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1bdf8190{/storage/rdd/json,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,294 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@ec0c838{/environment,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,296 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@21c64522{/environment/json,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,297 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@76a82f33{/executors,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,299 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@532a02d9{/executors/json,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,301 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@62923ee6{/executors/threadDump,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,303 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@b91d8c4{/executors/threadDump/json,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,311 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@a1217f9{/static,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,313 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@1950e8a6{/,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,315 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@724f138e{/api,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,317 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@782a4fff{/jobs/job/kill,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,318 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@6063d80a{/stages/stage/kill,null,AVAILABLE,@Spark}
2021-12-07 16:03:31,320 [main] INFO [org.apache.spark.ui.SparkUI] - Bound SparkUI to 0.0.0.0, and started at http://qb:4040
2021-12-07 16:03:31,400 [main] INFO [org.apache.spark.executor.Executor] - Starting executor ID driver on host localhost
2021-12-07 16:03:31,446 [main] INFO [org.apache.spark.util.Utils] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62531.
2021-12-07 16:03:31,446 [main] INFO [org.apache.spark.network.netty.NettyBlockTransferService] - Server created on qb:62531
2021-12-07 16:03:31,447 [main] INFO [org.apache.spark.storage.BlockManager] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2021-12-07 16:03:31,448 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registering BlockManager BlockManagerId(driver, qb, 62531, None)
2021-12-07 16:03:31,450 [dispatcher-event-loop-10] INFO [org.apache.spark.storage.BlockManagerMasterEndpoint] - Registering block manager qb:62531 with 1990.8 MB RAM, BlockManagerId(driver, qb, 62531, None)
2021-12-07 16:03:31,452 [main] INFO [org.apache.spark.storage.BlockManagerMaster] - Registered BlockManager BlockManagerId(driver, qb, 62531, None)
2021-12-07 16:03:31,452 [main] INFO [org.apache.spark.storage.BlockManager] - Initialized BlockManager: BlockManagerId(driver, qb, 62531, None)
2021-12-07 16:03:31,578 [main] INFO [org.spark_project.jetty.server.handler.ContextHandler] - Started o.s.j.s.ServletContextHandler@29a23c3d{/metrics/json,null,AVAILABLE,@Spark}
2021-12-07 16:03:32,038 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0 stored as values in memory (estimated size 312.7 KB, free 1990.5 MB)
2021-12-07 16:03:32,249 [main] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KB, free 1990.5 MB)
2021-12-07 16:03:32,251 [dispatcher-event-loop-0] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_0_piece0 in memory on qb:62531 (size: 27.3 KB, free: 1990.8 MB)
2021-12-07 16:03:32,254 [main] INFO [org.apache.spark.SparkContext] - Created broadcast 0 from textFile at ValidationListSize.scala:21
2021-12-07 16:03:32,615 [main] WARN [org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory] - The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows.
2021-12-07 16:03:32,721 [main] INFO [org.apache.hadoop.mapred.FileInputFormat] - Total input paths to process : 1
2021-12-07 16:03:32,730 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at ValidationListSize.scala:25
2021-12-07 16:03:32,740 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 0 (count at ValidationListSize.scala:25) with 2 output partitions
2021-12-07 16:03:32,740 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 0 (count at ValidationListSize.scala:25)
2021-12-07 16:03:32,741 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 16:03:32,742 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 16:03:32,748 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 0 (MapPartitionsRDD[2] at map at ValidationListSize.scala:23), which has no missing parents
2021-12-07 16:03:32,777 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 1990.5 MB)
2021-12-07 16:03:32,782 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1973.0 B, free 1990.5 MB)
2021-12-07 16:03:32,783 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_1_piece0 in memory on qb:62531 (size: 1973.0 B, free: 1990.8 MB)
2021-12-07 16:03:32,783 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2021-12-07 16:03:32,792 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at ValidationListSize.scala:23) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 16:03:32,793 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 0.0 with 2 tasks
2021-12-07 16:03:32,822 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7914 bytes)
2021-12-07 16:03:32,824 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7914 bytes)
2021-12-07 16:03:32,829 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 0.0 (TID 0)
2021-12-07 16:03:32,829 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 0.0 (TID 1)
2021-12-07 16:03:32,864 [Executor task launch worker for task 1] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:104158+104159
2021-12-07 16:03:32,865 [Executor task launch worker for task 0] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:0+104158
2021-12-07 16:03:33,057 [Executor task launch worker for task 0] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 0.0 (TID 0). 709 bytes result sent to driver
2021-12-07 16:03:33,057 [Executor task launch worker for task 1] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 0.0 (TID 1). 709 bytes result sent to driver
2021-12-07 16:03:33,066 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 0.0 (TID 1) in 241 ms on localhost (executor driver) (1/2)
2021-12-07 16:03:33,067 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 0.0 (TID 0) in 253 ms on localhost (executor driver) (2/2)
2021-12-07 16:03:33,068 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2021-12-07 16:03:33,072 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 0 (count at ValidationListSize.scala:25) finished in 0.311 s
2021-12-07 16:03:33,075 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 0 finished: count at ValidationListSize.scala:25, took 0.345521 s
2021-12-07 16:03:33,092 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at ValidationListSize.scala:26
2021-12-07 16:03:33,100 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 4 (distinct at ValidationListSize.scala:26)
2021-12-07 16:03:33,101 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 1 (count at ValidationListSize.scala:26) with 2 output partitions
2021-12-07 16:03:33,101 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 2 (count at ValidationListSize.scala:26)
2021-12-07 16:03:33,101 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 1)
2021-12-07 16:03:33,102 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 1)
2021-12-07 16:03:33,102 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[4] at distinct at ValidationListSize.scala:26), which has no missing parents
2021-12-07 16:03:33,110 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1990.5 MB)
2021-12-07 16:03:33,114 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1990.5 MB)
2021-12-07 16:03:33,115 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_2_piece0 in memory on qb:62531 (size: 2.8 KB, free: 1990.8 MB)
2021-12-07 16:03:33,115 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2021-12-07 16:03:33,117 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[4] at distinct at ValidationListSize.scala:26) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 16:03:33,117 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 1.0 with 2 tasks
2021-12-07 16:03:33,118 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7903 bytes)
2021-12-07 16:03:33,118 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7903 bytes)
2021-12-07 16:03:33,119 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 1.0 (TID 2)
2021-12-07 16:03:33,119 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 1.0 (TID 3)
2021-12-07 16:03:33,122 [Executor task launch worker for task 2] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:0+104158
2021-12-07 16:03:33,122 [Executor task launch worker for task 3] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:104158+104159
2021-12-07 16:03:33,263 [Executor task launch worker for task 2] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 1.0 (TID 2). 1032 bytes result sent to driver
2021-12-07 16:03:33,263 [Executor task launch worker for task 3] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 1.0 (TID 3). 1032 bytes result sent to driver
2021-12-07 16:03:33,278 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 1.0 (TID 2) in 161 ms on localhost (executor driver) (1/2)
2021-12-07 16:03:33,278 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 1.0 (TID 3) in 160 ms on localhost (executor driver) (2/2)
2021-12-07 16:03:33,278 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2021-12-07 16:03:33,279 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 1 (distinct at ValidationListSize.scala:26) finished in 0.175 s
2021-12-07 16:03:33,280 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 16:03:33,280 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 16:03:33,280 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 2)
2021-12-07 16:03:33,280 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 16:03:33,283 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 2 (MapPartitionsRDD[6] at distinct at ValidationListSize.scala:26), which has no missing parents
2021-12-07 16:03:33,287 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3 stored as values in memory (estimated size 3.7 KB, free 1990.5 MB)
2021-12-07 16:03:33,290 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1990.4 MB)
2021-12-07 16:03:33,291 [dispatcher-event-loop-1] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_3_piece0 in memory on qb:62531 (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 16:03:33,291 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 3 from broadcast at DAGScheduler.scala:1039
2021-12-07 16:03:33,292 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at distinct at ValidationListSize.scala:26) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 16:03:33,292 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 2.0 with 2 tasks
2021-12-07 16:03:33,293 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 7649 bytes)
2021-12-07 16:03:33,293 [dispatcher-event-loop-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 16:03:33,293 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 2.0 (TID 4)
2021-12-07 16:03:33,293 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 2.0 (TID 5)
2021-12-07 16:03:33,305 [Executor task launch worker for task 4] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 1 non-empty blocks out of 2 blocks
2021-12-07 16:03:33,305 [Executor task launch worker for task 5] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 16:03:33,306 [Executor task launch worker for task 5] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 4 ms
2021-12-07 16:03:33,306 [Executor task launch worker for task 4] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 4 ms
2021-12-07 16:03:33,338 [Executor task launch worker for task 4] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 2.0 (TID 4). 1096 bytes result sent to driver
2021-12-07 16:03:33,338 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 2.0 (TID 4) in 46 ms on localhost (executor driver) (1/2)
2021-12-07 16:03:33,339 [Executor task launch worker for task 5] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 2.0 (TID 5). 1096 bytes result sent to driver
2021-12-07 16:03:33,339 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 2.0 (TID 5) in 46 ms on localhost (executor driver) (2/2)
2021-12-07 16:03:33,339 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2021-12-07 16:03:33,340 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 2 (count at ValidationListSize.scala:26) finished in 0.054 s
2021-12-07 16:03:33,340 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 1 finished: count at ValidationListSize.scala:26, took 0.246770 s
2021-12-07 16:03:33,368 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at ValidationListSize.scala:31
2021-12-07 16:03:33,368 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Registering RDD 7 (map at ValidationListSize.scala:28)
2021-12-07 16:03:33,368 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 2 (take at ValidationListSize.scala:31) with 1 output partitions
2021-12-07 16:03:33,368 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 4 (take at ValidationListSize.scala:31)
2021-12-07 16:03:33,368 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 3)
2021-12-07 16:03:33,368 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List(ShuffleMapStage 3)
2021-12-07 16:03:33,369 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ShuffleMapStage 3 (MapPartitionsRDD[7] at map at ValidationListSize.scala:28), which has no missing parents
2021-12-07 16:03:33,370 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4 stored as values in memory (estimated size 4.7 KB, free 1990.4 MB)
2021-12-07 16:03:33,374 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.7 KB, free 1990.4 MB)
2021-12-07 16:03:33,374 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_4_piece0 in memory on qb:62531 (size: 2.7 KB, free: 1990.8 MB)
2021-12-07 16:03:33,374 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 4 from broadcast at DAGScheduler.scala:1039
2021-12-07 16:03:33,375 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[7] at map at ValidationListSize.scala:28) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 16:03:33,375 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 3.0 with 2 tasks
2021-12-07 16:03:33,375 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 7903 bytes)
2021-12-07 16:03:33,376 [dispatcher-event-loop-8] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 7903 bytes)
2021-12-07 16:03:33,376 [Executor task launch worker for task 6] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 3.0 (TID 6)
2021-12-07 16:03:33,376 [Executor task launch worker for task 7] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 3.0 (TID 7)
2021-12-07 16:03:33,378 [Executor task launch worker for task 6] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:0+104158
2021-12-07 16:03:33,378 [Executor task launch worker for task 7] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:104158+104159
2021-12-07 16:03:33,414 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 53
2021-12-07 16:03:33,414 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 48
2021-12-07 16:03:33,414 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 23
2021-12-07 16:03:33,414 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 35
2021-12-07 16:03:33,414 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 65
2021-12-07 16:03:33,414 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 2
2021-12-07 16:03:33,419 [Executor task launch worker for task 7] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 3.0 (TID 7). 1032 bytes result sent to driver
2021-12-07 16:03:33,419 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 3.0 (TID 7) in 44 ms on localhost (executor driver) (1/2)
2021-12-07 16:03:33,420 [Executor task launch worker for task 6] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 3.0 (TID 6). 989 bytes result sent to driver
2021-12-07 16:03:33,421 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 3.0 (TID 6) in 46 ms on localhost (executor driver) (2/2)
2021-12-07 16:03:33,421 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2021-12-07 16:03:33,421 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ShuffleMapStage 3 (map at ValidationListSize.scala:28) finished in 0.052 s
2021-12-07 16:03:33,421 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - looking for newly runnable stages
2021-12-07 16:03:33,422 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - running: Set()
2021-12-07 16:03:33,422 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - waiting: Set(ResultStage 4)
2021-12-07 16:03:33,422 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - failed: Set()
2021-12-07 16:03:33,422 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 4 (MapPartitionsRDD[9] at map at ValidationListSize.scala:30), which has no missing parents
2021-12-07 16:03:33,423 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5 stored as values in memory (estimated size 3.6 KB, free 1990.4 MB)
2021-12-07 16:03:33,426 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.4 MB)
2021-12-07 16:03:33,427 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_5_piece0 in memory on qb:62531 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 16:03:33,427 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 5 from broadcast at DAGScheduler.scala:1039
2021-12-07 16:03:33,428 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at ValidationListSize.scala:30) (first 15 tasks are for partitions Vector(0))
2021-12-07 16:03:33,428 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 4.0 with 1 tasks
2021-12-07 16:03:33,428 [dispatcher-event-loop-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 4.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 7649 bytes)
2021-12-07 16:03:33,429 [dispatcher-event-loop-6] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_3_piece0 on qb:62531 in memory (size: 2.2 KB, free: 1990.8 MB)
2021-12-07 16:03:33,429 [Executor task launch worker for task 8] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 4.0 (TID 8)
2021-12-07 16:03:33,431 [Executor task launch worker for task 8] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 0 non-empty blocks out of 2 blocks
2021-12-07 16:03:33,431 [Executor task launch worker for task 8] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 16:03:33,432 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 55
2021-12-07 16:03:33,432 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 61
2021-12-07 16:03:33,432 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 40
2021-12-07 16:03:33,432 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 49
2021-12-07 16:03:33,432 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 51
2021-12-07 16:03:33,433 [dispatcher-event-loop-9] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_2_piece0 on qb:62531 in memory (size: 2.8 KB, free: 1990.8 MB)
2021-12-07 16:03:33,435 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned shuffle 0
2021-12-07 16:03:33,435 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 26
2021-12-07 16:03:33,436 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 69
2021-12-07 16:03:33,436 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 24
2021-12-07 16:03:33,436 [Executor task launch worker for task 8] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 4.0 (TID 8). 1054 bytes result sent to driver
2021-12-07 16:03:33,436 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 66
2021-12-07 16:03:33,436 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 32
2021-12-07 16:03:33,436 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 18
2021-12-07 16:03:33,436 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 62
2021-12-07 16:03:33,436 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 45
2021-12-07 16:03:33,436 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 27
2021-12-07 16:03:33,436 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 74
2021-12-07 16:03:33,436 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 28
2021-12-07 16:03:33,436 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 15
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 20
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 43
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 38
2021-12-07 16:03:33,437 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 4.0 (TID 8) in 9 ms on localhost (executor driver) (1/1)
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 7
2021-12-07 16:03:33,437 [task-result-getter-1] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 6
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 1
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 44
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 25
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 4
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 47
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 33
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 11
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 42
2021-12-07 16:03:33,437 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 4 (take at ValidationListSize.scala:31) finished in 0.015 s
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 17
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 73
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 8
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 31
2021-12-07 16:03:33,437 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 5
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 58
2021-12-07 16:03:33,438 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 2 finished: take at ValidationListSize.scala:31, took 0.070677 s
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 10
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 14
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 54
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 72
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 67
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 16
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 41
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 30
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 52
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 56
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 22
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 12
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 19
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 59
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 68
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 60
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 70
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 29
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 34
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 39
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 57
2021-12-07 16:03:33,438 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 0
2021-12-07 16:03:33,439 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 21
2021-12-07 16:03:33,439 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 64
2021-12-07 16:03:33,439 [dispatcher-event-loop-4] INFO [org.apache.spark.storage.BlockManagerInfo] - Removed broadcast_1_piece0 on qb:62531 in memory (size: 1973.0 B, free: 1990.8 MB)
2021-12-07 16:03:33,440 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 50
2021-12-07 16:03:33,440 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 37
2021-12-07 16:03:33,440 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 46
2021-12-07 16:03:33,440 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 63
2021-12-07 16:03:33,440 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 9
2021-12-07 16:03:33,440 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 71
2021-12-07 16:03:33,440 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 3
2021-12-07 16:03:33,440 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 13
2021-12-07 16:03:33,440 [Spark Context Cleaner] INFO [org.apache.spark.ContextCleaner] - Cleaned accumulator 36
2021-12-07 16:03:33,444 [main] INFO [org.apache.spark.SparkContext] - Starting job: take at ValidationListSize.scala:31
2021-12-07 16:03:33,444 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 3 (take at ValidationListSize.scala:31) with 1 output partitions
2021-12-07 16:03:33,444 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 6 (take at ValidationListSize.scala:31)
2021-12-07 16:03:33,444 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List(ShuffleMapStage 5)
2021-12-07 16:03:33,445 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 16:03:33,445 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 6 (MapPartitionsRDD[9] at map at ValidationListSize.scala:30), which has no missing parents
2021-12-07 16:03:33,446 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6 stored as values in memory (estimated size 3.6 KB, free 1990.5 MB)
2021-12-07 16:03:33,448 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1990.4 MB)
2021-12-07 16:03:33,449 [dispatcher-event-loop-3] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_6_piece0 in memory on qb:62531 (size: 2.1 KB, free: 1990.8 MB)
2021-12-07 16:03:33,449 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 6 from broadcast at DAGScheduler.scala:1039
2021-12-07 16:03:33,450 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[9] at map at ValidationListSize.scala:30) (first 15 tasks are for partitions Vector(1))
2021-12-07 16:03:33,450 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 6.0 with 1 tasks
2021-12-07 16:03:33,450 [dispatcher-event-loop-6] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 6.0 (TID 9, localhost, executor driver, partition 1, ANY, 7649 bytes)
2021-12-07 16:03:33,450 [Executor task launch worker for task 9] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 6.0 (TID 9)
2021-12-07 16:03:33,452 [Executor task launch worker for task 9] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Getting 2 non-empty blocks out of 2 blocks
2021-12-07 16:03:33,452 [Executor task launch worker for task 9] INFO [org.apache.spark.storage.ShuffleBlockFetcherIterator] - Started 0 remote fetches in 0 ms
2021-12-07 16:03:33,466 [Executor task launch worker for task 9] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 6.0 (TID 9). 1013 bytes result sent to driver
2021-12-07 16:03:33,467 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 6.0 (TID 9) in 17 ms on localhost (executor driver) (1/1)
2021-12-07 16:03:33,467 [task-result-getter-0] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2021-12-07 16:03:33,468 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 6 (take at ValidationListSize.scala:31) finished in 0.023 s
2021-12-07 16:03:33,468 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 3 finished: take at ValidationListSize.scala:31, took 0.024112 s
2021-12-07 16:03:33,473 [main] INFO [org.apache.spark.SparkContext] - Starting job: count at ValidationListSize.scala:34
2021-12-07 16:03:33,474 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Got job 4 (count at ValidationListSize.scala:34) with 2 output partitions
2021-12-07 16:03:33,474 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Final stage: ResultStage 7 (count at ValidationListSize.scala:34)
2021-12-07 16:03:33,474 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Parents of final stage: List()
2021-12-07 16:03:33,474 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Missing parents: List()
2021-12-07 16:03:33,474 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting ResultStage 7 (MapPartitionsRDD[10] at flatMap at ValidationListSize.scala:34), which has no missing parents
2021-12-07 16:03:33,476 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7 stored as values in memory (estimated size 3.5 KB, free 1990.4 MB)
2021-12-07 16:03:33,478 [dag-scheduler-event-loop] INFO [org.apache.spark.storage.memory.MemoryStore] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2046.0 B, free 1990.4 MB)
2021-12-07 16:03:33,479 [dispatcher-event-loop-7] INFO [org.apache.spark.storage.BlockManagerInfo] - Added broadcast_7_piece0 in memory on qb:62531 (size: 2046.0 B, free: 1990.8 MB)
2021-12-07 16:03:33,479 [dag-scheduler-event-loop] INFO [org.apache.spark.SparkContext] - Created broadcast 7 from broadcast at DAGScheduler.scala:1039
2021-12-07 16:03:33,479 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at flatMap at ValidationListSize.scala:34) (first 15 tasks are for partitions Vector(0, 1))
2021-12-07 16:03:33,480 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Adding task set 7.0 with 2 tasks
2021-12-07 16:03:33,480 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 0.0 in stage 7.0 (TID 10, localhost, executor driver, partition 0, ANY, 7914 bytes)
2021-12-07 16:03:33,481 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.TaskSetManager] - Starting task 1.0 in stage 7.0 (TID 11, localhost, executor driver, partition 1, ANY, 7914 bytes)
2021-12-07 16:03:33,481 [Executor task launch worker for task 11] INFO [org.apache.spark.executor.Executor] - Running task 1.0 in stage 7.0 (TID 11)
2021-12-07 16:03:33,481 [Executor task launch worker for task 10] INFO [org.apache.spark.executor.Executor] - Running task 0.0 in stage 7.0 (TID 10)
2021-12-07 16:03:33,482 [Executor task launch worker for task 10] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:0+104158
2021-12-07 16:03:33,482 [Executor task launch worker for task 11] INFO [org.apache.spark.rdd.HadoopRDD] - Input split: hdfs://hdp1:8020/sk/chongqing/list/validation-production/part-00000:104158+104159
2021-12-07 16:03:33,504 [Executor task launch worker for task 11] INFO [org.apache.spark.executor.Executor] - Finished task 1.0 in stage 7.0 (TID 11). 667 bytes result sent to driver
2021-12-07 16:03:33,504 [task-result-getter-2] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 1.0 in stage 7.0 (TID 11) in 24 ms on localhost (executor driver) (1/2)
2021-12-07 16:03:33,513 [Executor task launch worker for task 10] INFO [org.apache.spark.executor.Executor] - Finished task 0.0 in stage 7.0 (TID 10). 667 bytes result sent to driver
2021-12-07 16:03:33,514 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSetManager] - Finished task 0.0 in stage 7.0 (TID 10) in 34 ms on localhost (executor driver) (2/2)
2021-12-07 16:03:33,514 [task-result-getter-3] INFO [org.apache.spark.scheduler.TaskSchedulerImpl] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2021-12-07 16:03:33,514 [dag-scheduler-event-loop] INFO [org.apache.spark.scheduler.DAGScheduler] - ResultStage 7 (count at ValidationListSize.scala:34) finished in 0.039 s
2021-12-07 16:03:33,515 [main] INFO [org.apache.spark.scheduler.DAGScheduler] - Job 4 finished: count at ValidationListSize.scala:34, took 0.041032 s
2021-12-07 16:03:33,517 [Thread-1] INFO [org.apache.spark.SparkContext] - Invoking stop() from shutdown hook
2021-12-07 16:03:33,523 [Thread-1] INFO [org.spark_project.jetty.server.AbstractConnector] - Stopped Spark@4d63b624{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2021-12-07 16:03:33,524 [Thread-1] INFO [org.apache.spark.ui.SparkUI] - Stopped Spark web UI at http://qb:4040
2021-12-07 16:03:33,533 [dispatcher-event-loop-3] INFO [org.apache.spark.MapOutputTrackerMasterEndpoint] - MapOutputTrackerMasterEndpoint stopped!
2021-12-07 16:03:33,552 [Thread-1] INFO [org.apache.spark.storage.memory.MemoryStore] - MemoryStore cleared
2021-12-07 16:03:33,552 [Thread-1] INFO [org.apache.spark.storage.BlockManager] - BlockManager stopped
2021-12-07 16:03:33,553 [Thread-1] INFO [org.apache.spark.storage.BlockManagerMaster] - BlockManagerMaster stopped
2021-12-07 16:03:33,555 [dispatcher-event-loop-9] INFO [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint] - OutputCommitCoordinator stopped!
2021-12-07 16:03:33,558 [Thread-1] INFO [org.apache.spark.SparkContext] - Successfully stopped SparkContext
2021-12-07 16:03:33,559 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Shutdown hook called
2021-12-07 16:03:33,559 [Thread-1] INFO [org.apache.spark.util.ShutdownHookManager] - Deleting directory C:\Users\ACER\AppData\Local\Temp\spark-008392d3-4db5-449c-88be-59262b8d3ebb
